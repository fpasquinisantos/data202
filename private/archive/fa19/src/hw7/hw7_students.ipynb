{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          ax=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if title is None:\n",
    "            title = \"Normalized confusion matrix\"\n",
    "    elif title is None:\n",
    "        title = 'Confusion matrix (without normalization)'\n",
    "\n",
    "    img = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar(img, ax=ax)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set(xticks=tick_marks, yticks=tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=0)\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    ax.set(title=title, ylabel='True label', xlabel='Predicted label')\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the data file from https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
    "* Dataset source: https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = 'checking_status duration credit_history purpose credit_amount savings_status employment installment_commitment personal_status other_parties residence_since property_magnitude age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker actual'.split()\n",
    "credit_data = pd.read_csv(\n",
    "    \"german.data\",\n",
    "    delimiter=' ', index_col=False,\n",
    "    names=column_names)\n",
    "\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['actual'] = credit_data['actual'].map({1: \"good\", 2: \"bad\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"personal status\" attribute encodes a few different quantities. Let's split them apart.\n",
    "credit_data[['sex', 'marital_status']] = credit_data['personal_status'].map({\n",
    "    'A91': \"male : divorced/separated\",\n",
    "    'A92': \"female : divorced/separated/married\",\n",
    "    'A93': \"male : single\",\n",
    "    'A94': \"male : married/widowed\",\n",
    "    'A95': \"female : single\",\n",
    "}).str.split(\" : \", expand=True)\n",
    "credit_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Credit Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does credit history relate with current credit risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(credit_data['credit_history'], credit_data['actual'], margins=True, normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(credit_data[['checking_status', 'savings_status', 'credit_history', 'employment', 'job']], drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (credit_data['actual'] == 'good').astype(int)\n",
    "\n",
    "# Compute the \"prevalence\" (see https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix)\n",
    "total_population = len(credit_data)\n",
    "total_positive = np.sum(y_true)\n",
    "prevalence = total_positive / total_population\n",
    "print(\"Fraction of 'good' customers in the dataset:\", prevalence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lab we tried using a Linear Regression to make this score.\n",
    "\n",
    "```\n",
    "clf = LinearRegression().fit(X, y_true)\n",
    "```\n",
    "\n",
    "Logistic Regression uses the same kind of linear model to create a score, but it accounts for the fact that a difference of, say, plus or minus 1.0 in the score makes a bigger difference when the score is close to the decision boundary. Notice the following things about `LogisticRegression`:\n",
    "\n",
    "* The interface is almost identical to `LinearRegression`, at least in the case of a binary classification (e.g., good vs bad credit).\n",
    "* The `predict` function gives the classifier's single decision. If we want the *score*, we need to ask it for the `decision_function`.\n",
    "* The `decision_function` is very much like `LinearRegression`'s `.predict`, but the score is defined such that positive scores correspond to a decision of \"1\" and negative scores to \"0\". This makes more sense when we extend to more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs').fit(X, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_score = clf.decision_function(X)\n",
    "predicted_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predicted_score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction by thresholding the score. First we'll try setting a threshold at 0. To summarize how this threshold does, we can plot the \"confusion matrix\". We'll use a function (defined up top) to make a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = (predicted_score > 0.).astype(int)\n",
    "print(\"First 5 predictions:\", predicted[:5])\n",
    "plot_confusion_matrix(y_true, predicted, classes=[\"Bad\", \"Good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Try a different threshold.\n",
    "**Change the threshold so that all four boxes are non-zero**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = (predicted_score > 0.).astype(int)\n",
    "print(\"First 5 predictions:\", predicted[:5])\n",
    "plot_confusion_matrix(y_true, predicted, classes=[\"Bad\", \"Good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Label false positives vs false negatives\n",
    "Fill in the following cell with the counts corresponding to the threshold you chose in Exercise 1.\n",
    "\n",
    "You may refer to the reading to clarify the definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* True positives: **###**\n",
    "* True negatives: **###**\n",
    "* False positives: **###**\n",
    "* False negatives: **###**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Write expressions to calculate FP/FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = np.sum((predicted == 1) & (y_true == 1))\n",
    "true_negatives = np.sum((predicted == 0) & (y_true == 0))\n",
    "false_positives = ...\n",
    "false_negatives = ...\n",
    "\n",
    "print(\"True positives:\", true_positives)\n",
    "print(\"True negatives:\", true_negatives)\n",
    "print(\"False positives:\", false_positives)\n",
    "print(\"False negatives:\", false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Compute derived quantities\n",
    "Fill in the blanks in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", ...)\n",
    "print(\"Recall:\", ...)\n",
    "print(\"Positive rate:\", ...)\n",
    "print(\"True positive rate\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Calculate cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dataset description (see above), the bank assigned a cost of *5* to giving a loan to a \"bad\" customer (presumably because the customer defaulted on the loan) but a cost of only *1* to not giving a loan to a customer who actually would have been good (presumably because they would have paid the loan back with interest).\n",
    "\n",
    "*Using your variables for `true_positive`, `true_negative`, `false_positive` and `false_negative`*, **fill in the blanks in the following cell** to *calculate* the cost to the bank associated with the threshold you chose. You may not need to use all four variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_to_bank = ...\n",
    "avg_cost_per_person = cost_to_bank / total_population\n",
    "print(f\"Cost to bank is {cost_to_bank:.1f}, an average of {avg_cost_per_person:.2f} per person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Minimize cost (maximize profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use your answers to the previous exercises to fill in the blanks in the code below that plots cost vs threshold.\n",
    "2. Use that plot (and perhaps `results.sort_values`) to select a threshold value.\n",
    "3. Go back to Exercise 1 and put this threshold value there.\n",
    "4. Report the threshold value and its corresponding cost, positive rate, and true positive rate. (You can re-run the calculations above with your new threshold to get these numbers, but write them here for reference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a common pattern for collecting data into a Pandas DataFrame.\n",
    "results = []\n",
    "for threshold in np.linspace(np.min(predicted_score), np.max(predicted_score), 100):\n",
    "    predicted = (predicted_score > threshold).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"avg_cost\": avg_cost_per_person\n",
    "    })\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's almost always a good idea to separate the code that *makes a plot* from the code that *generates the data*. \n",
    "plt.plot(results.threshold, results.avg_cost)\n",
    "plt.xlabel(\"Score threshold\")\n",
    "plt.ylabel(\"Cost for the bank, per person\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = ...\n",
    "predicted = (predicted_score > threshold).astype(int)\n",
    "\n",
    "print(\"My selected threshold was\", threshold)\n",
    "print(\"At that threshold, the cost to the bank is\", cost)\n",
    "print(\"... the positive rate is\", positive_rate)\n",
    "print(\"... and the true positive rate is\", true_positive_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Evaluating Ageism\n",
    "\n",
    "The [Equal Credit Opportunity Act (ECOA)](https://www.consumer.ftc.gov/articles/0347-your-equal-credit-opportunity-rights) protects against lending discrimination on the basis of age. The Treasury Dept's [summary](https://www.occ.treas.gov/topics/consumers-and-communities/consumer-protection/fair-lending/index-fair-lending.html) says that \"A lender's policies, *even when applied equally to all its credit applicants*, may have a negative effect on certain applicants.\" (emphasis mine.)\n",
    "\n",
    "Even though our classifier does not consider age as a factor in making a decision, it is possible that it has *disparate impact* depending on an individual's age.\n",
    "\n",
    "**Fill in the blanks in the following cell** to evaluate the classifier *that uses the threshold you chose above*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data['age_bin'] = np.where(credit_data['age'] >= 25, \">= 25\", \"< 25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = ...\n",
    "predicted = (predicted_score > threshold).astype(int)\n",
    "\n",
    "age = credit_data['age_bin']\n",
    "num_young = np.sum(age == '< 25')\n",
    "num_old = np.sum(age == '>= 25')\n",
    "print(f\"{num_young} young, {num_old} old\")\n",
    "\n",
    "true_positives_young = np.sum((predicted == 1) & (y_true == 1) & (age == '< 25'))\n",
    "true_positives_old = np.sum((predicted == 1) & (y_true == 1) & (age == '>= 25'))\n",
    "\n",
    "positive_rate_young = ...\n",
    "positive_rate_old = ...\n",
    "\n",
    "true_positive_rate_young = ...\n",
    "true_positive_rate_old = ...\n",
    "\n",
    "print(f\"The positive rate is {positive_rate_young:.2f} for young and {positive_rate_old:.2f} for old\")\n",
    "print(f\"The true positive rate is {true_positive_rate_young:.2f} for young and {true_positive_rate_old:.2f} for old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "1. Does this classifier exhibit **demographic parity**? Explain.\n",
    "2. Does this classifier exhibit **equal opportunity** (considering \"good\" as the \"favorable\" class)? Explain.\n",
    "\n",
    "Refer to the readings to remind yourself of the definitions of these terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Your answer here*\n",
    "2. *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks in the following cell to make plots by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_age = []\n",
    "for threshold in np.linspace(np.min(predicted_score), np.max(predicted_score), 100):\n",
    "    predicted = (predicted_score > threshold).astype(int)\n",
    "    \n",
    "    for cur_age in ['< 25', '>= 25']:\n",
    "        num_of_this_age = np.sum(age == cur_age)\n",
    "        true_positives_age = np.sum((predicted == 1) & (y_true == 1) & (age == cur_age))\n",
    "\n",
    "\n",
    "        results_age.append({\n",
    "            \"age\": cur_age,\n",
    "            \"threshold\": threshold,\n",
    "            \"avg_cost\": cost_age / num_of_this_age,\n",
    "            \"positive_rate\": ...,\n",
    "            \"true_positive_rate\": ...\n",
    "        })\n",
    "results_age = pd.DataFrame(results_age)\n",
    "results_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_age_tidy = pd.melt(results_age, id_vars=['threshold', 'age'], var_name=\"measure\")\n",
    "results_age_tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    x=\"threshold\", y=\"value\", hue=\"age\",\n",
    "    col=\"measure\", facet_kws={'sharey': False},\n",
    "    kind='line',\n",
    "    data=results_age_tidy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Making a fairer classifier (maybe)\n",
    "Pick one of \"demographic parity\" or \"equal opportunity\".\n",
    "\n",
    "Use the charts above to select **two different thresholds**, one for each age group, that lead to a fairer classification by that metric. Fill in the code below to evaluate the fairness of your classifier as well as its cost to the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_young = ...\n",
    "threshold_old = ...\n",
    "predicted = (predicted_score > np.where(age == \">= 25\", threshold_old, threshold_young)).astype(int)\n",
    "\n",
    "age = credit_data['age_bin']\n",
    "num_young = np.sum(age == '< 25')\n",
    "num_old = np.sum(age == '>= 25')\n",
    "print(f\"{num_young} young, {num_old} old\")\n",
    "\n",
    "true_positives_young = np.sum((predicted == 1) & (y_true == 1) & (age == '< 25'))\n",
    "true_positives_old = np.sum((predicted == 1) & (y_true == 1) & (age == '>= 25'))\n",
    "\n",
    "positive_rate_young = ...\n",
    "positive_rate_old = ...\n",
    "\n",
    "true_positive_rate_young = ...\n",
    "true_positive_rate_old = ...\n",
    "\n",
    "\n",
    "print(f\"The average cost per person is {avg_cost_per_person:.2f}\")\n",
    "print(f\"The positive rate is {positive_rate_young:.2f} for young and {positive_rate_old:.2f} for old\")\n",
    "print(f\"The true positive rate is {true_positive_rate_young:.2f} for young and {true_positive_rate_old:.2f} for old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Reflection\n",
    "\n",
    "1. Does your new classifier exhibit **demographic parity**? Explain.\n",
    "2. Does your new classifier exhibit **equal opportunity** (considering \"good\" as the \"favorable\" class)? Explain.\n",
    "3. Suppose you were going to recommend that the bank use a specific classifier. (a) How would you *choose the thresholds* for this classifier? (b) How would you *argue to the bank* that your choice of thresholds is *good*, in light of considerations of cost (profit) and fairness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *your answer here*\n",
    "2. *your answer here*\n",
    "3. \n",
    "    1. *your answer here*\n",
    "    2. *your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
