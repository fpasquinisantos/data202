# Classification

* [Chapter 17](https://www.textbook.ds100.org/ch/17/classification_intro.html) in the textbook
* [video](https://www.youtube.com/watch?v=Ibw0WrnzT7I) and [slides](https://docs.google.com/presentation/d/1D4JXVXhVT6YfkQs5KbE98H11bHiJ_2bjLFDc1G73-YQ/edit#slide=id.g55c0827bd8_0_115) from Berkeley DATA 100 Summer 2019

## Evaluating classifiers

* [slides](https://docs.google.com/presentation/d/1ZmEdhdUIA1Yvq7SxMnV41jrpxZd_S9lD0D23Qe7vFa0/edit) from DATA100 su19
* [Sensitivity/Specificity / Confusion Matrix on Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix)

### Extra resources

* [A visual representation of cross-entropy loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)
    * related: [Visualizing Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)
* Area Under the Curve = chance of correctly ranking a random positive-negative pair
  * [A visual explanation](https://madrury.github.io/jekyll/update/statistics/2017/06/21/auc-proof.html)
  * [A math-y explanation](https://www.alexejgossmann.com/auc/)
