---
course:     Data 202
instructor: Kenneth Arnold
term:       Fall 2019
time:       9 am
days:       MWF
css:        calendar.css
---

days:: MWF

items::
Topic;  b;      color=black;    T:
Read;   i;      color=blue;     R:
Assign; i;      color=green;    A:
HW;     t;      color=green;    A:
Due;    t;      color=red> Due: <!-- --;        D:
Note;   b;      color=green;    N:
Vocab;  t;      color=purple;   V:


===10/22/2019

Note:: Advising
Note:: No Class

===10/23/2019

Note:: Advising
Note:: No Class

===11/27/2019

Note:: Thanksgiving Break
Note:: No Class

===11/29/2019

Note:: Thanksgiving Break
Note:: No Class

===12/09/2019

Note:: Wednesday Class Schedule

===12/10/2019

Note:: Reading Recess

===09/04/2019

Topic:: Course Intro

===
Topic:: Discussion on Bias in Facial Recognition
Read:: [Compassion through Computation: Fighting Algorithmic Bias | Joy Buolamwini](https://www.youtube.com/watch?v=_sgji-Bladk)

===
Lab:: Jupyter Notebook + Pandas

===
Topic:: Pandas indexing syntax

===
Lab:: Homework debrief, Pandas indexing

===
Lab:: Pandas Indexing continued (project check-ins, lab/hw work)

===
Topic:: Project goals, communication (visual, computational notebooks), quiz, homework feedback

===
Lab:: broadcasting, functions, measuring prediction error

===
Topic:: Intro to predictive modeling

===
Topic:: Discussion: applications of predictions for climate change, measuring error
Read:: [Machine Learning for Climate Change](https://www.technologyreview.com/s/613838/ai-climate-change-machine-learning/)

===
Topic:: Review

===
Topic:: Review, Python functions, error measurement

===
Topic:: Reidentification, linear regression intuition / squared error intuition
Read:: [Reidentification of Anonymized Data](https://georgetownlawtechreview.org/re-identification-of-anonymized-data/GLTR-04-2017/)

===
Topic:: sklearn, model selection basics, Python patterns

===
Topic:: Data science lifecycle, project 2 intro, outline of rest of semester

===
Topic:: Linear regression @@topics/linreg.html

===
Topic:: Linear regression (and review of error measurement)

===
Topic:: Multiple Linear Regression and Feature Engineering

===
Topic:: Multiple Linear Regression and Feature Engineering, continued

===
Lab:: Multiple Linear Regression, Feature Engineering, and Overfitting

===
Topic:: Linear Regression Review, open Q&A (e.g., how model fitting works)

===
(Advising)

===10/24/2019
Due:: Lab 5 (feature engineering and overfitting)

===
Topic:: Model Selection (how to measure the impact of modeling decisions on *generalization*)

Guiding questions:
* What are some decisions we might make while modeling?
  * Type of model to use (kNN vs Linreg vs ...)
  * Features to include
  * How to represent these features
  * Hyperparameters (number of neighbors)
* How might these decisions affect accuracy on training data?
  * Looking at few neighbors can allow us to almost perfectly match the training data; looking at many forces smoother prediction
  * Transformed features let us incorporate more information

Concepts:
* Hyperparameters
* Feature Engineering
* generalization
* (Interpretability)
* Validation data, validation error
* Cross-validation


Note:: Homework on feature engineering and cross-validation

===
Topic:: Model Selection Continued

Guiding questions:
* How might these modeling decisions affect generalization?
  * we could **underfit** by not modeling real patterns in the data
  * we could **overfit** by modeling patterns that only occur in the training data
  * Type of model: is there an underlying smooth relationship, or is similarity to examples best?
  * Linreg: assumption that data *is* linear in the features
  * Are features meaningful? How can features be transformed into something more meaningful?
* How might we *measure* these impacts? What are some sources of *uncertainty* in these measures?

* I forgot to mention an important kind of feature engineering: **interactions**.

* Measure effects of modeling decisions on accuracy in training, validation, and test
  * Generalization error is a *noisy measurement*
    * We get different errors for the *same* model on different validation splits
    * We get different errors on the *same* validation data for models trained on different splits

* Bike-sharing example: When we get an accuracy score on the test set, what does that mean?
  * Ideally we'd like it to be indicating generalization: is our model going to do well at predicting future rides?
    * so that we can make decisions like which features to include, how to transform them, and what kind of model to use
    * and so that we can have appropriate confidence when making decisions based on our model's conclusions
      * like how many bikes to buy or which stations to expand
  * But what if 2012 had unusual weather? Their sports teams had big games? The subway had a major failure?
  * Upshot: test set performance is a *noisy* indicator of generalization performance

We looked at Edmunds' True Market Value tool (comparable to the Kelly Blue Book), which I used for determining what would be a fair price for the car I'm buying.
It gives you a price prediction.
We discussed where that price number came from, and concluded that it was likely based on *data* (namely, historical car title transfer data) that was used to train a *regression model*.
(We have to be careful about *outliers* in that data, e.g., parents "selling" cars to their children, manufacturer buybacks aka "lemons", etc., and probably find ways of identifying and removing these outliers.)

* Features of the regression model: 
  * car make/model
  * Year
  * Mileage
  * Condition and history (e.g., Carfax)
* We discussed many considerations about how we might engineer these features into something that a linear model might need
  * Mileage (or age) might need a non-linear transformation, since brand new cars depreciate much more rapidly
  * Make/model probably needs one-hot encodings, but thought is needed to make sure we're modeling both overall brand-level characteristics and specific trim levels of specific models
  * We might want to model *interaction effects*: some brands age better than others, so we might want a Toyota-specific mileage coefficient. We can model that using interaction terms (`make` * `mileage`).
* We discussed how we might measure errors. MSE is a pretty good option, but maybe do we want to favor *overestimating* vs *underestimating*? Or do we want to worry more about *relative* vs *absolute* accuracy?
* We discussed how to measure generalization.
  * Any specific validation set may have issues, e.g., a major reliability issue crops up with a particular vehicle that happens to be in the validation set a lot.
  * So we should use multiple validation sets.
  * But how to do that when we have a fixed set of training data?
  * So we discussed *cross-validation*: hold out some data, train on the rest, test on the held-out; repeat for different held-out data.

===10/29/2019
Due:: Visualization Workshop Prep (reading)

===
Topic:: Visualization Workshop

===10/31/2019
Due:: Project 1 Milestone: Visualization Workshop Aftermath

===
Topic:: Final Project proposal discussions

===
Topic:: Data Wrangling: Aggregation and Joining @@topics/wrangling.html


===
Topic:: Data Wrangling Continued: Coding it up

===
Topic:: Data Wrangling Lab

===11/7/19
Due:: Project 1 Final

===

* Visualization
  * Principles
  * Exploratory Analytics
  * Practice / Critique

===
Topic:: Classification @@topics/classification.html

* COMPAS as an example of a classification problems
* There are at least two ways to be wrong: false positive vs false negative. Difference matters.

===
Topic:: Discussion about fairness in classification

* Correction of Precision / Recall
* "Blind" classifiers may be unfair.

===
Topic:: Classification Lab

===
Topic:: Exploratory Analysis and Visualization (EDA) @@topics/eda_unsupervised.html

Due:: Project 2 Revised proposal

EDA is a frame of mind.

* If you were actually doing this for a job, what might you do differently?

* Review hw6, focusing on where EDA would have helped us
  * What's the range of temperature?
  * What's the range of rides?
  * How many weekdays / holidays / working days do we have?
  * What values are missing?
* Other things we could ask
  * Visualize the system network (locations?)


===
Topic:: Unsupervised learning lab 1

Bike share station data - cluster by location

===
Topic:: Unsupervised learning and EDA lab 2

Bike share data
* cluster stations by traffic patterns
* cluster days by system traffic pattern
* predict by day, look at outliers

===
Topic:: Recommender Systems and Reinforcement Learning @@topics/actions.html

===11/26/2019
Due:: Project 2 Dataset Exploration


===
(Thanksgiving)

===
(Thanksgiving)

===
Topic:: Model Selection redux (and hw/quiz review)

* Announce final assignment
* Clarify project logistics
  * Everyone should have Proj 2 Milestone 3 feedback
  * Expectations clarified: you need to do some kind of prediction or clustering.
  * Share about projects (What kind of prediction / clustering can I do?)
* Announce SQL lab Wed, prep
* Project stuff:
  * Tidy data (GDP, temperature quality codes, unclear missing-ness)
  * Validation: time series vs exchangeable (and are countries exchangeable?)
    - examples: bike ridership, car sale price, risk of extreme weather, loan defaults
    - discussion about how to validate performance of each of these models
  * Time series prediction: autoregressive (lag) features

===
Topic:: SQL @@topics/sql.html

===12/5/2019
Due:: Project 2 Initial modeling

===
Topic:: What we didn't cover

* Answer questions in Excel sheet
* Missing data
* Repeat cross-validation (perhaps motivated by a Kaggle competition?)
* Inference (e.g., bootstrapping)
* Time series
* Text features
  * easy: bag of words
  * fancy: dense representations (word2vec etc.)
* Image features
* Advanced modeling
  * predictive
    * how logistic regression works
    * tree-based
    * ensembles
    * neural Networks
  * clustering algorithms
  * reinforcement learning
* Scraping, web, APIs, regular expressions, ...
* Visualization: interactivity (see D3 interim)


===
Topic:: Beyond DATA 202, continued

* First 10 minutes: course evals
* Remind students of Cathy O'Neil talk
* Sharing interesting things students found
* Answering questions from Friday's whirlwind

Due:: Course Reflection

===12/12/2019
Note:: Final Project Presentations @ 1:30pm
Due:: Final Project Write-ups due 11:59pm

===endofcal