
# Data Wrangling

```{r message=FALSE}
library(tidyverse)
```


## Resources

First, here are [some questions to ask if you're working with data that you didn't collect yourself](http://bit.ly/quaesita_notyours).
(That article is one of my favorites from the [analytics writings by the Head of Decision Intelligence at Google](https://decision.substack.com/p/analytics-the-complete-minicourse)


For more resources, see the previous chapter but also:

* [R for Data Science: Factors](https://r4ds.had.co.nz/factors.html)
* [dplyr](https://dplyr.tidyverse.org/): [cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)
* [lubridate](https://lubridate.tidyverse.org/): [cheat sheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)

### Practice

[TidyTuesday](https://github.com/rfordatascience/tidytuesday) has weekly examples!

David Robinson, contributor to several notable R packages, has done [screencasts](https://www.youtube.com/user/safe4democracy/videos)
of analyzing many TidyTuesday examples. [Here's the code](https://github.com/dgrtwo/data-screencasts).

## SQL and BigQuery

Query languages allow us to query big datasets from our small computers. The most
popular by far is SQL.

Google's BigQuery is a SQL-like language for querying datasets stored on its
cloud infrastructure. Most of the time you'll be querying data that are internal
to your organization, but Google and other providers have published some open
datasets. Some examples:

* [NFL Play-by-Play](https://calogica.com/r/bigquery/2020/08/18/r-bigquery.html)
* [NYC Yellow-Cab Trips](https://cfss.uchicago.edu/notes/sql-databases/#interacting-with-google-bigquery-via-dplyr)
* [FiveThirtyEight analysis of subreddit relationships](https://github.com/fivethirtyeight/data/tree/master/subreddit-algebra)

## A File Per Year

When you have multiple data files containing the same data, differing only by year or the like, it's typically best to combine them together early and do all the data wrangling to the combined data frame. For example, let's *pretend* that gapminder gave us their data one year at a time, and we got data frames like:

```{r include=FALSE}
# Just pretending -- don't actually do this!
library(gapminder)
gm_1952 <- gapminder %>% filter(year == 1952) %>% select(-year)
gm_1957 <- gapminder %>% filter(year == 1957) %>% select(-year)
# etc.
```

```{r}
head(gm_1952, 3)
head(gm_1957, 3)
```

etc. We could merge those together like this:

```{r}
gm_combined <- bind_rows(
  year_1952 = gm_1952,
  year_1957 = gm_1957,
  .id = "year"
)
gm_combined
```

We'd then want to convert that `year` column into a number. Two approaches: first the lazy one:

```{r}
gm_combined %>% mutate(year = parse_number(year))
```

And then the more principled one:

```{r}
gm_combined %>% 
  separate(year, into = c(NA, "year")) %>% 
  mutate(year = as.numeric(year)) # or pass `convert = TRUE` to separate
```

For some more advanced techniques, see "[Read Multiple Files into a Single Data Frame](https://www.mjandrews.org/blog/readmultifile/)".

## Panel Survey Data (e.g., Pew Research)

Pew and other sources release data in a file format used by SPSS, a commercial
statistical analysis tool. Fortunately it's straightforward to read this data
in R, using the [`haven`](https://haven.tidyverse.org/) package.

I'll show an example with the [American Trends Panel](https://www.pewresearch.org/our-methods/u-s-surveys/the-american-trends-panel/).

### Reading SPSS files with `haven`

```{r message=FALSE}
library(tidyverse)
atp_w34 <- haven::read_sav("data/W34_Apr18/ATP W34.sav")
```

The easiest way to look at this data is to click on it in the "Environment" panel,
or run `View(atp_w34)` on the Console. (Remember not to leave a `View` call in
an Rmd when you Knit.)

You'll see that each column has a label. It might be hard to read all of them, so
here's a bit of magic code to make a table of just the column labels:

```{r}
getColumnLabels <- function(df) {
  tibble(
    name = names(df),
    label = map_chr(names(df), ~ attr(df[[.]], "label"))
  )
}
getColumnLabels(atp_w34)
```



Many of the columns are actually factors in disguise. To decode their labels,
call `as_factor`. For example, to get party affiliations and leanings from
the ATP data, we can do:

```{r}
atp_w34_wrangled <- atp_w34 %>% 
  mutate(
    party = as_factor(F_PARTY_FINAL),
    party_lean = as_factor(F_PARTYLN_FINAL),
    age = as_factor(F_AGECAT_FINAL))
atp_w34_wrangled %>% select(party, party_lean)
```

Here's a function to rename a bunch of columns to match their labels. I think it's clumsy; probably the above is better.

```{r}
name_as_label <- function(df, cols_to_rename) {
  new_names <- list()
  for (col in cols_to_rename) {
    new_names[[attr(df[[col]], "label")]] = col
  }
  df %>% rename(!!as_vector(new_names))
}

atp_w34 %>% 
  name_as_label(cols_to_rename = c("F_PARTY_FINAL", "Device_Type_W34", "LANGUAGE_W34"))
```


### Weights

Note that Pew survey data include *weights*. Read their Methodology sections for details about these weights.
Once you've identified the correct weights to use, you can use the `wt` parameter to `count`
to weight your counts accordingly, or the `weighted.mean` function if you're interested in a
specific outcome.

For example, the following gives the proportion of each party among *survey respondents*:

```{r}
atp_w34_wrangled %>% 
  count(party) %>% 
  mutate(proportion = n / sum(n))
```

while this gives the (estimated) proportion of each party in the US:

```{r}
atp_w34_wrangled %>% 
  count(party, wt = WEIGHT_W34) %>% 
  mutate(proportion = n / sum(n))
```

Add more variables to `count` to get cross-tabulations:

```{r}
atp_w34_wrangled %>% 
  count(party, age, wt = WEIGHT_W34) %>% 
  group_by(age) %>% # Get party membership within each age range
  mutate(proportion = n / sum(n))
```




## Afterward

* [Arquero](https://observablehq.com/@uwdata/introducing-arquero) is a new
  JavaScript library that uses almost all of the same basic concepts
  of the Grammar of Data, though sometimes with different names.
