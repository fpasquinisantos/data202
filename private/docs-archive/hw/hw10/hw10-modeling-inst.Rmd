---
title: "HW 10: Predictive Modeling"
output: 
  tufte::tufte_html:
    css: ../hw.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidymodels)
theme_set(theme_bw())
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

## Goals

* Apply predictive modeling using `tidymodels` to a time-series prediction task
* Use visualization to understand the strengths and weaknesses of different models
* Compare and contrast linear regression and decision trees

We'll again be using the Capital Bike Share dataset. In an earlier assignment we wrangled this
raw data ourselves, but we'll go back to the pre-wrangled data we worked with
before since our focus here is on the modeling.

## Getting Started

Pull your portfolio repo and find the template in the `hw10` folder.

Your template includes a utility function called `add_predictions`. It's just a
convenience shortcut for the `predict %>% bind_cols` pipeline we use to add 
predictions to a model; you're welcome to just use that and ignore this utility.

To use `add_predictions`, you pipe in a dataset and pass one or more 
models as arguments:

```r
data %>% add_predictions(model1, model2, model3)
```

It makes a tidy data frame with the predictions of each model for each data point,
and a column called `model` that gives the name of the model. Here is the source
code (but it's already in your template):

```{r echo=TRUE}
add_predictions <- function(data, ...) {
  purrr::imap_dfr(
    rlang::dots_list(..., .named = TRUE),
    function(model, model_name) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}
```



### Load data

```{r preproc, include=FALSE}
read_csv("../../data/bikeshare/day.csv", col_types = cols(
  .default = col_double(),
  dteday = col_date()
)) %>%
  mutate(
    yr = as_factor(2011 + yr),
    weekday = lubridate::wday(dteday, label = TRUE),
    workingday = if_else(workingday == 1, "workday", "weekend"),
    temp = temp * (39 + 8) - 8,
    atemp = atemp * (50 + 16) - 16
  ) %>%
  rename(
    year = yr,
    date = dteday
  ) %>% write_csv("data/day-hw10.csv")
file.copy("data/day-hw10.csv", "../../../src/hw/hw10-modeling/data/day-hw10.csv")
```

```{r load-data, echo=TRUE}
daily_rides <- read_csv("data/day-hw10.csv", col_types = cols_only(
  date = col_date(),
  year = col_integer(),
  workingday = col_character(),
  temp = col_double(),
  atemp = col_double(),
  casual = col_double(),
  registered = col_double()
)) %>% mutate(across(c(workingday, year), as_factor))
```


### Exploratory Analytics

Make a scatter plot of the number of `casual` riders by `date`, color-coding
whether each day is a `workingday` or not.

```{r casual-vs-day-eda}
daily_rides %>%
  ggplot(aes(x = date, y = casual, color = workingday)) +
  geom_point()
```

### Train-test split

Split this data into a training set (call it `train`) and a testing set (call it
`test`). Use `year` of 2011 as the `train`ing set and 2012 as the `test`ing set.
To do this, just `filter` by year; don't use `rsample::initial_split` as we did
before.


```{marginfigure, echo=TRUE}
This is actually a *time series* prediction problem, sometimes just called *forecasting*.
Methods that are competitive for forecasting almost always look at recent history,
not just general trends. There are ways of adapting the models we use here to
do that, but we will keep it simple for now.

One aspect of time series prediction bears noting here, though: we need to be
mindful of how we evaluate performance. 
If we picked days at random to evaluate on, most of the evaluation days would
be within the time we've already seen. So we would be overconfident in our model's
ability to predict *future* ridership.

The `initial_time_split` function can be used for this purpose if there is not a
nice division by year like we have here.

More discussion of time series validation approaches can be found [here](https://otexts.com/fpp3/tscv.html).
```

```{r train-test-split}
train <- daily_rides %>% filter(year == 2011)
test <- daily_rides %>% filter(year == 2012)
```

Check that you get `r nrow(train)` training set days and `r nrow(test)` test set days. (why might this make sense?)

## Linear Regression using Temperature

Fit a linear regression model to predict the number of `casual` riders from the `temp`erature.
Use the training dataset (2011). Call the model `model1`.

```{r}
model1 <- linear_reg() %>%
  fit(casual ~ temp, data = train)
```

### Look inside the model

We can use the `tidy` function to get the model's coefficients:

```{r echo=TRUE}
model1 %>%
  tidy() %>%
  select(term, estimate)
```

Complete this sentence: "For every additional degree C, model1 predicts ___ additional riders."

### Predictions

Plot the model's predictions along with the number of casual rides. Do this by piping `train` into `add_predictions(model1)`
to make the predictions, then plot the result. You'll want to add a `geom_line`
where you specify `.pred` as the `y` aesthetic.

(*Very Optional*: For this plot, and only this plot, the result will look cleaner if you specify
a `color` for `geom_line` outside of the aesthetic mapping.)

```{r show-model1}
train %>%
  add_predictions(model1) %>%
  ggplot(aes(x = date, y = casual, color = workingday)) +
  geom_point() +
  geom_line(aes(y = .pred), color = "maroon", size = 1)
```

This is a linear model, so generally we expect to see straight lines. Why is
the maroon line not a straight line? *Hint*: you may want to try making another
plot, corresponding to the formula we used to fit this model.

### Residuals Histogram

Make a histogram of the residuals, faceted by weekend vs weekday. (The example
uses a `binwidth` of 100 and a `geom_vline` at 0.)

```{r resid-histogram-model1}
train %>%
  add_predictions(model1) %>%
  mutate(resid = casual - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_vline(xintercept = 0) +
  facet_wrap(vars(workingday), nrow = 2, scales = "free_y")
```

### Observed by Predicted

Make an observed-by-predicted plot. Add `coord_obs_pred()` to make the axes
exactly equal, and `geom_abline` to draw the line corresponding to exactly
correct predictions.

```{r observed-by-predicted-model1}
train %>%
  add_predictions(model1) %>%
  ggplot(aes(x = casual, y = .pred, color = workingday)) +
  geom_abline() +
  geom_point(alpha = .5) +
  coord_obs_pred()
```

### Analysis

Under what circumstances does this this model typically predict too high? Under
what circumstances does it predict too low? Use the plots above to justify your answer. 

Try to guess why might the model have done that.

### Validate the model on the test set

Now let's see how this model predicts on data it hasn't seen. Make the same
plots as above, but with the full `daily_rides` dataset
(so now we have 2012 also.)

#### Predictions

```{r show-model1-test}
daily_rides %>%
  add_predictions(model1) %>%
  ggplot(aes(x = date, y = casual, color = workingday)) +
  geom_point() +
  geom_line(aes(y = .pred), color = "maroon", size = 1)
```

#### Residuals

For this plot, facet on `year` instead of `workingday`, to compare the training set and test set.

```{r}
daily_rides %>%
  add_predictions(model1) %>%
  mutate(resid = casual - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_vline(xintercept = 0) +
  facet_wrap(vars(year), nrow = 2)
```

#### Observed by Predicted

Again, color by `year` instead of `workingday`.

```{r}
daily_rides %>%
  add_predictions(model1) %>%
  ggplot(aes(x = casual, y = .pred, color = year)) +
  geom_abline() +
  geom_point(alpha = .5) +
  coord_obs_pred()
```

### Quantify errors

Use the `mae` function, or write your own `summary` call, to compute the Mean
Absolute Error for each year (2011 and 2012).

```{r}
daily_rides %>%
  add_predictions(model1) %>%
  group_by(year) %>%
  summarize(mae = mean(abs(casual - .pred)))
#  mae(truth = casual, estimate = .pred)
```

How incorrect was the prediction, on average, for each year?

#### Summarize

Write one-sentence responses for the following:

* How well does this model perform on the training set?
* How does its test set performance compare with its training set performance?
* Compare the "quantify errors" table above with the plots of model performance.
  * What can you easily observe from the plots but not the table?
  * What can you easily observe from the table but not the plots?

## Linear Regression using Temperature and Working Day

Since it seems that `workingday` makes a big difference in the number of casual
riders, let's include it in our model. It's a categorical variable, so we'll
need to dummy-encode it.

Dummy encoding will only give us a conditional *intercept*. If we want our 
*slope* to be conditional also, we need to make an interaction term. We
can do that using `step_interact`. The code is non-obvious and not the point
of this exercise, so it's included below. Note that the code below also
uses a `workflow` to combine the model and the recipe.

```{r echo=TRUE}
recipe2 <-
  recipe(casual ~ temp + workingday, data = train) %>%
  step_dummy(workingday) %>%
  step_interact(~ temp:starts_with("workingday"))

model2 <- workflow() %>%
  add_recipe(recipe2) %>%
  add_model(linear_reg()) %>%
  fit(train)
```

Let's have a look at this model's coefficients.

```{r echo=TRUE}
model2 %>% tidy() %>% select(term, estimate)
```

```{r include=FALSE}
stopifnot("workingday_workday" %in% (model2 %>% tidy() %>% pull(term)))
```

(No answer needed, but: why might the coefficient by named `workingday_workday`?)

Plot the predictions of this model by date. Again, color by working day;
this time the `color` aesthetic will also apply to the predictions line
so we'll get two separate lines.

```{r}
train %>%
  add_predictions(model2) %>%
  ggplot(aes(x = date, y = casual, color = workingday)) +
  geom_point() +
  geom_line(aes(y = .pred), size = 1)
```

Now let's make a plot of what predictions this model makes. The code is a bit
tricky here too, so here's part of it: you'll need to add a `geom_line`
that specifies its own `data` and `mapping`.

```r
  geom_line(
    data = expand_grid(
      workingday = levels(train$workingday),
      temp = modelr::seq_range(train$temp, n = 100)
    ) %>%
      add_predictions(model2),
    mapping = aes(y = .pred)
```

```{marginfigure, echo=TRUE}
Since the prediction is a straight line, you can also get a similar plot by
using `add_predictions` to `train`. That won't work when the line isn't straight,
though, so we need this extra code.
```


```{r}
train %>%
  ggplot(aes(x = temp, y = casual, color = workingday)) +
  geom_point() +
  geom_line(
    data = expand_grid(
      workingday = levels(train$workingday),
      temp = modelr::seq_range(train$temp, n = 100)
    ) %>%
      add_predictions(model2),
    mapping = aes(y = .pred)
  )
```

Finally, quantify `model2`'s performance like you did before with `model1`.

```{r}
daily_rides %>%
  add_predictions(model2) %>%
  group_by(year) %>%
  mae(truth = casual, estimate = .pred)
```


## Decision Tree Regression

Now try using a `decision_tree` to predict `casual` rides from `temp` and `workingday`.
Use the default settings.

```{r}
model3 <-
  decision_tree(mode = "regression") %>%
  fit(casual ~ temp + workingday, data = train)
```

We can show the tree using the same method we used in class and lab:

```{r echo=TRUE}
model3 %>%
  extract_fit_engine() %>% 
  rpart.plot::rpart.plot(roundint = FALSE, digits = 3, type = 4)
```

Plot its predictions by date (the `train %>% add_predictions` approach
works again here).

```{r}
train %>%
  add_predictions(model3) %>%
  ggplot(aes(x = date, y = casual, color = workingday)) +
  geom_point() +
  geom_line(aes(y = .pred), size = 1)
```

Finally, quantify its performance.

```{r}
daily_rides %>%
  add_predictions(model3) %>%
  group_by(year) %>%
  mae(truth = casual, estimate = .pred)
```

Let's make a plot to compare all 3 models' performance. You can get the predictions
of all 3 models by using `daily_rides %>% add_predictions(model1, model2, model3)`.

```{r}
daily_rides %>%
  add_predictions(model1, model2, model3) %>%
  group_by(model, year) %>%
  summarize(mae = mean(abs(casual - .pred))) %>%
  ggplot(aes(x = model, y = mae, fill = year)) +
  geom_col(position = "dodge")
```

## Wrap-up

Write brief answers to the following questions:

* What differences did you observe between the linear regression and
  decision tree models?
* What differences did you observe between the accuracy of these models on
  2011 data vs 2012?
* Based on the graphs you made, what might explain both of these differences?
