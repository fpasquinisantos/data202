---
title: "Final Homework"
output: 
  tufte::tufte_html:
    css: ../hw.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
#library(mclust)
theme_set(theme_bw())
```

This homework is to wrap up our last few units with some hands-on practice.
It focuses on breadth rather than depth. They are very strongly based on exercises from the textbook.
Submit a well-formatted and reproducible R Markdown document that shows the process and clearly specifies the answer.

This homework should be completed individually, but you are heartily invited to ask questions on Piazza.

A template for this assignment is found in the `hwFinal` folder of your portfolio repo.

## Supervised Learning

(*MDSR2e Exercise 11.4*) Build a decision tree classifier for the `type` of each storm in `nasaweather::storms` based on its `wind` speed and `pressure`.
Report its accuracy.

**Note**: there is also a `storms` data frame in `dplyr`. Use the one in `nasaweather`.

```{r}
storms <- nasaweather::storms %>% mutate(type = as_factor(type))
head(storms)
ggplot(data = storms, aes(x = pressure, y = wind)) +
  geom_point(data = storms %>% select(-type), alpha = .25, color = "grey") +
  geom_point(alpha = .25, position = position_jitter(.1), color = "green") +
  facet_wrap(vars(type))
```

```{r include=FALSE}
storms %>% count(type)
nrow(storms)
set.seed(0)
storms_split <- initial_split(storms, prop = 9/10)

storm_model <- decision_tree(mode = "classification") %>% 
  fit(type ~ wind + pressure, data = training(storms_split))
storm_model %>%
  predict(testing(storms_split)) %>% 
  bind_cols(testing(storms_split)) %>% 
  summarize(accuracy = mean(.pred_class == type))
```

```{r include=FALSE, eval=FALSE}
# Demonstrate the problem in https://github.com/tidymodels/parsnip/issues/621
class(dplyr::storms$status)
parsnip::fit(
  parsnip::decision_tree(mode = "regression"),
  status ~ wind,
  data = dplyr::storms)
```


## Clustering

(*MDSR Exercise 12.1*) Consider the 4,000 biggest cities in the world, given by:

```{r}
big_cities <- mdsr::world_cities %>% 
  arrange(desc(population)) %>% 
  slice_head(n = 4000)
```

Construct a *k*-means clustering of the `latitude` and `longitude` of these cities.
Describe (qualitatively) the results of clustering with *k*=2.

```{r include=FALSE}
set.seed(0)
clustering_results <- big_cities %>% 
  select(latitude, longitude) %>% 
  kmeans(centers = 2) %>% 
  fitted("classes") %>% 
  as_factor()
big_cities %>% 
  mutate(cluster = clustering_results) %>% 
  ggplot(aes(x = longitude, y = latitude, color = cluster)) +
    geom_point()
```


## Databases

(*MDSR Exercise 15.9c*) Which baseball players have hit 500 home runs (`HR`) OR 3000 hits (`H`) but have not (yet?) been inducted into the Baseball Hall of Fame?

Use the `Lahman` package.

```{r}
library(Lahman)
```

Consider:

- the `inducted` column of `HallOfFame` 
- the `HR` and `H` columns of `Batting` (note that there are *many rows for each `playerID`*)
- the `nameFirst` and `nameLast` columns of `People`

```{r include=FALSE}
Lahman::Batting %>% 
  group_by(playerID) %>% 
  summarize(career_H = sum(H), career_HR = sum(HR)) %>% 
  filter(career_HR >= 500 | career_H >= 3000) %>% 
  anti_join(Lahman::HallOfFame %>% filter(inducted == "Y"), by = "playerID") %>% 
  left_join(Lahman::People %>% select(playerID, nameFirst, nameLast), by = "playerID")
```

```{r include=FALSE}
# This is a more complex version to illustrate some of the ways that you can get it wrong,
# e.g., by assuming that a N vote in HoF implies that the player was *never* inducted,
# when in fact some players just took several times.
hof_summary <- Lahman::HallOfFame %>%
  group_by(playerID) %>%
  summarize(
    hof = "Y" %in% inducted,
    num_fails_before_success = sum(inducted == "N"),
    num_ballots = length(inducted))
Lahman::Batting %>% 
  group_by(playerID) %>% 
  summarize(career_H = sum(H), career_HR = sum(HR)) %>% 
  left_join(hof_summary, by = "playerID") %>% 
  mutate(hof = coalesce(hof, FALSE)) %>% 
  filter(career_HR >= 500 | career_H >= 3000, !coalesce(hof, FALSE)) %>% #coalesce(num_fails_before_success, 9999) > 0) %>% 
  #anti_join(Lahman::HallOfFame %>% filter(inducted == "Y")) %>% 
  left_join(Lahman::People %>% select(playerID, nameFirst, nameLast), by = "playerID") %>% 
  #arrange(desc(num_fails_before_success))
  arrange(playerID)
```

## Text Data

1. (*MDSR Exercise  19.1a*) How many speaking lines are there in Macbeth? Speaking lines are identified by a line that starts with two spaces, then a string of capital letters (possibly including spaces) indicating the character's name, followed by a period.

```{r include=FALSE}
macbeth <- mdsr::Macbeth_raw %>% 
  stringi::stri_split_lines1()
macbeth %>% str_subset("^\\s{2}[A-Z ]+\\. ") %>% length()

# Close but incorrect results:
# 646 for not requiring a space after the period:
macbeth %>% str_subset("^\\s{2}[A-Z ]+\\.") %>% length()
# 1140 for forgetting that `.` matches *any* character
macbeth %>% str_subset("^\\s{2}[A-Z ]+. ") %>% length()
# 477 for not allowing spaces in character names (e.g., "FIRST WITCH")
macbeth %>% str_subset("^\\s{2}[A-Z]+\\. ") %>% length()
```


2. (*MDSR Exercise 19.4a*) Find the 10 most popular boys' names in 2017 that end in a vowel. Use the `babynames::babynames` table. (Hint: `str_detect`.)

```{r include=FALSE}
babynames::babynames %>% 
  filter(year == 2017, sex == "M", str_detect(name, "[aeiou]$")) %>% 
  arrange(desc(n)) %>% 
  head(10) %>% 
  select(name, n, prop)
```

