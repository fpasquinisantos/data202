---
title: "Guided Exercise 11 - Clustering"
output: 
  tufte::tufte_html:
    css: ../ex.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

<!-- Next year: -->
<!-- Maybe start by having students figure out what number to divide by to put everything on the same scale. -->
<!-- - 0-1 scaling for Gr_Liv_Area didn't make enough of a difference for students -->
<!-- - Provide a mechanism for students to see the effective ranges -->
<!-- - Provide a visual explanation of the sum-of-squares, or just drop it. -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
library(tidyverse)
library(tidymodels)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

The goal of this exercise is to practice with clustering. Objectives:

- Identify application areas for clustering
- Contrast supervised learning with unsupervised learning
- Predict the effect of changes in distance metrics on clustering results

## Getting started

Pull your portfolio repo as usual, and we'll be working with the home sale data as usual.

```{r load-and-subset-data}
# Get the data from the "modeldata" package, which comes with tidymodels.
data(ames, package = "modeldata")
ames_all <- ames %>% 
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>% 
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

Let's do a train-test split again as usual. It's not as important for unsupervised
analysis, but if we get an idea about some pattern in the data and want to
check whether it's real, it'll be helpful to have data we haven't peeked at.
We can get away with a smaller test set, though (so use 4/5 for training).

```{r train-test-split}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames_all, prop = 4 / 5) # Split our data randomly
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```


## Clustering

We first need to define what data we want the clustering to use. Why might it not make
sense to use all of the data?

1. Some kinds of differences are more interesting or important than others!
1. The numeric columns are all on different scales.
1. We might or might not want to look at an "outcome" column like `Sale_Price`.
1. Many columns are categorical, so we'd need to define what "distance" means for those.

etc. So let's define the data we want to use for clustering manually.

We'll start with lat/long, but we'll revisit this decision many times.

```{r prep-clustering}
data_for_clustering <- ames_train %>% 
  select(Latitude, Longitude)
```

Now we cluster the data. We'll set a random *seed* so that the random initialization
is reproducible (although with 10 random restarts it's unlikely to make a difference in practice).

We'll ask for 3 clusters for now. Again, we'll come back and revisit this later.

```{r make-clusters}
set.seed(20211119)
clustering_results <- data_for_clustering %>% 
  kmeans(nstart = 10, centers = 3)
```

Finally, let's add the cluster assignment back to the original data so we can visualize.
(Don't worry about this syntax, although you can probably figure out what it does.)

```{r augment-clusters}
ames_with_clusters <- ames_train %>% 
  mutate(cluster = as.factor(clustering_results$cluster))
```

We can use a `glance` at the clustering results to get an overall quantitative picture
of how the clustering did. It tells us three different "sums of squares" (`ss`).
By that, it means we're looking at the *distances* between pairs of observations, *squaring* each distance,
and summing up what we get.

Consider all pairs of observations. They'll be one of two kinds:

1. Both obs are in the same cluster (`withinss` = sum of squares *within* a cluster), or
2. They are in two different clusters (`betweenss` = sum of squares *between* clusters).

A good clustering would have low distances *within* a cluster and high distances *between* clusters.
**Try increasing the number of clusters. How does the total within-cluster distance (`tot.withinss`) change?**


```{r glance-clustering-results}
glance(clustering_results)
```

Now we look at the clusters themselves.

```{r tidy-clustering-results}
tidy(clustering_results)
```

Let's plot that. We're using a neat library called `patchwork` that lets us
put `ggplot` plots side by side. Basically we just make the plots and instead of showing
them immediately we assign them to variables... and then we can `+` them together 
to put them side-by-side, or `/` them to put them top and bottom. See the [`patchwork` docs](https://patchwork.data-imaginist.com/index.html)
for details.


```{r cluster-plots, fig.asp=0.4}
latlong_plot <- 
  ggplot(ames_with_clusters, aes(y = Latitude, x = Longitude, color = cluster)) +
    geom_point(alpha = .5)

year_area_plot <- 
  ggplot(ames_with_clusters, aes(x = Gr_Liv_Area, y = Year_Built, color = cluster)) +
    geom_point(alpha = .5)

library(patchwork)
latlong_plot + year_area_plot + plot_layout(guides='collect')
```

## Exploring Parameter Settings

Your turn:

::: {.exercises}

1. What differences do you notice between the plot on the left and the plot on the right?
1. Try increasing the number of `centers`. What changes about both plots?
2. Use only `Year_Built` for clustering (removing latitude and longitude). What can you say about the age of homes in different parts of town?
2. Try clustering using `select(Latitude, Longitude, Gr_Liv_Area)`. What changes about both plots?
  Why are they different?
2. Try scaling `Gr_Liv_Area` to have a maximum of 1 using `Gr_Liv_Area = rescale(Gr_Liv_Area, to = c(0, 1))` etc. What changes about both plots? Why?
2. Try adding scaling for `Latitude` (but not `Longitude`). What changes and why?
2. Now add scaling for for `Longitude`. What changes and why?
2. Try changing the maximum to `10` for `Gr_Liv_Area`. Then try `0.1`. What changes and why?
2. Try adding `Year_Built`.

:::


## Relating to sale price

Do the patterns captured by these clusters also happen to relate to sale price?

```{r sale-price-by-cluster, fig.asp=0.4}
ames_with_clusters %>% 
  ggplot(aes(x = Sale_Price, y = cluster)) + geom_boxplot()
```


```{r include=FALSE, eval=FALSE}
# This is a way to show lots of clusters.
ggplot(ames_with_clusters, aes(y = Latitude, x = Longitude, color = cluster)) +
    geom_point(data = select(ames_with_clusters, -cluster), color = "grey") +
    geom_point(alpha = .5) +
  facet_wrap(vars(cluster)) + 
  guides(color = "none")
```

