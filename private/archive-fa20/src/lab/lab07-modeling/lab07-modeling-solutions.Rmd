---
title: "Lab 07: Making Predictive Models"
author: "TEAM"
date: "DATE"
output: github_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
 options(scipen = 5) # encourage metrics to print in fixed-point notation
```


## Data

We'll be using the Ames home sales dataset, as we have been using this past week.
Again, see [Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt).

I re-read the [original paper](http://jse.amstat.org/v19n3/decock.pdf)
and found that the author suggests working with a
subset of the data. So let's do that:

```{r load-and-subset-data}
# Get the data from the "modeldata" package, which comes with tidymodels.
data(ames, package = "modeldata")
ames <- ames %>% 
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal")
```

1. In your report, use inline code to report how many homes are left after this filtering.

## Exploratory analysis

Good analysis starts by exploring the data. One compelling way to explore data
is by making plots. Let's make one together.

2. Make a plot of how the sale price related to the number of square feet of above-grade living area.

Use the data documentation to find the appropriate column. Note that spaces
in variable names have been replaced by underscores in our data.

Your graph might look something like this:

```{r eda-living-area, echo=FALSE}
ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = .1, alpha = .25) +
  labs(x = "above-grade living area (ft^2)", y = "Sale Price ($)")
```

## Modeling

We will now go through the basic steps of making a predictive model. We will add
on to this workflow later, but this is a good start.

### Hold out some data to use for validation

First, let's set aside some data that we won't show to our model. We do this so
that we can estimate how well it will do at predicting in situations that we
*actually* haven't seen.

```{r train-test-split}
# Seed the random number generator
set.seed(10)
# Split our data randomly
ames_split <- initial_split(ames, prop = 2/3)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

3. Discuss the following questions with your team; you only need to write the specific answer indicated.

* What does `set.seed` do? Write one or two reasons why it might be a good idea.
* How many observations are in the training set? The testing set? Write inline R code to report these.
* Besides the given proportion, what might be other reasonable proportions? Give one example of
  an *un*reasonable proportion, and describe why that might be unreasonable.

### Specify the model

We want a linear regression, which we'll fit with the `lm` ("linear model") engine that's built-in to R.

```{r}
my_model_spec <- 
  parsnip::linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")
```

The default `linear_reg` model attempts to minimize the Mean Squared Error, with
no additional constraints.

### Fit the model on data

To fit the model, we need two things:

* A **formula** that tells us which columns to use for features and which to use as the outcome.
  * Let's try to predict `Sale_Price` given the above-grade living area (`Gr_Liv_Area`), as we plotted above.
* The **data** to try to fit
  * We will use the **training data**. (**NEVER** fit your model on your testing data!)

```{r}
my_trained_model <-
  my_model_spec %>% 
  fit(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
my_trained_model
```

3. Discuss the following questions with your team; write a succinct summary of your observations.

* How many **coefficients** do you see? What does each one correspond to?
* Try re-running the `fit`. Do the coefficients change?
* Try re-running the `initial_split` and then the `fit`. Do the coefficients change?
* Try changing the seed, then re-running. What changes?

### How well did it do?

We can tell the model to make predictions on data that it saw already (the **training set**):

```{r}
my_trained_model %>% 
  predict(ames_train)
```


4. Discuss the following questions with your team; write a succinct summary of your observations.

* How many predictions did it make? Why might that make sense?
* Look at the number that the model predicted for the first home sale. What would
  you type into a calculator in order to get that number? You will need to look
  at both the coefficients and the data (`ames_train`).

How good were those predictions? To be able to compare the actual sale prices
with the predicted prices, we need to get the two things side-by-side in the
same table. To do that, we can use `bind_cols`:

```{r}
my_trained_model %>% 
  predict(ames_train) %>% 
  bind_cols(ames_train %>% select(Sale_Price))
```
Then we can use the `yardstick` package to compute several different metrics for how 
those two columns compare.

```{r}
my_trained_model %>% 
  predict(ames_train) %>% 
  bind_cols(ames_train %>% select(Sale_Price)) %>% 
  yardstick::metrics(truth = Sale_Price, estimate = .pred)
```

* Why does this table have 3 rows? Write an English sentence for each row. (You may need to look at the `yardstick` package documentation if you can't guess what the metric abbreviations mean.) (Ignore the ".estimator" column for now.)
* Finish the sentence: "On average, this model's predictions were off by about $___ on its training data".


### How well did it do... on *unseen* data?

Now have the model `predict` on the test set and compute your estimates.
```{r}
my_trained_model %>% 
  predict(ames_test) %>% 
  bind_cols(ames_test) %>% 
  metrics(truth = Sale_Price, estimate = .pred)
```

### Is that good?

Finish the sentence: "On average, this model's predictions were off by about $___".

Can we do better?

* What data about a house did this model use to make its prediction?
* What kinds of computations could it do to that data?

## More features

5. Try training and validating (on the testing set) another model: still use
a linear model, but add in the `Lot_Area` and the number of `Full_Bath`s.

Did the model's performance on unseen data improve or not? Which error metric were you looking
at to conclude that? On what data frame was that metric computed? 

```{r train-model-2, include=FALSE}
my_trained_model_2 <- my_model_spec %>% 
  fit(Sale_Price ~ Lot_Area + Gr_Liv_Area + Full_Bath, data = ames_train) #<<
```

```{r valid-model-2, include=FALSE}
my_trained_model %>% 
  predict(ames_test) %>% 
  bind_cols(ames_test) %>% 
  metrics(truth = Sale_Price, estimate = .pred)
```

## LOTS more features!

Now, let's "throw in the kitchen sink": we'll use all the features!

Well, we haven't discussed how to use categorical features like `Neighborhood`
yet. But at least let's use all the *numeric* features, because why not?

Here's how to get a list of *all* the numeric features in formula-like format:

```{r get-all-numerics}
ames %>%
  select(where(is.numeric), -Sale_Price) %>% # Get all the numeric columns except the outcome
  names() %>% # get column names
  paste0(collapse = " + ") # combine them together with `+`s
```

6. Repeat the exercise above, but now using all of those features.

```{r train-model-3, include=FALSE}
my_trained_model <- my_model_spec %>% 
  fit(Sale_Price ~ Lot_Frontage + Lot_Area + Year_Built + Year_Remod_Add + Mas_Vnr_Area + BsmtFin_SF_1 + BsmtFin_SF_2 + Bsmt_Unf_SF + Total_Bsmt_SF + First_Flr_SF + Second_Flr_SF + Gr_Liv_Area + Bsmt_Full_Bath + Bsmt_Half_Bath + Full_Bath + Half_Bath + Bedroom_AbvGr + Kitchen_AbvGr + TotRms_AbvGrd + Fireplaces + Garage_Cars + Garage_Area + Wood_Deck_SF + Open_Porch_SF + Enclosed_Porch + Three_season_porch + Screen_Porch + Pool_Area + Misc_Val + Mo_Sold + Year_Sold + Longitude + Latitude, data = ames_train)
```

```{r eval-model-3, include=FALSE}
my_trained_model %>% 
  predict(ames_test) %>% 
  bind_cols(ames_test) %>% 
  metrics(truth = Sale_Price, estimate = .pred)
```
