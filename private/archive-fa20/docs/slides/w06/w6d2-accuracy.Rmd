---
title: "What makes a good prediction?"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives

* Compare and contrast regression tasks and classification tasks, and give examples of each
* Identify two different ways of measuring accuracy for regression and for classification
* Identify several reasons why a model may predict better on some subsets of data than others

---

## Types of Tasks

* **regression**: predict a *number* ("continuous")
  * number should be "close" in some sense to the correct number
* **classification**: predict a *category*
  * which one of these two groups? three groups? 500,000 groups?
  * could ask: "how likely is it to be in group *i*"

---

## Are these tasks *regression* or *classification*?

1. Is this a picture of the inside or outside of the restaurant?
1. How much will it rain in GR next year?
1. Is this person having a seizure?
1. How much will this home sell for?
1. How much time will this person spend watching this video?
1. How big a fruit will this plant produce?
1. Which word did this person mean to type?
1. Will this person "Like" this post?

---

## Today's examples

**Regression**: housing prices in Ames, Iowa. Details:

* [Paper](http://jse.amstat.org/v19n3/decock.pdf)
* [Data Dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)

**Classification**: *seizure classification*.

First FDA-approved AI-powered medical device: Empatica [Embrace2](https://www.empatica.com/embrace2/),
company founded by MIT data scientist Rosalind Picard

```{r echo=FALSE, out.width="20%"}
include_graphics("https://www.empatica.com/assets/images/embrace/features_em2_mb_a-lg-xhdpi.png")
```


---

## What makes a good prediction? *Regression*

We predicted the home would sell for $250k. It sold for $200k. Is that good?

--

* **residual**: actual minus predicted
  * If home sold for $200k but we predicted $250k, residual is _______
* **absolute error**
* **squared error**

--

Across the entire dataset:

* **average error**: do we tend to predict too high? too low? "*bias*"
* **max** absolute error
* **mean** absolute error
* **mean squared error** (MSE)
* normalized squared error: MSE / Variance
  * The confusingly named "R2" = 1 - normalized squared error

---

## What makes a good prediction? *Classification*

Suppose: every minute, the armband decides whether a seizure is occurring

<br>

The child was perfectly fine but our armband flagged a seizure. Is that good?

--

<br>

The child was having a seizure but our armband didn't flag it. Is that good?

---

## What makes a good prediction? *Classification*

|                      | Seizure happened              | No seizure happened           |
|----------------------|-------------------------------|-------------------------------|
| Seizure predicted    | True positive                 | False positive (Type 1 error) |
| No seizure predicted | False negative (Type 2 error) | True negative                 |

--
- **Accuracy** (% correct) = (TP + TN) / (# episodes)
- **False negative** ("miss") **rate** = FN / (# actual seizures)
- **False positive** ("false alarm") **rate** = FP / (# true non-seizures)

--
- **Sensitivity** ("true positive rate") = TP / (# actual seizures)
  - Sensitivity = 1 − False negative rate
- **Specificity** ("true negative rate") = TN / (# actual seizures)
  - Specificity = 1 − False positive rate
- [Wikipedia article](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

.question[
If you were designing a seizure alert system, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision? 
]

---

class: middle, center

## Validation

.large[**Key point**: you *must* evaluate predictions on *unseen* data]

---
```{r include=FALSE}
ames <- AmesHousing::make_ames()
data1 <- ames %>% slice_head(n = 2) %>% select(Lot_Area, Sale_Price)
data2 <- ames %>% slice_head(n = 3) %>% select(Lot_Area, Bsmt_Unf_SF, Sale_Price)
```

Hey look! I can exactly predict how much a home will sell for!

```{r include=FALSE}
m1 <- lm(Sale_Price ~ Lot_Area, data = data1)
intercept <- coef(m1)[["(Intercept)"]]
coef_lot_area <- coef(m1)[["Lot_Area"]]
```

.small[
```{r echo=FALSE}
data1 %>% knitr::kable()
```
]

sale price = `r format(intercept, scientific = FALSE)` + `r coef_lot_area` * lot area

```{r perfect-prediction-1, out.width="60%", echo = FALSE}
ggplot(data1, aes(x = Lot_Area, y = Sale_Price)) + geom_point() + geom_abline(intercept = intercept, slope = coef_lot_area)
```

---

## Validation: *unseen* data

.pull-left[
.small[
```{r echo=FALSE}
data2 %>% select(Lot_Area, Sale_Price) %>% knitr::kable()
```
]]

.pull-right[
```{r perfectly-wrong, out.width="100%", echo = FALSE}
ggplot(data2, aes(x = Lot_Area, y = Sale_Price)) +
  geom_point(size = 4) +
  geom_abline(intercept = intercept, slope = coef_lot_are) +
  geom_point(data = data2 %>% slice_tail(n=1), color = "red", size = 4)
```
]
--

```{r echo=FALSE}
broom::augment(m1, newdata = data2) %>% select(-Bsmt_Unf_SF) %>% rename(predicted = .fitted, residual = .resid) %>% knitr::kable()
```

---

## Oh ok, I'll just fix that one...

.small[
```{r echo=FALSE}
data2 %>% knitr::kable()
```
]


```{r include=FALSE}
m2 <- lm(Sale_Price ~ Lot_Area + Bsmt_Unf_SF, data = data2)
intercept <- coef(m2)[["(Intercept)"]]
coef_lot_area <- coef(m2)[["Lot_Area"]]
coef_bsmt_unf_sf <- coef(m2)[["Bsmt_Unf_SF"]]
```


sale price = `r format(intercept, scientific = FALSE)` + `r coef_lot_area` \* lot area + **`r coef_bsmt_unf_sf` \* basement sq ft**

### and look, it works!

```{r echo=FALSE}
broom::augment(m2, newdata = data2) %>% rename(predicted = .fitted, residual = .resid) %>% knitr::kable()
```

*Do you really think so?*

---

## Failure to generalize

Predictive models almost always do better on the data they're trained on than anything else.

Why?

* model uses a pattern that only held by chance
* model uses a pattern that only holds for some data
* model uses a pattern that's real but got a fuzzy picture of it

General name: **Overfitting**
