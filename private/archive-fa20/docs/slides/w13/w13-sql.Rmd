---
title: "Databases and Data Formats"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(leaflet)
```


## Q&As

> How can we control our personal use of tech when it's autonomously engineering
itself for maximum attention?

* As individuals: Set boundaries (a "rule of life"), and times of reflection ("examen"). Authors: [Justin W. Earley](https://www.thecommonrule.org/), James K.A. Smith, Andy Crouch.
* As developers
  * Suggest topics/questions instead of specific videos? 
  * Develop UI for helping people *reflect on* and *control* their engagement 
    ("you often click on X, Y, and Z. How many minutes per day do you want to spend watching videos about X?")

> Do companies have ethics requirements?

Many do, but they can be vague. Everyone needs to be responsible. ACM Code of Ethics.


---

## Q&As

> How has DS changed recently?

* *Much* more data, *much* more compute.
* Covid: data-inspired decisions are shaping our lives.

---

## Q&As

> My thoughts on social media censoring?

* Some censorship seems essential (child porn etc.)
* Some choices we have to make:
  * Subreddits (moderators decide) vs Twitter (committee decides)?
  * Control at the source (Twitter) vs recommendations/ads (Facebook)
  * Where does responsibility lie: publisher? platform? algorithm? reader?
  * Should we even have algorithms recommending / ranking content?
    * Recommendation algorithms are opaquely amplifying certain content (and implicitly censoring others).
    * Recommendation subtly implies endorsement. So some censorship is required.

---

## Q&As

> My thoughts on social media censoring?

An issue of the heart:

* Entirely uncensored anonymous communities tend to become hostile
* We'll find ways to "satisfy our itching ears" (if not YouTube then talk radio, moving to avoid uncomfortable people/ideas, ...)
* We must seek out, amplify, and create what is "true, honorable, just, pure", ... (Philippians 4)

---

## Rest of the semester

* Today: Data formats, APIs, SQL
* Wednesday: Text classification, bias
* Friday: advanced modeling and forecasting
* Monday: communication, publishing, more reflections on ethics and impact

All at-home focus is on projects.

I'm finally almost done with midterm project feedback!

---

## Data Formats

* Tabular, delimited (CSV, TSV) or fixed-width
* Tabular, structured: Excel, SPSS/Stata/SAS, etc.
* Hierarchical
  * JSON
  * XML
  * HTML
* Database
  * SQLite (most apps)
  * PostgreSQL / MySQL / Oracle / Microsoft SQL Server
  * Google BigQuery

(For a big table, see the [`rio` package vignette](https://cloud.r-project.org/web/packages/rio/vignettes/rio.html))

---

## Spreadsheets vs Databases

.pull-left[
* Often exchanged by email
* Hope the format doesn't change
* [Capacity limited](https://www.bbc.com/news/technology-54423988) ([other article](https://theconversation.com/why-you-should-never-use-microsoft-excel-to-count-coronavirus-cases-147681))
* Slow to query
* Decentralized (resilient?)
]

.pull-right[
* Centralized, highly available servers
* Documented schema
* Large capacity
* Fast queries
]

---

## APIs

.question[
Name as many APIs as you can think of.
]



---

class: larger-table

## API vs Local Data

| Local Data | API |
|-----|--------|
| Any query you want | Only queries the API exposes |
| As much as you want | Often rate-limited |
| Full dataset must fit on your computer | Practically unlimited data |
| | ... but you can only see a small part of it |
| Must be complete dataset | Can stream in new data |

---

## Example API: Bike Share Feeds

> The [General Bikeshare Feed Specification](https://github.com/NABSA/gbfs), known as GBFS, is the open data standard for bikeshare. GBFS makes real-time data feeds in a uniform format publicly available online, with an emphasis on findability.

* [Capital Bikeshare data](https://gbfs.capitalbikeshare.com/gbfs/gbfs.json)
* Station Information: <https://gbfs.capitalbikeshare.com/gbfs/en/station_information.json>

---

### Step 1: Retrieve data via HTTP(S)

```{r get-station-info, cache=TRUE}
station_info_response <- httr::GET("https://gbfs.capitalbikeshare.com/gbfs/en/station_information.json")
station_info_response
```

```{r extract-station-info, cache=TRUE}
station_info_text <- httr::content(station_info_response, as = "text")
str_sub(station_info_text, end = 500) %>% cat()
```

---

### Step 2: Parse the resulting JSON

```{r}
station_info <- jsonlite::fromJSON(station_info_text, simplifyDataFrame = TRUE)
stations <- station_info$data$stations
stations %>% glimpse()
```

---

### Step 3: Profit

.small-code[
```{r station-capacity, fig.width=10, fig.height=1, fig.asp=0.2}
capacity_palette <- leaflet::colorNumeric("GnBu", domain = stations$capacity)
lp <- stations %>% 
  leaflet() %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%  
  addCircleMarkers(~ lon, ~ lat, color = ~capacity_palette(capacity), radius = 2, stroke = FALSE, fillOpacity = 1) %>% 
  addLegend("bottomright", pal = capacity_palette, values = ~capacity)
lp$height <- 300
lp
```
]

See `leaflet` docs on [markers](http://rstudio.github.io/leaflet/markers.html) and
[colors](http://rstudio.github.io/leaflet/colors.html)

---

## Levels of API security

1. No key needed
2. Shared secret
3. Access delegation (typically [OAuth](https://en.wikipedia.org/wiki/OAuth))

---

### A typical access delegation dance

.center[
```{r echo=FALSE, out.width="50%"}
knitr::include_graphics("https://developer.spotify.com/assets/AuthG_AuthoriztionCode.png")
```
]

.floating-source[
Source: [Spotify Developer Docs](https://developer.spotify.com/documentation/general/guides/authorization-guide/)
]

---

class: center, middle

## SQL

---

## Grammar of Data

| dplyr (`data %>% `) | Pandas | SQL |
|-------|--------|-----|
| `select(col1, col2)` | `data[['col1', 'col2']]` | `SELECT col1, col2 FROM data` |
| `filter(col1 > 5)` | `data.query('col1 > 5')` | `SELECT * FROM data WHERE col1 > 5` |
| `left_join(data2, by = "col2")` | `pd.merge(data, data2, by="col2", type="left")` | `SELECT * FROM data LEFT JOIN data2 ON data.col2 == data2.col2` |
| `group_by(col2) %>% summarize(m = max(col1))` | `data.group_by('col2')['col1'].max()` | `SELECT max(col1) AS m FROM data GROUP BY col2` |
| `pivot_longer()` | `data.melt()` or `data.pivot()` | **No standard approach** |

* [SQLite syntax](https://sqlite.org/lang_select.html)
* [More examples](https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_r.html)

---

## BigQuery

To use BigQuery, you need to create a project in Google Cloud Platform to use
for billing.

```{r}
billing_project <- "calvindsdev"
```

Then you can use `bigrquery` to get set up to make BigQuery API calls.

```{r bq_connect, cache=TRUE}
library(bigrquery)
bigrquery::bq_auth(email = TRUE)

bq_connection <- DBI::dbConnect(
  bigrquery::bigquery(),
  project = "bigquery-public-data",
  billing = billing_project
)
```

---

## A COVID Example

```{r pop_join_covid_results, cache=TRUE, include=FALSE}
case_counts <- case_counts_query %>% collect()
```

```{r}
case_counts %>% kable()
```

---


```{r covid_glimpse, cache=TRUE}
covid_open_data <- 
  bq_connection %>% tbl("bigquery-public-data.covid19_open_data.covid19_open_data")
covid_open_data %>% glimpse()
```

---

```{r pop_glimpse, cache=TRUE, warning=FALSE}
world_bank_global_pop <- 
  bq_connection %>% 
  tbl("bigquery-public-data.world_bank_global_population.population_by_country")
world_bank_global_pop %>% glimpse()
```

---

.pull-left[
R (`dbplyr`)

```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)
```
]

.pull-right[
BigQuery SQL

```sql
SELECT
  country_code AS iso_3166_1_alpha_3,
  year_2018 AS population_2018
FROM
  `bigquery-public-data.world_bank_global_population.population_by_country`)
```
]

---

## Common Table Expression (`WITH`); `JOIN`

.pull-left[
```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
* left_join(
*   country_pop, by = "iso_3166_1_alpha_3")
```
]

.pull-right[
```sql
*WITH
* country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT *
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
*JOIN country_pop USING (iso_3166_1_alpha_3)
```
]

---


.pull-left[
```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
  left_join(
    country_pop, by = "iso_3166_1_alpha_3") %>%
* filter(
*   date == '2020-11-27',
*   aggregation_level == 0,
*   population_2018 > 100000000)
```
]

.pull-right[
```sql
WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT *
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN country_pop USING (iso_3166_1_alpha_3)
*WHERE
* date = '2020-11-27'
* AND aggregation_level = 0
* AND population_2018 > 100000000
```
]

---


.pull-left[
```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
  left_join(
    country_pop, by = "iso_3166_1_alpha_3") %>%
  filter(
    date == '2020-11-27',
    aggregation_level == 0,
    population_2018 > 100000000) %>% 
* mutate(
*   cases_per_capita = 
*     cumulative_confirmed / population_2018
* )
```
]

.pull-right[
```sql
WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT
  *,
* cumulative_confirmed/population_2018 AS cases_per_capita
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN country_pop USING (iso_3166_1_alpha_3)
WHERE
  date = '2020-11-27'
  AND aggregation_level = 0
  AND population_2018 > 100000000
```
]

---


.pull-left[
```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
  left_join(
    country_pop, by = "iso_3166_1_alpha_3") %>%
  filter(
    date == '2020-11-27',
    aggregation_level == 0,
    population_2018 > 100000000) %>% 
  mutate(
    cases_per_capita = 
      cumulative_confirmed / population_2018
  ) %>% 
*  select(
*    country_code, country_name,
*    cumulative_confirmed,
*    population_2018,
*    cases_per_capita)
```
]

.pull-right[
```sql
WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT
*  country_code,
*  country_name,
*  cumulative_confirmed,
*  population_2018,
*  cumulative_confirmed/population_2018 AS cases_per_capita
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN country_pop USING (iso_3166_1_alpha_3)
WHERE
  date = '2020-11-27'
  AND aggregation_level = 0
  AND population_2018 > 100000000
```
]


---

.pull-left[
```r
country_pop <- world_bank_global_pop %>%
  select(
    iso_3166_1_alpha_3 = country_code, 
    population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
  left_join(
    country_pop, by = "iso_3166_1_alpha_3") %>%
  filter(
    date == '2020-11-27',
    aggregation_level == 0,
    population_2018 > 100000000) %>% 
  mutate(
    cases_per_capita = 
      cumulative_confirmed / population_2018
  ) %>% 
  select(
    country_code, country_name,
    cumulative_confirmed,
    population_2018,
    cases_per_capita) %>% 
*  arrange(desc(cases_per_capita))
```
]

.pull-right[
```sql
WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT
  country_code,
  country_name,
  cumulative_confirmed,
  population_2018,
  cumulative_confirmed/population_2018 AS cases_per_capita
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN country_pop USING (iso_3166_1_alpha_3)
WHERE
  date = '2020-11-27'
  AND aggregation_level = 0
  AND population_2018 > 100000000
*ORDER BY
* cases_per_capita DESC
```
]

---

```{r pop_join_covid, cache=TRUE}
country_pop <- world_bank_global_pop %>%
  select(iso_3166_1_alpha_3 = country_code, population_2018 = year_2018)

case_counts_query <- covid_open_data %>%
  filter(date == '2020-11-27', aggregation_level == 0) %>% 
  left_join(country_pop, by = c("iso_3166_1_alpha_3")) %>%
  filter(population_2018 > 100000000) %>% 
  mutate(cases_per_capita = cumulative_confirmed / population_2018) %>% 
  select(country_code, country_name, cumulative_confirmed, population_2018, cases_per_capita) %>% 
  arrange(desc(cases_per_capita)) %>% 
  head(10)
case_counts_query %>% show_query()
```

---

### Getting the results

```{r ref.label="pop_join_covid_results", eval=FALSE}
```


```{r}
case_counts %>% kable()
```


---

<https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/covid19-open-data?filter=solution-type:dataset&q=covid19&project=calvindsdev>

```
WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT
  country_code,
  country_name,
  cumulative_confirmed,
  population_2018,
  cumulative_confirmed/population_2018 AS cases_per_capita
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN country_pop USING (iso_3166_1_alpha_3)
WHERE
  date = '2020-11-13'
  AND aggregation_level = 0
  AND population_2018 > 100000000
ORDER BY
  case_percent DESC
```

See also: <https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/>
from <https://fivethirtyeight.com/features/dissecting-trumps-most-rabid-online-following/>
