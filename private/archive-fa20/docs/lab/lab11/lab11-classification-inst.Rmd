---
title: "Lab 11: Classification"
output: 
  tufte::tufte_html:
    css: ../lab.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(reticulate)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

## Goals

Learning objectives:

* Basic EDA in Pandas
* Fit a classification model in Scikit-Learn (sklearn)


## Getting Started

Make teams, [claim your repo](https://github.com/Calvin-DS202-FA20/), clone, knit, etc. as usual.
(see [Lab 10](../lab10/lab10-tuning-inst.html) for reference.)

## Python

Please try to use Python (Pandas, sklearn) for this lab. 
For details about getting Python set up and tricks for how to use it, see
[Homework 4](../../hw/hw04/hw04-other-tools-inst.html).

**Note**: This instructions document includes many of the results needed to answer
the short answer questions. Include in your report both your answer to the question
and also the code you use to reproduce what you need to answer it.

## Acquiring Data

```{r download-data, echo=TRUE}
data_filename <- "data/autism.csv"
if (!file.exists(data_filename)) {
  dir.create("data")
  download.file("https://doi.org/10.1371/journal.pcbi.1005385.s001", data_filename)
}
```

```{r read-data, echo=TRUE}
col_names <- names(read_csv(data_filename, n_max = 1, col_types = cols(.default = col_character())))
autism <- read_csv(data_filename, skip = 2, col_names = col_names, col_types = cols(
  .default = col_double(),
  Group = col_character()
))
```

```{python, echo=TRUE}
autism = r.autism
```

```{r include=FALSE}
py_repr <- import_builtins()$repr
py_eval_repr <- function(x, quote_code = TRUE) { 
  res <- py_repr(py_eval(x, convert = FALSE))
  if (quote_code) {
    paste0('`', res, '`')
  } else {
    res
  }
}
```

```{python echo=TRUE}
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score
```


## EDA

1. How many children are represented in this dataset? How many variables are there?

```{python}
num_children, num_variables = autism.shape
#print(f"{num_children} children, {num_variables} variables")
```

2. What two variables are unlike the others? What is different about them?

```{python}
autism.info()
```

3. How many children do we have in each `Group`?

```{python}
autism['Group'].value_counts()
```

## Classfication

4. Create a new `DataFrame` called `asd_vs_neu` that has no `SIB` children.

```{python}
asd_vs_neu = autism.query("Group != 'SIB'")
asd_vs_neu = autism[autism['Group'] != "SIB"]
```

5. Verify that the new `DataFrame` has no `SIB` children but the same number of
children in the other groups.

```{python}
asd_vs_neu['Group'].value_counts()
```

`sklearn` has some conventions for what variables are named:

* `X` designates the input data, one row per observation, one column per feature.
  It is uppercase because it's a matrix.
* `y` designates the prediction targets, as a one-dimensional *vector* (and thus lowercase).

6. Create a variable called `y` (designating the *target* of the prediction)
  consisting of whether the `Group` is equal to `ASD` (`asd_vs_neu['Group'] == "ASD"`).

```{python}
y = asd_vs_neu['Group'] == "ASD"
```

7. Create a variable called `X` (designating the input data) consisting of 
  all of the columns of `asd_vs_neu` *except for* `Group` and `Vineland ABC`.

```{python}
X = asd_vs_neu.drop(["Group", "Vineland ABC"], axis = 1)
```

8. Check that the `shape` attribute of `X` is `r py_eval_repr('X.shape')`.

```{python}
X.shape
```

Hold out a test set of of 10% of the children. Remember to set the random seed.

```{python, echo=TRUE}
np.random.seed(0)
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.1)
```

Now, create a `LogisticRegression` object called `clf`, with the
regularization parameter `C` set to 0.001.

```{python, echo=TRUE}
clf = LogisticRegression(C = 0.001, solver = 'lbfgs')
```

9. Fit `clf` so that it tries to predict `y_train` from `X_train`.

```{python}
clf.fit(X_train, y_train)
```

12. Cross-validate to estimate the model's performance (accuracy, precision, and recall) using 5-fold cross-validation:

```{python}
cv_results = cross_validate(clf, X_train, y_train, cv=5,
  scoring=['accuracy', 'precision', 'recall'])
# Wrap the results in a DataFrame:
cv_results = pd.DataFrame(cv_results).reset_index()
```

(`sklearn`'s `cross_validate` refers to the assessment data in each fold as "`test`";
don't confuse this with the true test set that we held out earlier.)

Each row of `cv_results` is a fold (numbered `index`); each column is one of the metrics we asked
for (or some metadata).

```{python, echo=TRUE}
cv_results['test_accuracy']
```

We can now access this data in R.

```{r}
py$cv_results
```

```{r}
py$cv_results %>% 
  select(-fit_time, -score_time) %>% 
  pivot_longer(-index) %>% 
  ggplot(aes(y = name, x = value)) + geom_boxplot() + geom_point() 
```


1. What do you expect the accuracy, precision, and recall to be on the test set,
which we haven't seen yet?

Now we construct the model's predictions on the test set and store them
in a variable called `y_test_pred`.

```{python}
y_test_pred = clf.predict(X_test)
```

1. How many predictions are stored in `y_test_pred`?

```{python, include=FALSE}
y_test_pred.shape
```



11. Evaluate the model's *accuracy*, *precision*, and *recall* on the test set.

```{python}
accuracy_score(y_true = y_test, y_pred = y_test_pred)
precision_score(y_true = y_test, y_pred = y_test_pred)
recall_score(y_true = y_test, y_pred = y_test_pred)
```
