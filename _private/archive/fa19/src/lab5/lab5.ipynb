{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Feature Engineering, Model Selection, and Overfitting\n",
        "\n",
        "This lab will:\n",
        "\n",
        "* Show you how linear regression can be used to model data that aren't \"linear\" through the process of **feature engineering**\n",
        "* Show how conclusions about which model is best can be very different on the training set and the test set, which motivates the need for **model selection**\n",
        "* Demonstrate the concept of **overfitting**, which is when a change to a model improves its performance on its training set at the expense of its performance on a test set.\n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "%matplotlib inline\n",
        "\n",
        "# Set some display settings.\n",
        "sns.set(context='notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining a \"plot\" dataset\n",
        "\n",
        "The concepts of feature engineering, model selection, and overfitting all apply to models with high dimensionality. But those models get hard to visualize. So instead we're going to take some made-up **1-dimensional** data, which will let us plot the model's predictions (even though those models will actually be using higher-dimensional transformations of the data).\n",
        "\n",
        "We've done this plotting trick in past labs/hws by having a variable we called `x`. It was easy to get that `x` confused with the training and testing data, so for this lab we'll give it a special name: the \"plot dataset\". It will have the same *form* as our actual data (namely, a predictor called `x`), but it's totally made up just for the purposes of plotting.\n",
        "\n",
        "We'll use 500 points on the so-called *unit interval* (0 to 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_dataset = pd.DataFrame({\"x\": np.linspace(0.0, 1.0, 500)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make the data\n",
        "Run this block of code to generate the datasets... but don't peek at its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN ME\n",
        "if True:\n",
        "                                                                                                                                                                           data = pd.DataFrame({\"x\": np.linspace(0., 1., 200)}); data['y'] = .5 + 1. * np.sin(3 * data['x']) + .25 * np.random.RandomState(0).standard_normal(len(data.x)); data = data.sample(frac=1., random_state=0); train = data.iloc[:50]; test = data.iloc[50:]; del data; train.to_csv('train.csv', index=False); test.to_csv('test.csv', index=False); del train; del test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and visualize our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load up 'train.csv' (don't look at test.csv yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "print(len(train), 'datapoints')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(train['x'], train['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There seems to be some pattern to that data, but it might be pretty complicated. Let's try to model it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear model\n",
        "First, try fitting a simple linear regression model to predict `y` from `x`.\n",
        "\n",
        "1. **What MSE and R^2 does the linear regression model get** on the training data?\n",
        "2. **Plot the training set points and the linear model on the same graph**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform(data):\n",
        "    return data[['x']]\n",
        "\n",
        "train_X = transform(train)\n",
        "train_y = train['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit a linear regression\n",
        "# Your code here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate its performance. You may use the `metrics` module we imported above.\n",
        "# Your code here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training set points and the linear model\n",
        "plt.scatter(train.x, train.y, color=\"red\")\n",
        "\n",
        "# Notice that we can use the same `transform` function on the plot dataset.\n",
        "plot_X = transform(plot_dataset)\n",
        "\n",
        "# your code here (to plot the linear model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Piecewise linear approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try modeling this data with a piecewise linear function. That is, the function doesn't have to be linear overall, but consists of gluing together linear \"pieces\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are a few ways of representing piecewise linear functions. One simple way is by representing them as a sum of \"[rectifiers](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\". A *rectifier* is kind of like an absolute-value function, but instead of flipping negative values up to positive, it just chops them off (to zero) instead. These are widely used in deep neural networks these days; in this exercise you'll get a sense of how powerful these simple building blocks can be. A \"rectified linear unit\" (ReLU) is defined as:\n",
        "\n",
        "$$\\text{relu}(x) = \\max(0, x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a rectifier. Try changing the `.5` to something else to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "rectified_plot_x = relu(plot_dataset['x'] - .5)\n",
        "plt.plot(plot_dataset['x'], rectified_plot_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks like data changes from going up to going down somewhere around 0.5. Let's make a rectifier with its zero point at 0.5 (like the one above) and try to use it to model our data.\n",
        "\n",
        "The following code implements a predictive model that is *linear* in `x` and in `relu(x - .5)`. **Adjust the constants (except for 0.5) so that the model seems to roughly fit the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(train.x, train.y, color=\"red\")\n",
        "plt.plot(plot_dataset.x,\n",
        "         0.0 + # intercept\n",
        "         1.0 * plot_dataset.x + # linear term\n",
        "         1.0 * relu(plot_dataset.x - 0.5) # rectifier term\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the rectifier term was just another `Series`. Rather than calculate it each time, we could have put it in as a column in our dataset, like we did with one-hot encoding in class and last week's lab. Let's do that using a `transform` function like we did in class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform(data):\n",
        "    X = data[['x']].copy()\n",
        "    X['rectifier_1'] = relu(data['x'] - 0.5)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use that transform function to redo the plot we just did. Copy your constants from above into this cell and verify that it makes the same plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_X = transform(plot_dataset)\n",
        "plt.scatter(train.x, train.y, color=\"red\")\n",
        "\n",
        "plt.plot(plot_dataset.x,\n",
        "        0.0 + # intercept\n",
        "        1.0 * plot_X['x'] + # linear term \n",
        "        1.0 * plot_X['rectifier_1']) # rectifier term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a `transform` function, we can use `LinearRegression` to fit those constants for us instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X = transform(train)\n",
        "train_y = train['y']\n",
        "linreg = LinearRegression().fit(train_X, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, check that the coefficients it came up with are about the same as the ones you came up with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "linreg.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "linreg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And let's plot its predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(train.x, train.y, color=\"red\")\n",
        "plt.plot(plot_dataset.x, linreg.predict(transform(plot_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What MSE and R2 does this model get on the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pause to answer a few questions:\n",
        "\n",
        "1. How does the *shape* of the second model's prediction compare with the simple linear regression model you made first?\n",
        "2. The prediction isn't a straight line. In what sense is the model's prediction still *linear*?\n",
        "3. In what ways does the second model seem like a good fit for the data? How could it be better?\n",
        "4. How do the MSE and R^2 numbers compare between the first and second models? (self-check: does that match your intuition? no response needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improving the model\n",
        "We added a predictor (in this case, a rectifier) and got a model that performed better on the training data. Let's add more predictors to try to get an even better fit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1\n",
        "\n",
        "Make a single cell that combines all of the steps we performed with the second model above:\n",
        "\n",
        "1. transforms the data,\n",
        "2. fits a linear regression,\n",
        "3. prints out the training set R^2 and MSE, and\n",
        "4. plots the training data and the model's predictions on the same plot.\n",
        "\n",
        "**feel free to copy and paste your earlier code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2\n",
        "Modify the `transform` function to add predictors to this model to try to improve its accuracy. *copy and paste your cell above* and modify it here.\n",
        "\n",
        "Things you might try adding:\n",
        "\n",
        "* more rectifiers\n",
        "* polynomial terms ($x^2$, $x^3$ etc.)\n",
        "* Periodic functions (`np.sin(2*np.pi * x)`, `np.sin(2*np.pi * 5 * x)`, `np.cos(2*np.pi * x)`, etc.)\n",
        "* $\\log(x+.1)$, $\\exp$, ...\n",
        "\n",
        "**See if you can get the R^2 to be above 0.75.** Feel free to write loops, etc.\n",
        "\n",
        "*Note*: If you get tired of naming your new predictors, you can use this alternative version of `transform`.\n",
        "\n",
        "```\n",
        "def transform(data):\n",
        "    x = data['x'].values\n",
        "    predictors = [\n",
        "        x,\n",
        "        relu(x - .5), or whatever else here\n",
        "    ]\n",
        "    return np.stack(predictors, axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generalization\n",
        "Ok, we got a model that worked great! Now let's test it on some data it hasn't seen before. Load the test data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's make that plot and R^2 like above, but with the test data instead of the training data.\n",
        "\n",
        "**Important**: *don't* re-fit the model here! Just use the `linreg` model from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. What do you notice? \n",
        "2. Why do you think that happened?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below here, copy and paste the `transform` function, but change it to the one you would now use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why did you include those predictors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What are some things we could have *done* or *observed* that would have helped us realize that we were going to do so badly on unseen data *before* looking at the test set? How would these things have helped?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}