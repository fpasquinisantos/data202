---
title: "Lab 10: Model Tuning"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(rpart.plot)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

```{r utilities, appendix=TRUE}
append_new_results <- function(modeling_results, model_name, spec, cv_results) {
  if (nrow(modeling_results) > 0 && model_name %in% modeling_results$model_name)
    stop("Model with that name already exists.")
  
  bind_rows(
    modeling_results %>% mutate(model_name = as.character(model_name)),
    tibble(
      model_name = model_name,
      spec = list(spec),
      cv_results %>% select(-.estimator)
    )
  ) %>% 
    mutate(model_name = as_factor(model_name)) # Ensure that factor level matches insertion order.
}

get_spec_for_model <- function(model_name) {
  modeling_results %>% filter(model_name == !!model_name) %>% purrr::chuck("spec", 1)
}

add_predictions <- function(data, ...) {
  imap_dfr(
    rlang::dots_list(..., .named = TRUE),
    function(model, model_name) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}

sweep_model_examples <- function(model, dataset, vars_to_sweep, examples = slice_sample(dataset, n = 10)) {
  X <- map_dfr(vars_to_sweep, function(v) {
    var_to_sweep <- rlang::as_label(v)
    sweep_min <- min(dataset[[var_to_sweep]])
    sweep_max <- max(dataset[[var_to_sweep]])
    expand_grid(
      examples %>% select(-!!var_to_sweep) %>% mutate(.idx = row_number()),
      !!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500)) %>% 
      mutate(sweep_var = var_to_sweep, .sweep_val = .data[[var_to_sweep]])
  })
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}

linear_reg <- function(engine = "lm", ...) {
  parsnip::linear_reg(...) %>% set_engine(engine)
}

decision_tree <- function(mode = "regression", engine = "rpart", ...) {
  parsnip::decision_tree(mode = "regression", ...) %>%
    set_engine(engine)
}
```



## Data and Initial Split

```{r load-training-set}
ames_train <- read_csv("data/ames_train.csv")
```

## Modeling

### Initialize

Split the training data into 10 cross-validation folds:

```{r declare-cv}
ames_resamples <- ames_train %>% 
  vfold_cv(v = 10)
```

Create a data frame to collect modeling results as we go.

```{r empty-modeling-results}
modeling_results <- tibble()
```


### Linear Regression 1

Our first modeling workflow uses a linear regression model on `Latitude` and `Longitude`.

```{r linreg1}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude, data = ames_train)
  ) %>% 
  add_model(
    linear_reg()
  )

# We compute the cross-validation assessment scores:

cv_results <- 
  spec %>% 
  fit_resamples(resamples = ames_resamples, metrics = metric_set(mae)) %>% 
  collect_metrics(summarize = FALSE)

# Finally, we collect these results into our `modeling_results` data frame.

modeling_results <- modeling_results %>% 
  append_new_results(
    model_name = "Simple Linreg",
    spec = spec,
    cv_results = cv_results
  )
```

### Decision Tree

*your decision tree code here*

## Compare model performance

The following graph shows the estimates of the performance of the two models
we have trained so far, evaluated using 10-fold cross validation.

```{r compare-models}
modeling_results %>% 
  ggplot(aes(y = fct_rev(model_name), x = .estimate)) +
    geom_boxplot() +
    geom_point() +
    labs(x = "Mean Absolute Error ($1000)", y = "")
```


### Better Decision Tree

*your improved decision tree code here*

## Visualize models

```{r fit-on-full-train}
model <- 
  get_spec_for_model("Simple Linreg") %>%
  fit(ames_train)
```

#### Examine Residuals

```{r resids-against-all-numeric, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
ames_train %>% 
  add_predictions(model) %>% 
  mutate(resid = Sale_Price - .pred) %>% 
  # Pivot all of the numeric variables except for the outcomes and residuals.
  pivot_longer(c(where(is.numeric) & !any_of(c("Sale_Price", ".pred", "resid")))) %>% 
  # Plot!
  ggplot(aes(x = value, y = resid)) +
    geom_point() +
    geom_smooth(method = "lm") +
    facet_wrap(vars(name), scales = "free")
```

##### Examine Predictions

Plot sweeps of longitude, latitude, and above-grade living area:

```{r sweep-model-examples, fig.fullwidth=TRUE}
set.seed(0)
sweep_model_examples(model, ames_train, vars(Longitude, Latitude, Gr_Liv_Area)) %>% 
  ggplot(aes(x = .sweep_val, y = .pred, group = .idx, color = Latitude)) +
  geom_line(alpha = .5) +
  facet_wrap(vars(sweep_var), scales = "free")
```



### Better Linreg

*your better linear regression model here*

## Final model performance comparison

