---
title: "Feature Engineering and Review"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
```

## Logistics

* Project: outline is posted
* Midterm Quiz posted


Plan:

* TODAY: review
* Wed: decision trees
* Fri: lab about **overfitting**

---

## General Hints

* Visualization:
  * What *glyph* represents each *observation*? 
  * What *attributes*/aesthetics does that glyph have? (x, y, width, color, ...)
  * What *controls* each aesthetic? ("each party has a *y* position", ...)
* Wrangling
  * What does the *input* look like? (Translate the first row into a data sentence in English.)
  * What does the *output* need to look like? (Again, write a sentence.)
  * What *sequence of steps* needs to happen? (e.g., `filter-group_by-summarize-arrange`)
* Modeling
  * What *quantity* are you trying to predict?
  * What *error measure* will tell you the prediction is good / bad?
  * What *features* can help you make that prediction?

---

## Q&A

> Recipes vs Data Wrangling Pipelines?

* recipe (from `recipes` package) = data wrangling pipeline
  * ... that can be easily applied to new data (e.g., *test set*)
  * ... that can have learnable state (like ranges of data values)
  
> Why did the prediction on the example test set house come out the same when we re-scaled the data?

* We'd applied *exactly the same* transformation to the example house
  as we did to the training data.
* Linear regression is *linear*: it doesn't care what units the data is in.
* (So the specific *range* didn't matter.)

> Are we still going to work in cohorts/teams?

Friday's lab, hopefully.

---

```{r}
library(tidymodels)
data(ames, package = "modeldata")
ames <- ames %>% 
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>% 
  mutate(across(where(is.integer), as.double))
```

```{r train-test-split}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames, prop = 2/3) # Split our data randomly
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

We'll use one example home from the test set.

```{r example-home}
example_home <- ames_test %>% slice(1)
example_home %>% select(Gr_Liv_Area, Sale_Price)
```

---

class: center, middle

# Recipes

---

```{r}
ames_recipe <- 
  recipe(
    Sale_Price ~ Latitude + Longitude + Neighborhood + Year_Sold + Gr_Liv_Area,
    data = ames_train
  ) %>% 
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>% 
  #step_interact(~ starts_with("Neighborhood_") : Year_Sold) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  step_log(Sale_Price, base = 10) %>% 
  prep(trainig = ames_train, retain = TRUE)
ames_recipe %>% summary()
```

---

```{r}
ames_recipe %>% bake(new_data = ames_train)
```

