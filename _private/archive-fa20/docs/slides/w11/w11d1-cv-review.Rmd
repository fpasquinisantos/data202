---
title: "Cross-Validation Review"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
library(rsample)
library(parsnip)
library(workflows)
library(recipes)
library(tune)
library(yardstick)
#xaringanExtra::use_tachyons()
```

## Logistics

* Today: review, continue lab10
* Wednesday: Wrangling and Modeling in Python (*classification*).
  * *lab10 due*.
  * *Project milestone 1*: Data
* Friday: Classification *lab*
* Next Monday: (probably) brief notes about inference
* Next Tuesday: Discussion 11 due (fairness in classification)

---

## Midterm notes

* Remember grammar of graphics: each aesthetic maps to one *variable*.
* Think about the shape of your data!
* Don't wait for the last minute.

(*Academic integrity note.*)

---

## Feedback

Common themes in your comments:

* Modeling is fun
* Cross Validation is cool... but still confusing
* All that code is *really* confusing

Indeed. Let's review.

---

## Why train-test split? Memorizing the eye chart

.floating-source[
[Snellen chart on Wikimedia](https://commons.wikimedia.org/wiki/File:Snellen_chart.svg), CC-BY-SA  
Analogy by [Clem Wang](https://www.linkedin.com/pulse/metaphor-over-fitting-machine-learning-clem-wang)
]

```{r echo=FALSE, out.width="50%"}
include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Snellen_chart.svg/1000px-Snellen_chart.svg.png")
```

---

## Overfitting

.floating-source[
<https://xkcd.com/1122/>
]

```{r echo=FALSE, out.width="100%"}
include_graphics("https://imgs.xkcd.com/comics/electoral_precedent_2x.png")
```

---

## Why Cross-Validation?

#### Puzzle

* We want to pick the model that works best on *unseen* data
* ... but as soon as we try one model, **we've peeked at the data!**

#### Solution

* Divide training data into *V* piles (e.g., 10)
* Hide one pile from yourself.
  * train on ("analyze") the rest,
  * evaluate ("assess") on the one you held out.
* Repeat for each of the *V* piles.

---

```{r cv-anim, echo=FALSE, animation.hook='gifski', cache=TRUE, fig.width=6, fig.asp=.6}
# From https://yihui.org/animation/example/cv-ani/
library(animation)
set.seed(0)
ani.options(interval = 1, nmax = 15)
cv.ani(main = "Demonstration of 10-fold Cross Validation", k = 10,
  bty = "l")
```

---

## In code...

```{r eval=FALSE}
cross_val_scores <- function(complete_model_spec, training_data, v, metrics = metric_set(mae)) {
  # Split the data into V folds.
  set.seed(0)
  resamples <- vfold_cv(training_data, v = v) #<<
  
  ...
}
```

---

## In code...

```{r eval=FALSE}
cross_val_scores <- function(complete_model_spec, training_data, v, metrics = metric_set(mae)) {
  # Split the data into V folds.
  set.seed(0)
  resamples <- vfold_cv(training_data, v = v)
  
  # For each of the V folds, assess the result of analyzing on the rest.
  raw_cv_results <- complete_model_spec %>% 
    fit_resamples(resamples = resamples, metrics = metrics)
  
  # Return the collected metrics.
  collect_metrics(raw_cv_results, summarize = FALSE)
}
```

---

## What's a complete model spec?

Workflow = recipe + model\_spec.

```{r eval=FALSE}
spec <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(model)
```

e.g.,

```{r eval=FALSE}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude, data = ames_train) #<<
  ) %>% 
  add_model(
    linear_reg() #<<
  )
```

---

## Continuing with Lab 10

[Instructions](../../lab/lab10/lab10-tuning-inst.html)
