---
title: "Classification"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup(mark_languages = TRUE)
knitr::opts_chunk$set(echo = TRUE)
theme_set(theme_bw())
```

## Objectives

* Apply the `tidymodels` pipeline to a classification task
* Identify when validation is necessary to believe an outcome
* Apply the concepts of sensitivity and specificity

(punted to next time:)
* Identify corresponding elements between R and Python data wrangling and modeling workflows

---

## Outline

* Problem introduction
  * Data wrangling in R
* Classification workflow
  * Models: decision tree, logistic regression
  * Model outputs: scores and decisions
  * Model metrics: accuracy, sensitivity, specificity
  * Validation


<!-- If time permits... -->

<!-- * Python -->
<!--   * Data wrangling with Pandas -->
<!--   * scikit-learn model API: construct, fit, predict -->
<!--   * scikit-learn modeling pipelines -->

---

## Logistics notes

* Discussion on fairness definitions posted, due next Tuesday.
* Your **project** should be
  * **Interesting**: not every project will go into depth in every aspect (e.g., some won't have 
    much data wrangling), but all projects should be interesting in *some* aspect.
  * **Your own**: examples abound on the Internet. Following a tutorial is a
    very boring project. Adapting its approach to a new dataset or question? Interesting.

---

## Setup

```{r}
library(tidyverse)
library(tidymodels)
library(ggridges)
```

```{r utilities, include=FALSE}
add_predictions <- function(data, ...) {
  imap_dfr(
    rlang::dots_list(..., .named = TRUE),
    function(model, model_name) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}

sweep_model_examples <- function(model, dataset, vars_to_sweep, examples = slice_sample(dataset, n = 10)) {
  X <- map_dfr(vars_to_sweep, function(v) {
    var_to_sweep <- rlang::as_label(v)
    sweep_min <- min(dataset[[var_to_sweep]])
    sweep_max <- max(dataset[[var_to_sweep]])
    expand_grid(
      examples %>% select(-!!var_to_sweep) %>% mutate(.idx = row_number()),
      !!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500)) %>% 
      mutate(sweep_var = var_to_sweep, .sweep_val = .data[[var_to_sweep]])
  })
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}
```


---

## Can a blood test diagnose autism?

We'll use an example from <https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005385>


```{r download-data}
data_filename <- "data/autism.csv"
if (!file.exists(data_filename)) {
  dir.create("data")
  download.file("https://doi.org/10.1371/journal.pcbi.1005385.s001", data_filename)
}
```

```{r read-data}
col_names <- names(read_csv(data_filename, n_max = 1, col_types = cols(.default = col_character())))
autism <- read_csv(data_filename, skip = 2, col_names = col_names, col_types = cols(
  .default = col_double(),
  Group = col_character()
)) %>% mutate(
  Group = as_factor(Group)
)
```

---

We have 3 kinds of data about `r nrow(autism)` children:

1. The outcome (`Group`): ASD (diagnosed with ASD), SIB (sibling not diagnosed with ASD), and NEU (age-matched neurotypical children, for control)

```{r group-counts}
autism %>% group_by(Group) %>% summarize(n = n()) %>% kable()
```

---

1. The outcome (`Group`): ASD, SIB, NEU
2. Concentrations of various metabolites in a blood sample:

```{r show-metabolites}
autism %>% select(-1, -last_col())
```

---

1. The outcome (`Group`): ASD, SIB, NEU
2. Concentrations of various metabolites in a blood sample
3. For the ASD children only, a measure of life skills ("Vineland ABC")

```{r show-behavior-score, fig.asp=.25}
autism %>% 
  ggplot(aes(x = `Vineland ABC`, y = Group)) + geom_boxplot()
```

---

## Exploratory Data Analysis (EDA)

What do these metabolites look like?

```{r bad-metabolite-plot, echo=FALSE}
autism %>%
  select(-`Vineland ABC`) %>% 
  pivot_longer(-Group, names_to = "Measure") %>% 
  ggplot(aes(x = value, y = Measure)) +
  geom_density_ridges() + 
  facet_wrap(vars(Group), scales = "free_x")
```

---

code for the previous plot:
```{r ref.label="bad-metabolite-plot", eval=FALSE}
```

---

## EDA

Better question: **Can these metabolites help us distinguish autism?**

---

```{r ridgeplot, fig.width=10, fig.asp=1, echo=FALSE}
autism %>%
  select(-`Vineland ABC`) %>% 
  pivot_longer(-Group, names_to = "Measure") %>% 
  ggplot(aes(x = value, y = Group)) +
  geom_density_ridges() +
  facet_wrap(vars(Measure), scales = "free_x") + 
  theme_ridges()
```

---

code for previous plot:

```{r ref.label="ridgeplot", eval=FALSE}
```

---

## Can we predict ASD vs non-ASD from metabolites?

* Let's start by (1) ignoring the behavior scores (that's an *outcome*) and comparing
just ASD and NEU.
* We need to drop SIB... and tell the model that we don't actually care about it.

```{r}
data <- 
  autism %>% 
  select(-`Vineland ABC`) %>% 
  filter(Group != "SIB") %>% 
  mutate(Group = factor(Group))
```

---

## Decision Tree *Classification*

```{r fit-tree}
spec <- workflow() %>% add_recipe(
  recipe(Group ~ ., data = data)) %>% #<<
  add_model(decision_tree(mode = "classification") %>% set_engine("rpart")) #<<
model <- spec %>% fit(data)
```


```{r show-tree, echo=FALSE, fig.asp= .4}
rpart.plot::rpart.plot(pull_workflow_fit(model)$fit, roundint = FALSE)
```

---

### What do the *predictions* look like?

```{r predicted-probs-tree}
model %>% predict(data, type = "prob")
```

---

### Were those predictions *good*?

.small-code[
```{r plot-predicted-probs-tree, fig.asp=.4}
model %>%
  predict(data, type = "prob") %>%
  bind_cols(data) %>% 
  mutate(idx = row_number()) %>% 
  ggplot(aes(x = idx, y = .pred_ASD, color = Group, shape = Group)) +
  geom_hline(yintercept = .5) +
  geom_point() 
```
]

---

### Quantifying that:

```{r}
metrics <- yardstick::metric_set(accuracy, sensitivity, specificity)
model %>% 
  predict(data, type = "class") %>% 
  bind_cols(data) %>% 
  metrics(truth = Group, estimate = .pred_class)
```

---

## Recall from Week 6...

|                      | Seizure happened              | No seizure happened           |
|----------------------|-------------------------------|-------------------------------|
| Seizure predicted    | True positive                 | False positive (Type 1 error) |
| No seizure predicted | False negative (Type 2 error) | True negative                 |

--
- **Accuracy** (% correct) = (TP + TN) / (# episodes)
- **False negative** ("miss") **rate** = FN / (# actual seizures)
- **False positive** ("false alarm") **rate** = FP / (# true non-seizures)

--
- **Sensitivity** ("true positive rate") = TP / (# actual seizures)
  - Sensitivity = 1 − False negative rate
- **Specificity** ("true negative rate") = TN / (# actual seizures)
  - Specificity = 1 − False positive rate
- [Wikipedia article](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)


---

class: center, middle

## Logistic Regression

---

## Logistic Regression

```{r}
spec <- workflow() %>% add_recipe(
  recipe(Group ~ ., data = data)) %>% #<<
  add_model(logistic_reg(penalty = .001) %>% set_engine("glmnet")) #<<
model <- spec %>% fit(data)
```

.small-code[
```{r vis-logisticreg-model, fig.asp=.3}
model %>% pull_workflow_fit() %>% pluck('fit') %>% coef(s = .1) %>% as.matrix() %>% as_tibble(rownames = "name") %>% 
  rename(coef = 2) %>% filter(abs(coef) > .01) %>% 
  ggplot(aes(x = coef, y = fct_reorder(name, coef, abs))) + geom_col()
```
]

---

```{r predicted-probs}
model %>% predict(data, type = "prob")
```

---

### Are those predictions good?

.small-code[
```{r plot-predicted-probs-linreg, fig.asp=.4}
model %>%
  predict(data, type = "prob") %>%
  bind_cols(data) %>% 
  mutate(idx = row_number()) %>% 
  ggplot(aes(x = idx, y = .pred_ASD, color = Group, shape = Group)) +
  geom_hline(yintercept = .5) +
  geom_point() 
```
]

---

### Quantifying that...

```{r}
model %>% 
  predict(data, type = "class") %>% 
  bind_cols(data) %>% 
  metrics(truth = Group, estimate = .pred_class)
```


---

## Cross validation!

Stratify by `group` to ensure each fold has good representation.

```{r define-cv}
resamples <- data %>% vfold_cv(v = 10, strata = Group)
```


```{r}
cv_results <- spec %>% 
  fit_resamples(resamples, metrics = metrics)
```

---

```{r cv-results-logreg}
cv_results %>% 
  collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x = .estimate, y = .metric)) + geom_boxplot()
```

---

## Cross-validate the decision tree

```{r}
spec <- workflow() %>% add_recipe(
  recipe(Group ~ ., data = data)) %>% #<<
  add_model(decision_tree(mode = "classification") %>% set_engine("rpart")) #<<

cv_results <- spec %>% 
  fit_resamples(resamples, metrics = metrics)
```

```{r cv-results-tree, fig.asp=.3}
cv_results %>% 
  collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x = .estimate, y = .metric)) + geom_boxplot()
```

