---
title: "Cross-Validation"
author: ["K Arnold", "DATA 202 Fall 2020"]
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glue)
library(recipes)
library(rsample)
library(parsnip)
library(tune)
library(yardstick)
library(rpart.plot)
library(gifski)
theme_set(theme_bw())
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

.small-code[
```{r utilities, echo=FALSE}
add_predictions <- function(data, ...) {
  imap_dfr(
    rlang::dots_list(..., .named = TRUE),
    function(model, model_name) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}

sweep_model <- function(model, var_to_sweep, sweep_min, sweep_max, ...) {
  X <- expand_grid(!!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500), ...)
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}

linear_reg <- function(engine = "lm", ...) {
  parsnip::linear_reg(...) %>% set_engine(engine)
}

decision_tree <- function(mode = "regression", engine = "rpart", ...) {
  parsnip::decision_tree(mode = "regression", ...) %>%
    set_engine(engine)
}
```

```{r load-and-subset-data, echo=FALSE}
data(ames, package = "modeldata")
ames_all <- ames %>%
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>%
  mutate(across(where(is.integer), as.double)) %>%
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

```{r train-test-split, echo=FALSE}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames_all, prop = 2 / 3)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

```{r lat-long-grid, echo=FALSE}
lat_long_grid <- expand_grid(
  Latitude  = modelr::seq_range(ames_train$Latitude,  n = 200, expand = .05),
  Longitude = modelr::seq_range(ames_train$Longitude, n = 200, expand = .05),
)
```


```{r plot-util, echo=FALSE}
show_latlong_model <- function(dataset, model, model_name = deparse(substitute(model))) {
  ggplot(dataset, aes(x = Longitude, y = Latitude)) +
    geom_raster(
      data = lat_long_grid %>% add_predictions(model),
      mapping = aes(fill = .pred)
    ) +
    geom_point(aes(color = Sale_Price), size = .5) +
    scale_color_viridis_c(aesthetics = c("color", "fill")) +
    coord_equal() +
    labs(title = model_name)
}
```

]

## Q&A

> Was `model3` bad b/c it had a big difference between train and test?

Great question! The model *itself* was not bad, but we *over-sold* it.

> When is decision tree better than linreg? Worse?

* Decision tree: crisp regions are easy; smooth variation is hard.
* Linear regression: smooth variation is easy; crisp regions are hard.

> Could a linear regression use sinusoidal models?

Yes! There's lots of "basis functions" we can use (splines, sinusoids, rectifiers, ...)

---

## Q&A 2

> Can a tree check multiple conditions at once?

No, though it can check them in sequence. If you know that's important, you can create new features.

> Is there a limit to num decisions?

Yes: each decision must be *justified by the data*.

> Can greedy algorithms be worse?

Yes. Seeking short-term gain gives long-term regret.

---

## Quick datavis tips

* Using data from Pew? See [notes on *weights*](https://cs.calvin.edu/courses/data/202/fa20/spss-tips.html)
* Don't forget the cheat sheets! [`dplyr`](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf), [`ggplot` ](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-visualization-2.1.pdf),  ...
  * I learned some things glancing over them again! (`coord_cartesian` instead of `xlim` etc.)
  * You can also use plotly, seaborn, etc., as long as it's reproducible.
* I'm not watching your every commit. If you need help, send a quick screenshot over Teams
  (preferably Q&A channel)

---

class: center, middle

# Cross-Validation

---

## Why Cross-Validation?

Measure accuracy on unseen data *without peeking at test set* (compare lab9)

```{r declare-cv, echo=FALSE}
ames_resamples <- ames_train %>% vfold_cv(v = 10)
```

```{r declare-all-models, echo=FALSE, cache=TRUE}
all_models <- tribble(
  ~model_name, ~spec,
  "model1",    decision_tree(mode = "regression", tree_depth = 2),
  "model2",    decision_tree(mode = "regression", tree_depth = 30),
  "model3",    decision_tree(mode = "regression", cost_complexity = 1e-6, min_n = 2)
)
```

```{r sample-all-models, echo=FALSE, cache=TRUE}
models_with_samples <- all_models %>% 
  rowwise() %>% 
  mutate(samples = list(
    spec %>% fit_resamples(
      Sale_Price ~ Latitude + Longitude,
      resamples = ames_resamples,
      metrics = metric_set(mae))))
```

```{r model-specs, echo=FALSE}
model3_spec <- all_models$spec[[3]]
```


```{r last-fit, echo=FALSE}
test_predictions <- 
  all_models %>% 
  rowwise(model_name) %>% 
  # Fit on all training data
  mutate(fit_on_all_training_data = list(spec %>% fit(Sale_Price ~ Latitude + Longitude, data = ames_train))) %>% 
  # Test on test set
  summarize(ames_test %>% add_predictions(fit_on_all_training_data) %>% mae(truth = Sale_Price, estimate = .pred))
```

```{r compare-models-traintest, echo=FALSE}
models_with_samples %>% 
  rowwise(model_name) %>% 
  summarize(collect_metrics(samples, summarize = FALSE)) %>% 
  bind_rows(
    train = .,
    test = test_predictions,
    .id = "assessment_data"
  ) %>% 
  mutate(assessment_data = as_factor(assessment_data)) %>% 
  ggplot(aes(x = model_name, y = .estimate, color = assessment_data)) +
  geom_boxplot() +
  labs(x = "Model name", y = "Mean absolute error ($1000)", fill = "Assessment dataset") +
  coord_cartesian(ylim = c(0, NA))
```




---

## What is Cross-Validation?

```{r resampling-flowchart, echo=FALSE, out.width="90%"}
knitr::include_graphics("https://www.tmwr.org/premade/resampling.svg")
```

---

```{r three-cv-iter, echo=FALSE}
knitr::include_graphics("https://www.tmwr.org/premade/three-CV-iter.svg")
```

---


```{r ames-cv-anim, echo=FALSE, cache=TRUE, fig.show="animate", animation.hook="gifski", out.width="100%"}
ames_train %>%
  vfold_cv(v = 10) %>%
  pull(splits) %>% 
  iwalk(function(split, split_idx) {
    print(
      bind_rows(analysis = analysis(split), assessment = assessment(split), .id = "role") %>% 
      ggplot(aes(x = Latitude, y = Longitude, color = role, shape = role)) +
      geom_point(size = 1) +
      scale_color_manual(values = c("analysis" = "grey", "assessment" = "red")) +
      labs(title = glue("Fold {split_idx}")) +
      theme_bw() +
      theme(panel.grid = element_blank())
    )
  })
```

---

## How to do CV?

1. Declare the splitting strategy:

```{r ref.label="declare-cv", eval=FALSE}
```

```{r show-resamples-df}
ames_resamples
```

---

## How to do CV?

1. Declare the splitting strategy
2. Fit on each resample, evaluate using a set of metrics.

```{r ames-cv-model3-anim, echo=FALSE, cache=TRUE, fig.show="animate", animation.hook="gifski"}
withr::with_seed(0, {
  ames_train %>%
    vfold_cv(v = 10) %>%
    pull(splits) %>% 
    iwalk(function(split, split_idx) {
      model <- model3_spec %>% fit(Sale_Price ~ Latitude + Longitude, data = analysis(split))
      assess_mae <- assessment(split) %>% 
        add_predictions(model) %>% 
        mae(truth = Sale_Price, estimate = .pred) %>%
        pull(.estimate)
      print(
        bind_rows(analysis = analysis(split), assessment = assessment(split), .id = "role") %>% 
        ggplot(aes(x = Latitude, y = Longitude, color = role)) +
        geom_raster(
          data = lat_long_grid %>% add_predictions(model),
          mapping = aes(fill = .pred, color = NULL)
        ) +
        geom_point(size = .1) +
        scale_color_manual(values = c("analysis" = "grey", "assessment" = "red")) +
        labs(title = glue("Fold {split_idx} (MAE on assessment = {format(assess_mae, format='f', digits = 4)})")) +
        theme_bw() +
        theme(panel.grid = element_blank())
      )
    })
})
```

---

## How to do CV?

1. Declare the splitting strategy
2. Fit on each resample, evaluate using a set of metrics.

```{r sample-model3, cache=TRUE}
model3_samples <- model3_spec %>%
  fit_resamples( #<<
    Sale_Price ~ Latitude + Longitude,
    resamples = ames_resamples, #<<
    metrics = metric_set(mae))
model3_samples %>% collect_metrics(summarize = FALSE)
```

---

## How to do CV?

1. Declare the splitting strategy
2. Fit on each resample, evaluate using a set of metrics.
3. Plot and/or summarize the metrics.

.pull-left[
```{r crude-plot-folds, fig.height=1, fig.width=3, fig.asp=NULL}
model3_samples %>%
  collect_metrics(summarize = FALSE) %>% 
  ggplot(aes(x = .estimate, y = "model3")) +
  geom_point()
```
]

.pull-right[
```{r summarize-performance}
model3_samples %>%
  collect_metrics(summarize = TRUE)
```

]

---

## A tidy way to compare models

1. Make a data frame of model specs:

```{r ref.label="declare-all-models", eval=FALSE}
```

---

## A tidy way to compare models

1. Make a data frame of model specs:
2. Sample each model (using `dplyr::rowwise`):

```{r ref.label="sample-all-models", eval=FALSE}
```

```{r show-models-with-samples}
models_with_samples
```

---

```{r}
models_with_samples %>% 
  rowwise(model_name) %>% #<<
  summarize(collect_metrics(samples, summarize = FALSE)) %>% #<<
  ggplot(aes(x = model_name, y = .estimate)) +
    geom_boxplot() + labs(x = "Model name", y = "Mean absolute error ($1000)") + coord_cartesian(ylim = c(0, NA))
```


---

## Appendix: code

.small-code[
```{r get-labels, echo=FALSE}
labs <- setdiff(
  knitr::all_labels(echo == FALSE),
  c("setup", "get-labels")
)
```

```{r all-code, ref.label=labs, eval=FALSE}
```
]
