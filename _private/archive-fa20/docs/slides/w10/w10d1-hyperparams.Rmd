---
title: "Hyperparameters and Validation"
author: "K Arnold"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: FALSE
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, message=FALSE}
source("../slides-common.R")
slideSetup()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glue)
library(tidymodels)
library(rpart.plot)
theme_set(theme_bw())
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

.small-code[
```{r utilities, echo=FALSE}
add_predictions <- function(data, ...) {
  model_names <- as.character(eval(substitute(alist(...))))
  models <- list(...)
  map2_dfr(
    model_names,
    models,
    function(model_name, model) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}

sweep_model <- function(model, var_to_sweep, sweep_min, sweep_max, ...) {
  X <- expand_grid(!!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500), ...)
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}

linear_reg <- function(engine = "lm", ...) {
  parsnip::linear_reg(...) %>% set_engine(engine)
}

decision_tree <- function(mode = "regression", engine = "rpart", ...) {
  parsnip::decision_tree(mode = "regression", ...) %>%
    set_engine(engine)
}
```

```{r load-and-subset-data}
data(ames, package = "modeldata")
ames_all <- ames %>%
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>%
  mutate(across(where(is.integer), as.double)) %>%
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

```{r train-test-split}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames_all, prop = 2 / 3)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

```{r echo=FALSE}
lat_long_grid <- expand_grid(
  Latitude  = modelr::seq_range(ames_train$Latitude,  n = 200, expand = .05),
  Longitude = modelr::seq_range(ames_train$Longitude, n = 200, expand = .05),
)
```


```{r plot-util, echo=FALSE}
show_latlong_model <- function(dataset, model, model_name = deparse(substitute(model))) {
  ggplot(dataset, aes(x = Longitude, y = Latitude)) +
    geom_raster(
      data = lat_long_grid %>% add_predictions(model),
      mapping = aes(fill = .pred)
    ) +
    geom_point(aes(color = Sale_Price), size = .5) +
    scale_color_viridis_c(aesthetics = c("color", "fill")) +
    coord_equal() +
    labs(title = model_name)
}
```

]
```{r train-model1}
model1 <-
  decision_tree(mode = "regression", tree_depth = 2) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

```{r train-model2}
model2 <-
  decision_tree(mode = "regression", tree_depth = 30) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

```{r train-model3}
model3 <-
  decision_tree(mode = "regression", cost_complexity = 1e-6, min_n = 2) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

---

## This week

* *Today*: how modeling decisions affect performance; why validate?
* *Wednesday*: **how** to validate?
* *Friday*: cross-validation lab

---

## Decision Trees

.pull-left[
```{r show-model1-alg, echo=FALSE}
model1 %>%
  pluck("fit") %>%
  rpart.plot(roundint = FALSE, digits = 5, type = 4)
```

```{r show-model1-data, echo=FALSE, out.width="100%", fig.width=10, fig.asp=NULL}
show_latlong_model(ames_train, model1) + guides(fill = "none", color = "none")
```

]

.pull-right[
```{r show-model2-alg, echo=FALSE}
model2 %>%
  pluck("fit") %>%
  rpart.plot(roundint = FALSE, digits = 5, type = 4)
```

```{r show-model2-data, echo=FALSE, out.width="100%", fig.width=10, fig.asp=NULL}
show_latlong_model(ames_train, model2) + guides(fill = "none", color = "none")
```

]

---

## Q&A

> How do we *train* a decision tree?

* The model: "choose your own adventure": at each step, check one simple condition
about one variable (e.g., `Latitude < 42.05`)
* Goal: find the best tree (for regression: minimize MSE)
* Approach: greedy algorithm: try all possible splits, keep the best one, repeat.

> I missed one of the check-in quizzes (or even weekly quizzes)!

Email me.

> Is midterm project individual?

At this point, yes.


---

## Objectives

* Identify modeling decisions that affect the performance of decision tree and
linear regression models, including:
  * Choice of model type
  * Pre-processing steps
  * Hyperparameter settings
* Explain the importance of *validation* for assessing and comparing models.

---


## What are some decisions that we've made when making models so far?

--

* Which model to use

--

* Shifting and scaling features

--

* Hyperparameters: Tree depth, number of observations per leaf, ...

---

## Hyperparameters for Decision Trees

```{r ref.label=c("train-model1", "train-model2", "train-model3"), eval=FALSE}
```

* Tree depth: how many levels of decisions
* Leaf size: how many observations need to be in each leaf node
* Complexity penalty: how much improvement for a split to be "worth it"

---

```{r show-model3, fig.width=10, echo=FALSE}
show_latlong_model(ames_train, model3)
```

---

## Hyperparameters for Linear Regression

.pull-left[
```{r}
recipe <-
  recipe(Sale_Price ~ Latitude + Longitude, data = ames_train) %>% 
  step_poly(Latitude, Longitude, degree = 5)
```

.small-code[
```{r}
model <- workflow() %>% add_recipe(recipe) %>% add_model(linear_reg()) %>%
  fit(ames_train)
show_latlong_model(ames_train, model)
```
]
]

.pull-right[
```{r}
recipe <-
  recipe(Sale_Price ~ Latitude + Longitude, data = ames_train) %>% 
  step_discretize(Latitude, Longitude, num_breaks = 10)
```

.small-code[
```{r}
model <- workflow() %>% add_recipe(recipe) %>% add_model(linear_reg()) %>%
  fit(ames_train)
show_latlong_model(ames_train, model)
```
]
]

---

## Polynomial and interaction features

.pull-left[
```{r}
recipe <-
  recipe(Sale_Price ~ Gr_Liv_Area + Bldg_Type, data = ames_train) %>%
  step_dummy(Bldg_Type) %>% 
  step_interact(
    ~ Gr_Liv_Area:starts_with("Bldg_Type")) %>% 
  step_poly(
    starts_with("Gr_Liv_Area"), degree = 2) #<<
```

```{r echo=FALSE, warning=FALSE}
model <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(linear_reg()) %>%
  fit(ames_train)

ames_train %>% 
  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price, color = Bldg_Type)) +
  geom_point(alpha = .5) +
  geom_line(
    data = model %>%
      sweep_model(Gr_Liv_Area, 0, 4000, Bldg_Type = levels(ames_train$Bldg_Type)),
    mapping = aes(y = .pred)) +
  ylim(0, 600) +
  theme(legend.position = "bottom")
```
]

.pull-right[
```{r}
recipe <-
  recipe(Sale_Price ~ Gr_Liv_Area + Bldg_Type, data = ames_train) %>%
  step_dummy(Bldg_Type) %>% 
  step_interact(
    ~ Gr_Liv_Area:starts_with("Bldg_Type")) %>% 
  step_poly(
    starts_with("Gr_Liv_Area"), degree = 5)
```

```{r echo=FALSE, warning=FALSE}
model <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(linear_reg()) %>%
  fit(ames_train)

ames_train %>% 
  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price, color = Bldg_Type)) +
  geom_point(alpha = .5) +
  geom_line(
    data = model %>%
      sweep_model(Gr_Liv_Area, 0, 4000, Bldg_Type = levels(ames_train$Bldg_Type)),
    mapping = aes(y = .pred)) +
  ylim(0, 600) +
  theme(legend.position = "bottom")
```
]

