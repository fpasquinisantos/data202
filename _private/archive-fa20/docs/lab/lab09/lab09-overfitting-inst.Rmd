---
title: "Lab 09: Over-fitting"
output: 
  tufte::tufte_html:
    css: ../lab.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(rpart.plot)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

## Goals

* Adjust parameters to obtain a model that fits *too* well
* Practice *model validation* on held-out data
* Practice more visualization (as always!)

## Getting started

* Pick a team to work with--ideally 2, no more than 3.
  * A team should be from within a single cohort.
  * (Crossing cohort lines is possible, but you'll need to ask the course staff to make an appropriate repo.)
* Go to [our GitHub organization](https://github.com/Calvin-DS202-FA20/).
  You'll find 5 `lab09` repos available to your cohort. Pick one for your team,
  making sure that no two teams share the same repo.
* On the attendance sheet, each team member should fill in the name of their team repo under the "Team" column. It should look like `lab09-V2`.
* Each team member should clone the repo as usual.

### Optional, experimental workflow

The RStudio server lets multiple people edit a single project at the same time!
This can be confusing since it's different from collaborating using GitHub, so
try it only if all of your team is game to try:

* The person whose name appears first on the Attendance sheet (or someone else if needed)
  clones the team's repo on the RStudio Server.
* They Share the project with the other team members
* They copy the URL and paste it into the attendance sheet
* Others go to that URL.

## Data

We'll again be using the Ames home sales dataset.
Again, see [Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt).
And again, we use a subset of the original data as suggested by the
[original paper](http://jse.amstat.org/v19n3/decock.pdf).

We'll also scale the sale price to be in units of $1000, to keep the numbers more manageable.

```{r load-and-subset-data}
data(ames, package = "modeldata")
ames_all <- ames %>%
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>%
  mutate(across(where(is.integer), as.double)) %>%
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

We'll use the same train-test split that we have been using:

```{r train-test-split}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames_all, prop = 2 / 3) # Split our data randomly
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

**Note**: unless explicitly mentioned below, use the *training set* for plots
and models.

## "Location, location, location!"

How much does a home's location tell us about how much it sells for?
In this lab, we'll fit several models that try to predict the sale price
based on the latitude and longitude.

1. Make a plot showing how `Sale_Price` relates to `Latitude` and `Longitude`
(make sure you get them the right way!).

* Add `coord_equal()` to make the grid square.
* Add `scale_color_viridis_c` to use an easier-to-discern range of colors.

```{r sale-price-by-location}
ggplot(ames_train, aes(x = Longitude, y = Latitude, color = Sale_Price)) +
  geom_point(size = .5) +
  scale_color_viridis_c() +
  coord_equal()
```

Now we'll fit a decision tree to predict `Sale_Price` from `Latitude` and `Longitude`.
We'll set the `tree_depth` to 2 to keep the tree shallow.

```{r}
set.seed(0)
model1 <-
  decision_tree(mode = "regression", tree_depth = 2) %>%
  set_engine("rpart") %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

You can use the following code to visualize the tree. The root of the tree is
at the top, and each branch represents checking one condition in the data. For 
example, the top left branch, labeled `Longitude >= -93.63`, means that we follow
that branch if the longitude is east of 93.63 degrees West. Each node is
labeled with two numbers: the one on top is the prediction (the average value
of all home sales that reach that node), and the one on the bottom is the
proportion of total homes that meet that condition. The top node reads 100% 
because the algorithm starts there for all homes, and 175.9 because
`mean(ames_train$Sale_Price)` is `r mean(ames_train$Sale_Price)`.

```{r}
model1 %>%
  pluck("fit") %>%
  rpart.plot(roundint = FALSE, digits = 5, type = 4)
```

1. Using the tree visualization above, manually compute the predicted sale price for a home at
   a latitude of 41.98650 and longitude of -93.60372. (Be careful with comparisons
   with negative numbers!) Briefly summarize the steps you followed. Check that
   your answer is below $150k.
   
```{r include=FALSE}
ames_train %>%
  filter(Longitude >= -93.63, Latitude < 42.06) %>%
  arrange(Latitude) %>% 
  select(Latitude, Longitude, Sale_Price)
stopifnot(
  model1 %>% predict(tibble(Latitude = 41.98650, Longitude = -93.60372)) %>% pull(.pred)
  < 150)
```

### Visualizing the predictions in "data space"

Since the features are latitude and longitude, we get the unique ability to
visualize the tree in a different way: by looking at what predictions it makes
for each location.

Specifically, we make a grid that roughly covers the city of Ames. We'll ask the model
how much it thinks a home at each one of those grid points would sell for.

```{r}
lat_long_grid <- expand_grid(
  Latitude  = modelr::seq_range(ames_train$Latitude,  n = 200, expand = .05),
  Longitude = modelr::seq_range(ames_train$Longitude, n = 200, expand = .05),
)
```


We start by defining a utility function that takes a data frame and adds a model's
predictions onto it (don't worry about how this works).

```{r utilities}
add_predictions <- function(data, model, variable_name = ".pred", model_name = deparse(substitute(model))) {
  model %>%
    predict(data) %>%
    rename(!!enquo(variable_name) := .pred) %>%
    mutate(model = model_name) %>%
    bind_cols(data)
}
```

And here's another utility function, to show the model's predictions:

```{r plot-util}
show_latlong_model <- function(dataset, model, model_name = deparse(substitute(model))) {
  ggplot(dataset, aes(x = Longitude, y = Latitude)) +
    geom_raster(
      data = lat_long_grid %>% add_predictions(model, model_name = model_name),
      mapping = aes(fill = .pred)
    ) +
    geom_point(aes(color = Sale_Price), size = .5) +
    scale_color_viridis_c(aesthetics = c("color", "fill")) +
    coord_equal() +
    labs(title = model_name)
}
```

Let's see how `model1` predicts.

```{r show-model1-data}
show_latlong_model(ames_train, model1)
```

### Inspecting the mistakes

Make a histogram of the residuals.

```{r}
ames_train %>%
  add_predictions(model1) %>%
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
    geom_histogram(binwidth = 10)
```

We can quantify that by looking at the mean absolute error.

```{r}
ames_train %>%
  add_predictions(model1) %>%
  # summarize(mae = mean(abs(Sale_Price - .pred)))
  mae(truth = Sale_Price, estimate = .pred)
```


Now let's allow the tree to grow: set `tree_depth = 30` (which is actually the maximum
allowed by the `rpart` engine).

```{r}
model2 <-
  decision_tree(mode = "regression", tree_depth = 30) %>%
  set_engine("rpart") %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

Now let's visualize the tree again, first as an algorithm:

```{r}
model2 %>%
  pluck("fit") %>%
  rpart.plot(roundint = FALSE, digits = 4, type = 4)
```

and then in data space:

```{r}
show_latlong_model(ames_train, model2)
```

1. Compare the data-space visualization of `model2` with `model1` above. How are
they similar? How are they different?

Plot a histogram of the residuals again. Use `+ coord_cartesian(xlim = c(XMIN, XMAX))`,
filling in the minimum and maximum appropriately, so that all of your residuals
histograms are on the same scale.

```{r}
ames_train %>%
  add_predictions(model2) %>%
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 10) +
  coord_cartesian(xlim = c(-200, 400))
```

And we can quantify with MAE again:

```{r}
ames_train %>%
  add_predictions(model2) %>%
  mae(truth = Sale_Price, estimate = .pred)
```


1. Overall, how did the residuals change between `model1` and `model2`? Which model
is better? Write a concise sentence comparing the two models quantitatively:
"On average, the predictions of model X ___ ___ ____ than model Y."

Finally, we create `model3` by allowing the model to make leaves with as few
as 2 observations (`min_n = 2` instead of the default of 20) and lower the threshold for how much improvement
each split must add (`cost_complexity = 1e-6`, instead of the default 0.01).

Since this will make a very large tree, **don't try to run `rpart.plot`**. But do repeat the
data space and residual plots.

```{r}
model3 <-
  decision_tree(mode = "regression", cost_complexity = 1e-6, min_n = 2) %>%
  set_engine("rpart") %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

```{r}
show_latlong_model(ames_train, model3)
```

1. Compare the data-space visualization of `model3` with that of the other two models
above. What do you notice? 

Now we analyze the errors again.

```{r}
ames_train %>%
  add_predictions(model3) %>%
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 1) +
  coord_cartesian(xlim = c(-200, 400))
```

1. Write a summary of 1 to 2 sentences summarizing what you have noticed about `model3`.
  Discuss both how it makes its predictions and how good those predictions are.


## Validation

Now we evaluate the three models on the test set.

The following code fragment collects the residuals for all 3 models on both 
`ames_train` and `ames_test`.

```{r}
all_resids <- expand_grid(
  model = c("model1", "model2", "model3"),
  data = c("ames_train", "ames_test")
) %>%
  pmap_dfr( # Run this function on all of the data points, binding rows of results together.
    function(model, data) {
      get(data) %>%
        add_predictions(get(model)) %>%
        mutate(resid = Sale_Price - .pred) %>%
        mutate(model = !!model, data = !!data)
    }
  ) %>%
  mutate(data = fct_relevel(data, "ames_train", "ames_test"))
```



```{r include=FALSE}
ggplot(all_resids, aes(x = resid, y = after_stat(density), color = model)) +
  geom_freqpoly(binwidth = 10) +
  facet_wrap(vars(data), ncol = 1)
```

```{r include=FALSE}
validation_summaries <- all_resids %>%
  group_by(model, data) %>%
  summarize(mae = mean(abs(Sale_Price - .pred))) %>% 
  pivot_wider(names_from = data, values_from = mae) %>% 
  mutate(train_test_diff = ames_test - ames_train)
diffs_by_model <- validation_summaries %>% pull(train_test_diff, name = model)
stopifnot(diffs_by_model[['model3']] > 20)
stopifnot(abs(diffs_by_model[['model1']]) < 5)
```


```{r}
all_resids %>%
  group_by(model, data) %>%
  summarize(mae = mean(abs(Sale_Price - .pred))) %>%
  ggplot(aes(x = model, y = mae, fill = data)) +
  geom_col(position = 'dodge') +
  labs(y = "Mean absolute error ($1000)", fill = "Assessment dataset")
```

1. Now that you see this, what can you say about the performance of the three models?
