---
title: "Lab 10: Model Tuning"
output: 
  tufte::tufte_html:
    css: ../lab.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(rpart.plot)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

## Goals

In this lab (turned homework) you will try to make models of two different types
and tune their parameters and data choices to do well at a prediction task.
This is the central task of data modeling. Many (but far from all) people think it's the most
fun part.

Our learning objectives are:

* Design an accurate predictive model
* Compare model performance using cross-validation
* Validate model performance on held-out data
* Practice more visualization (as always!)

## Getting started

* Pick a team to work with--ideally 2, no more than 3.
  * A team should be from within a single cohort.
  * (Crossing cohort lines is possible, but you'll need to ask the course staff to make an appropriate repo.)
* Go to [our GitHub organization](https://github.com/Calvin-DS202-FA20/).
  You'll find 5 `lab10` repos available to your cohort. Pick one for your team,
  making sure that no two teams share the same repo.
* On the attendance sheet, each team member should fill in the name of their team repo under the "Team" column. It should look like `lab10-V2`.
* Each team member should clone the repo as usual.

### Optional, experimental workflow

The RStudio server lets multiple people edit a single project at the same time!
This can be confusing since it's different from collaborating using GitHub, so
try it only if all of your team is game to try:

* The person whose name appears first on the Attendance sheet (or someone else if needed)
  clones the team's repo on the RStudio Server.
* They Share the project with the other team members
* They copy the URL and paste it into the attendance sheet
* Others go to that URL.

## Data and Initial Split

You have been given a training set with a random subset of the Ames home sale dataset.
Your repo includes a command to load it.
See the [Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)
if you need it.

We've scaled the sale price to be in units of $1000, to keep the numbers more manageable.

```{r load-and-subset-data, echo=FALSE}
data(ames, package = "modeldata")
ames_all <- ames %>%
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>%
  mutate(across(where(is.integer), as.double)) %>%
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

```{r train-test-split, echo=FALSE, message=FALSE}
set.seed(527)
invisible(rnorm(5))
ames_split <- initial_split(ames_all, prop = 3 / 4)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

ames_train %>% write_csv("data/ames_train.csv")
invisible(file.copy("data/ames_train.csv", "../../../src/lab/lab10-tuning/data/ames_train.csv"))
ames_train <- read_csv("data/ames_train.csv")
```

## Modeling

Now that we have techniques for specifying a model, preprocessing data to give to it,
and estimating its performance, let's try to just make a good model.

We learned about *recipes* and *workflows* but only used them occasionally last week.
For this lab, though, they will become essential as we seek to compare models
that use different variables, different preprocessing steps, different models,
etc, using the same methodology.

Our basic strategy will be to collect model specs and metrics in a data frame.
We'll run through the approach once for a basic linreg and decision tree,
then you'll try out things on your own.

### Initialize

Overall let's use 10-fold cross-validation. We need to set a seed here
because `vfold_cv` uses random splitting.

```{marginfigure}
Since the toolkit I usually use, Scikit-Learn, uses deterministic cross-validation
by default, I didn't realize that this needs a seed to be reproducible.
Sorry for those of you who got confused because our numbers didn't match.
```


```{r declare-cv}
set.seed(2)
ames_resamples <- ames_train %>% 
  vfold_cv(v = 10)
```

These models can be time-consuming to run, so let's collect the results in a
`modeling_results` data frame as we go. Let's start off with no results:

```{r empty-modeling-results}
modeling_results <- tibble()
```

And here's a function to add a new set of results. Don't worry about how it works.

```{r append-new-results}
append_new_results <- function(modeling_results, model_name, spec, cv_results) {
  if (nrow(modeling_results) > 0 && model_name %in% modeling_results$model_name)
    stop(paste0(
      "There's already results for a model with the name ", model_name,
      ". Did you forget to change the name?"))
  
  bind_rows(
    modeling_results %>% mutate(model_name = as.character(model_name)),
    tibble(
      model_name = model_name,
      spec = list(spec),
      cv_results %>% select(-.estimator)
    )
  ) %>% 
    mutate(model_name = as_factor(model_name)) # Ensure that factor level matches insertion order.
}

get_spec_for_model <- function(model_name) {
  modeling_results %>% filter(model_name == !!model_name) %>% purrr::chuck("spec", 1)
}
```

Finally, a few utilities:

```{r utilities}
add_predictions <- function(data, ...) {
  imap_dfr(
    rlang::dots_list(..., .named = TRUE),
    function(model, model_name) {
      model %>%
        predict(data) %>%
        bind_cols(data) %>%
        mutate(model = !!model_name)
    }
  )
}

sweep_model_examples <- function(model, dataset, vars_to_sweep, examples = slice_sample(dataset, n = 10)) {
  X <- map_dfr(vars_to_sweep, function(v) {
    var_to_sweep <- rlang::as_label(v)
    sweep_min <- min(dataset[[var_to_sweep]])
    sweep_max <- max(dataset[[var_to_sweep]])
    expand_grid(
      examples %>% select(-!!var_to_sweep) %>% mutate(.idx = row_number()),
      !!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500)) %>% 
      mutate(sweep_var = var_to_sweep, .sweep_val = .data[[var_to_sweep]])
  })
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}

linear_reg <- function(engine = "lm", ...) {
  parsnip::linear_reg(...) %>% set_engine(engine)
}

decision_tree <- function(mode = "regression", engine = "rpart", ...) {
  parsnip::decision_tree(mode = "regression", ...) %>%
    set_engine(engine)
}
```


### Linear Regression

Let's first fit a simple linear regression. First we define our spec.

```{marginfigure}
Note that we need to give `recipe` a `data` parameter. This is the *template*
data, just used to tell the recipe what columns there are and what data type
each one has. It does not actually train on this data!
```


```{r linreg1-spec}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude, data = ames_train)
  ) %>% 
  add_model(
    linear_reg()
  )
```

Then, let's get the cross-validation assessment scores:

```{r linreg1-cv}
cv_results <- 
  spec %>% 
  fit_resamples(resamples = ames_resamples, metrics = metric_set(mae)) %>% 
  collect_metrics(summarize = FALSE)

cv_results
```

Finally, we collect these results into our `modeling_results` data frame.

```{r linreg1-collect}
modeling_results <- modeling_results %>% 
  append_new_results(
    model_name = "Simple Linreg",
    spec = spec,
    cv_results = cv_results
  )
modeling_results
```

Notice that in your repo these three code blocks are all together in the same
chunk. That's intentional, to avoid accidentally getting the three parts
out of sync.

### Decision Tree

Your turn: repeat the above, but now using the default `decision_tree()`.
Start by copying and pasting the `linreg` chunk from above and changing the
chunk name, model spec, and `model_name` parameter.

```{r tree1-spec, echo=FALSE}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude, data = ames_train)
  ) %>% 
  add_model(
    decision_tree()
  )
```

```{r tree1-cv, echo=FALSE}
cv_results <- 
  spec %>% 
  fit_resamples(resamples = ames_resamples, metrics = metric_set(mae)) %>% 
  collect_metrics(summarize = FALSE)
```

```{r tree1-collect, echo=FALSE}
modeling_results <- modeling_results %>% 
  append_new_results(
    model_name = "Decision Tree",
    spec = spec,
    cv_results = cv_results
  )
```

Here's what my modeling results looked like after I did that:

```{r tree1-modeling-results}
modeling_results
```

## Compare model performance

Compare the performance of the two models. Which one is better?

```{r compare-models}
modeling_results %>% 
  ggplot(aes(y = fct_rev(model_name), x = .estimate)) +
    geom_boxplot() +
    geom_point() +
    labs(x = "Mean Absolute Error ($1000)", y = "")
```


```{r echo=FALSE}
modeling_results %>% 
  group_by(model_name) %>% 
  summarise(mae_median = median(.estimate))
```


### Better Decision Tree

Try getting a better decision tree model:

* Use more features
* Tweak the `decision_tree` parameters (run `?rpart::rpart.control` for details)
  * `cost_complexity`: any split that does not improve the fit by this much is not attempted (defaults to 0.01, aka `cp`)
  * `tree_depth`: maximum depth of any node in the final tree (defaults to 30, aka `maxdepth`)
  * `min_n`: minimum number of observations that must exist in a node in order for a split to be attempted (defaults to 20, aka `minsplit`)

```{r tree2-spec, echo=FALSE}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude + Gr_Liv_Area + Bldg_Type, data = ames_train)
  ) %>% 
  add_model(
    decision_tree()
  )
```

```{r tree2-cv, echo=FALSE}
cv_results <- 
  spec %>% 
  fit_resamples(resamples = ames_resamples, metrics = metric_set(mae)) %>% 
  collect_metrics(summarize = FALSE)
```

```{r tree2-collect, echo=FALSE}
modeling_results <- modeling_results %>% 
  append_new_results(
    model_name = "Decision Tree 2",#: Lat, Long, LivArea, BldgType",
    spec = spec,
    cv_results = cv_results
  )
```

## Visualize models

Let's look at two ways of visualizing what's going on in fitted models.
The examples in this section will show the results of the last model that
your instructor happened to spec out (while playing around with decision trees
for the last section); notice what's different when you apply these
techniques to different models.

First, let's get a model fit on the whole training set. Remember: the cross validation process trained a bunch of smaller models; now we want to look at the slightly bigger full model.

```{r fit-on-full-train}
model <- 
  get_spec_for_model("Decision Tree 2") %>%
  fit(ames_train)
```

#### Examine Residuals

Here's a way to see at a glance what might be helpful features to add becasue
there are patterns in the residuals. We just plot the residuals against all
of the other numeric features. (It doesn't handle categorical features, but 
you can try mapping a categorical feature to color or something.)

```{r resids-against-all-numeric, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
ames_train %>% 
  add_predictions(model) %>% 
  mutate(resid = Sale_Price - .pred) %>% 
  # Pivot all of the numeric variables except for the outcomes and residuals.
  pivot_longer(c(where(is.numeric) & !any_of(c("Sale_Price", ".pred", "resid")))) %>% 
  # Plot!
  ggplot(aes(x = value, y = resid)) +
    geom_point() +
    geom_smooth(method = "lm") +
    facet_wrap(vars(name), scales = "free")
```

##### Examine Predictions

The `sweep_model_examples` function defined above creates example predictions
by starting with 10 randomly selected example data points and sweeping a variable
(columns) across the whole range of values seen in the training set.
You give it one or more variables using `vars`, the same quoting
function that `facet_wrap` uses. **Note**: it currently only works for continuous
variables.

The following example shows how to sweep several variables and plot each sweep
as a line.

```{r sweep-model-examples, fig.fullwidth=TRUE}
set.seed(0)
sweep_model_examples(model, ames_train, vars(Longitude, Latitude, Gr_Liv_Area)) %>% 
  ggplot(aes(x = .sweep_val, y = .pred, group = .idx, color = Latitude)) +
  geom_line(alpha = .5) +
  facet_wrap(vars(sweep_var), scales = "free")
```



### Better Linreg

Now, try to get a better linear regression model. You might try:

* Use more features
  * add dummy encoding steps as needed (`step_dummy`, and perhaps `step_other` if needed)
* Transform existing features
  * `step_poly`, or its better-behaved cousin `step_ns`
  * `step_discretize`
  * `step_interact` (see hw9 for an example)

```{r linreg2-spec, echo=FALSE}
spec <- workflow() %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude + Gr_Liv_Area + Bldg_Type, data = ames_train) %>% 
      step_other(Bldg_Type) %>% 
      step_dummy(Bldg_Type) %>% 
      step_ns(Latitude) %>% 
      step_ns(Longitude)
  ) %>% 
  add_model(
    linear_reg()
  )
```

```{r linreg2-cv, echo=FALSE}
cv_results <- 
  spec %>% 
  fit_resamples(resamples = ames_resamples, metrics = metric_set(mae)) %>% 
  collect_metrics(summarize = FALSE)
```

```{r linreg2-collect, echo=FALSE}
modeling_results <- modeling_results %>% 
  append_new_results(
    model_name = "LinReg 2",#: Lat, Long, LivArea, BldgType, natsplines on LatLong",
    spec = spec,
    cv_results = cv_results
  )
```



## Compare model performance

Once you have collected performance data for a few different models, compare their
performance by plotting data from the `modeling_results` data frame.

(The other visuals are just for you to explore. Keep your model performance visual 
chunk after all of your modeling results have been collected, so that you can
plot the performance here.)

For example, here is the result with some models I was playing around with:

```{r example-model-comparison}
modeling_results %>% 
  ggplot(aes(y = fct_rev(model_name), x = .estimate)) +
    geom_boxplot() +
    geom_point() +
    labs(x = "Mean Absolute Error ($1000)", y = "")
```

## Wrap-up

1. Make a list of all of the models that you tried. (Give enough detail that another student would be able to try the same model).
   Mark which one is your best decision tree and which one is your best linear regression.

2. How does the numerical performance of your best linear regression compare with your best decision tree?
  Refer to both the center and spread of cross-validated performance estimates.

3. Pick a house from the training set. Explain how each of the two models you described
  in the previous question makes a prediction on that house.
  
Here's the example home to use:

```{r get-example-home}
example_home <- ames_train %>% head(1)
example_home
```

And here's how to get your model's prediction on it.

```{r get-last-fit}
# Get the fitted model
model <- 
  get_spec_for_model("Decision Tree") %>%
  fit(ames_train)

# Make the prediction
model %>% predict(example_home)
```

You can use the plots from "Examine Predictions" above to help understand what your model
is doing. For a small enough decision tree, you can also use `rpart.plot` like
we did in Lab 9 (but we need very slightly different code since we're using `workflow`s):

```{r show-final-tree, eval=FALSE}
model %>%
  workflows::pull_workflow_fit() %>% 
  pluck("fit") %>%
  rpart.plot::rpart.plot(roundint = FALSE, digits = 5, type = 4)
```

4. Which model do you think will do best on the test set that the course staff has held out?
  What error do you expect? Report your answer as follows:
  
```
spec <- YOUR SPEC CODE HERE
expected_test_set_MAE <- YOUR NUMBER HERE
```

Explain your reasoning.
