[["index.html", "DATA 202 Fall 2020 Preface", " DATA 202 Fall 2020 K Arnold 2020-12-01 Preface These notes were assembled to accompany a Data Science 202 course at Calvin University. "],["tools.html", "1 Tools 1.1 Useful Resources 1.2 Why these tools? 1.3 R and RStudio 1.4 R packages", " 1 Tools 1.1 Useful Resources Markdown Cheatsheet, Tutorial Getting Used to R, RStudio, and R Markdown: Screenshots and screencasts (with no audio) Tidyverse Style Guide 1.2 Why these tools? R It uses nouns and verbs where Python/Pandas uses syntax Its operations map pretty cleanly onto generalizable concepts It has an extensive library of useful packages It plays well with Python Python It’s very commonly used. It’s extremely versatile. It’s reasonably fast. RMarkdown It embodies reproducibility … which helps us practice integrity and humility. Git and GitHub It helps us be hospitable to teammates and future readers It’s very commonly used. 1.3 R and RStudio 1.3.1 rstudio.calvin.edu Your Calvin login and password should get you onto https://rstudio.calvin.edu 1.3.2 RStudio on the Linux machines R and RStudio are also installed on the Linux machines in the Maroon and Gold Labs. They are accessible via https://remote.cs.calvin.edu. 1.3.3 Rstudio on your own machine You can download and install RStudio and R on your personal computer. 1.4 R packages You will need a number of R packages. We will have these installed for your on https://rstudio.calvin.edu, but if you work on your own machine or on the Linux lab machines, you will need to install these yourself. Individual packages can be installed from within RStudio by clicking on the install icon in the packages tab. Alternatively, you can run the following code from the command line. # make sure all current packages are up-to-date update.packages() pkgs &lt;- c( &quot;rmarkdown&quot;, &quot;tidyverse&quot;, &quot;ggformula&quot;, &quot;plotly&quot;, &quot;tidymodels&quot;, &quot;pins&quot;, &quot;reticulate&quot;, &quot;mosaic&quot;, &quot;devtools&quot;, &quot;usethis&quot;, &quot;keras&quot;, &quot;nycflights13&quot;, &quot;skimr&quot;, &quot;timeDate&quot;, &quot;qualtRics&quot; ) # skip package you already have to save time pkgs &lt;- setdiff(pkgs, installed.packages()[,1]) # install what&#39;s left install.packages(pkgs) We’re additionally using these packages for development; they’re optional for you. if (!(&quot;emo&quot; %in% installed.packages()[, 1])) devtools::install_github(&quot;hadley/emo&quot;) "],["visualization.html", "2 Visualization 2.1 Reading 2.2 Application 2.3 References 2.4 Tweaks 2.5 Mapping", " 2 Visualization We start with visualization because, well, you can see the results. 2.1 Reading To design good visuals, you need both whys and hows. You may have come here for the hows, but both are important. Our tools are changing more rapidly than ever, so if we want knowledge that lasts, we really need to know the why. 2.1.1 Why Read Look at Data from Healy “Data Visualization”. The text is wordy but well organized, so your speed reading skills should work well. Look at the examples: can you explain to someone else what those examples show? 2.1.2 How Read Data Visualization from ModernDive. Try to actually answer the “Learning Check” questions for yourself. Yes this takes longer than just skimming right past them. But they may show up on a quiz… 2.2 Application You did some visualization in Lab 1. How did that exercise relate to the “why” reading? 2.3 References If you’re the slides type, here’s some slides. We got to most of this material but not quite all of it yet. https://socviz.co/ Fundamentals of Data Visualization 2.4 Tweaks 2.4.1 Reordering bars in a bar plot Use fct_reorder on the categorical variable. starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = species)) + geom_boxplot() starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = fct_reorder(species, height))) + geom_boxplot() starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = fct_reorder(species, height, .fun = max))) + geom_boxplot() For more info, see the forcats vignette. 2.4.2 Tweaking scales A common request: scientific notation vs not. A few options: Use different units. e.g., millions of people. gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop / 1e6)) + geom_line() + labs(y = &quot;Population (millions)&quot;) Use scale_y_continuous with labels = scales::comma. gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop)) + geom_line() + scale_y_continuous(labels = scales::comma) + labs(y = &quot;Population&quot;) Use scales::label_number for even more control (see the help page). gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop)) + geom_line() + scale_y_continuous(labels = scales::label_number(scale = 1e-6, suffix = &quot;M&quot;)) + labs(y = &quot;Population&quot;) 2.4.3 Direct Labels When you have many lines, colors don’t work well for labels. Instead, use two tricks: Create a data frame with just the rightmost point of each line: gapminder_filtered &lt;- gapminder::gapminder %&gt;% group_by(country) %&gt;% filter(max(pop) &gt; 100000000) last_pop &lt;- gapminder_filtered %&gt;% group_by(country) %&gt;% slice_tail(n = 1) Use text geoms to label those points: gapminder_filtered %&gt;% ggplot(aes(x = year, y = pop, color = country)) + geom_line() + geom_text( data = last_pop, aes(label = country), # use different data color = &quot;black&quot;, hjust = &quot;left&quot; # text starts at &quot;x&quot; and faces right ) + scale_x_continuous(expand = expansion(mult = c(0, .2))) + # make some room scale_y_log10() + theme(legend.position = &quot;none&quot;) # turn off legend since it&#39;s redundant Use ggrepel::geom_text_repel to keep them from running into each other: gapminder_filtered %&gt;% ggplot(aes(x = year, y = pop, color = country)) + geom_line() + ggrepel::geom_text_repel( data = last_pop, aes(label = country), color = &quot;black&quot;, hjust = &quot;left&quot;, direction = &quot;y&quot;, # only move up or down, never left/right segment.alpha = .1, # lighten the connecting lines nudge_x = 3 ) + scale_x_continuous(expand = expansion(mult = c(0, .3))) + scale_y_log10() + theme(legend.position = &quot;none&quot;) 2.4.4 Legends and Labels If you need multiple rows for your legend, you probably have too many different values. But you can grit your teeth and do it… starwars %&gt;% skimr::skim() Table 2.1: Data summary Name Piped data Number of rows 87 Number of columns 14 _______________________ Column type frequency: character 8 list 3 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace name 0 1.00 3 21 0 87 0 hair_color 5 0.94 4 13 0 12 0 skin_color 0 1.00 3 19 0 31 0 eye_color 0 1.00 3 13 0 15 0 sex 4 0.95 4 14 0 4 0 gender 4 0.95 8 9 0 2 0 homeworld 10 0.89 4 14 0 48 0 species 4 0.95 3 14 0 37 0 Variable type: list skim_variable n_missing complete_rate n_unique min_length max_length films 0 1 24 1 7 vehicles 0 1 11 0 2 starships 0 1 17 0 5 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist height 6 0.93 174.36 34.77 66 167.0 180 191.0 264 ▁▁▇▅▁ mass 28 0.68 97.31 169.46 15 55.6 79 84.5 1358 ▇▁▁▁▁ birth_year 44 0.49 87.57 154.69 8 35.0 52 72.0 896 ▇▁▁▁▁ starwars %&gt;% ggplot(aes(x = height, y = mass, color = species)) + geom_point() + theme( legend.position = &quot;bottom&quot;, legend.key.size = unit(0.3, &quot;cm&quot;) # legend.box.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = &quot;pt&quot;) ) + guides(fill = guide_legend(nrow = 2, byrow = TRUE)) ## Warning: Removed 28 rows containing missing values (geom_point). 2.5 Mapping 2.5.1 Plotly This document shows examples of two simple mapping tasks using Plotly. More details are available in the plotly-r book. We’ll be using the tidyverse and the plotly package. library(tidyverse) library(plotly) 2.5.1.1 Markers When you just want to mark something on a map, you can give lat/long coordinates to add_markers. For example, let’s use a dataset of US cities: maps::us.cities %&gt;% head() ## name country.etc pop lat long capital ## 1 Abilene TX TX 113888 32.45 -99.74 0 ## 2 Akron OH OH 206634 41.08 -81.52 0 ## 3 Alameda CA CA 70069 37.77 -122.26 0 ## 4 Albany GA GA 75510 31.58 -84.18 0 ## 5 Albany NY NY 93576 42.67 -73.80 2 ## 6 Albany OR OR 45535 44.62 -123.09 0 Here’s how to draw it on a map. maps::us.cities %&gt;% # Fix the column names. rename(state = country.etc) %&gt;% # Keep only larger cities. filter(pop &gt; 100000) %&gt;% # Construct the &quot;geo&quot; projection. plot_geo() %&gt;% # Add state markers add_markers( # Set marker position. x = ~long, y = ~lat, # Set other aesthetics (here, redundantly encode population) size = ~pop, color = ~pop, # Customize the label. text = ~ glue::glue(&quot;{name}, population {scales::comma(pop)}&quot;), hoverinfo = &quot;text&quot; ) %&gt;% layout( # Zoom into just USA. geo = list( scope = &#39;usa&#39; ) ) ## Warning: `arrange_()` is deprecated as of dplyr 0.7.0. ## Please use `arrange()` instead. ## See vignette(&#39;programming&#39;) for more help ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: `line.width` does not currently support multiple values. 2.5.1.2 Choropleths Plotly has builtin support for countries and US states. Any other granularity requires manually working with GeoJSON files; see the documentation. Let’s make a world population map. First, let’s construct a dataset of the most recent data that Gapminder has for each country: library(gapminder) latest_country_data &lt;- gapminder::gapminder_unfiltered %&gt;% arrange(year) %&gt;% group_by(country) %&gt;% slice_tail(n = 1) %&gt;% left_join(gapminder::country_codes, by = &quot;country&quot;) Now we add a “choropleth” trace. Note that this has the typical problem of choropleth maps and densities; see Fundamentals of Data Visualization for some discussion of this. latest_country_data %&gt;% plot_geo() %&gt;% add_trace( type = &quot;choropleth&quot;, # Specify that the &quot;country&quot; column contains the country names. locations = ~country, locationmode = &quot;country names&quot;, # Use fill to show population. (I don&#39;t know why it&#39;s called &#39;z&#39; and not &#39;fill&#39;.) z = ~pop ) "],["data-wrangling.html", "3 Data Wrangling 3.1 Resources 3.2 SQL and BigQuery 3.3 Afterward", " 3 Data Wrangling 3.1 Resources First, here are some questions to ask if you’re working with data that you didn’t collect yourself. (That article is one of my favorites from the analytics writings by the Head of Decision Intelligence at Google For more resources, see the previous chapter but also: R for Data Science: Factors dplyr: cheat sheet lubridate: cheat sheet Some tips for working with SPSS data (e.g., Pew) 3.1.1 Practice TidyTuesday has weekly examples! David Robinson, contributor to several notable R packages, has done screencasts of analyzing many TidyTuesday examples. Here’s the code. 3.2 SQL and BigQuery Query languages allow us to query big datasets from our small computers. The most popular by far is SQL. Google’s BigQuery is a SQL-like language for querying datasets stored on its cloud infrastructure. Most of the time you’ll be querying data that are internal to your organization, but Google and other providers have published some open datasets. Some examples: NFL Play-by-Play NYC Yellow-Cab Trips FiveThirtyEight analysis of subreddit relationships 3.3 Afterward Arquero is a new JavaScript library that uses almost all of the same basic concepts of the Grammar of Data, though sometimes with different names. "],["predictive-modeling.html", "4 Predictive Modeling 4.1 Lingo 4.2 Reading Guide 4.3 Modeling Goals 4.4 Defining Overfitting", " 4 Predictive Modeling 4.1 Lingo You’ll see people calling the approaches we’ll discuss here by a lot of different names: “Predictive Modeling”: my current preferred term, since our goal is to predict and our approach is modeling. “Predictive Analytics”: I’m unclear exactly what this means, apparently it’s used a bit more broadly than “predictive modeling”. “Data Mining”: emphasizes the observational (rather than experimental) focus. Similar to “predictive analytics” in scope I think. “Statistical Learning”: probably the most broadly used technical term “Machine Learning”: focuses on the math and engineering of predictive modeling. I see this as a good name for the internals of statistical learning; but when you also are considering the context of the data (where it came from and the story you’ll tell with it), I think “predictive modeling” and “data science” are more holistic terms. Here are some names I don’t like: “AI”: it does have a specific meaning, but it’s become both overused (to mean anything with data in it) and underused (discounting a lot of good work in intelligent systems that doesn’t happen to need tons of data) “Big Data”: a popular but (in my opinion) not very helpful umbrella term “Data Science”: some people think (wrongly) that predictive modeling is all of what data science is about. Data Science includes, but goes far beyond, predictive modeling. 4.2 Reading Guide 4.2.1 Prediction as a Goal Below you’ll find links to an interactive visual introduction to some of the basic concepts of predictive modeling. As you read, don’t worry about the details of the classification model they’re using, but read to answer questions like these: In what sense is this a “prediction” task? Why might having multiple features help make a better prediction? What is a “model”? What does it mean to “train” a model? What makes a predictive model a good model? How can we measure that? What does it mean to “test” a model? Should a model be tested on the same data that it was trained on? Why or why not? Now the readings: Part 1 Part 2 (we won’t get here for another week, so it’s optional for now) Here’s a fun application to try out: Google’s Teachable Machine. (They made a v2 that’s less fun but more practical.) 4.2.2 Linear models for regression Many models that are used in statistical inference are also used in predictive modeling, so learning about how those models work will be helpful even though we’ll use them differently. If you’re already well familiar with these kinds of models from your stats background, you can just skim these for a refresher: Read: Introduction to Modern Statistics (OpenIntro) 3.1 Fitting a Line 3.2 Least Squares Regression Try: Introduction to Linear Models tutorials, specifically: Review: Visualizing two variables Focus on: Simple linear regression continue to others if you have time or curiosity 4.2.3 tidymodels Supervised Machine Learning: Case Studies in R Case Study 4.3 Modeling Goals (Note: a better discussion of this is found in Brieman 2001) Most of what is studied in statistics classes is modeling for the sake of inference: you want to make conclusions about a population in general based on what you observe in a sample. For example, if 1 person who received a vaccine later gets Covid compared with 2 people who didn’t, how effective is the vaccine in general? What about if it were 100 people vs 200? Is it more effective for certain demographic groups, or were they just more likely to have gotten the vaccine in the first place? Inferential stats helps you think through what information you need to know to be able to start to answer a question like that and how to design a controlled experiment so that you can actually give a robust answer, and gives you mathematical tools to compute those answers from observed data. In general, a controlled experimental setting with randomization is needed to make robust inferences about a population from a sample. And in many cases we can do this: for example, most large websites are constantly running hundreds of controlled experiments on their visitors. But in many other cases, we can’t make the experimental intervention we might want but we still observe a lot of data; is that data useless? Predictive modeling is one of several angles with which to make useful conclusions from data that was collected without randomized interventions. (Other approaches include instrumental variables analysis, propensity matching, and small confirmatory controlled experiments to evaluate hypothesis generated from observational studies.) The key idea is to change the goal: instead of trying to make an inference about a proposed relationship in isolation (like “does being male make you more likely to get a severe case of Covid, all else being equal?”, which is complicated by difference in underlying health conditions, lifestyle, etc.), a predictive model only tries to make a prediction about the outcome (“here is a 70-year-old man; how severe is his disease likely to be?”). 4.4 Defining Overfitting The Wikipedia article is actually pretty good; it's a bit sloppy, but data scientists tend to be sloppy about that definition. There's a more precise definition for the underlying phenomena, which have statistically rigorous but confusing names: bias: even with infinite training data, a linear regression could never fit a parabola (without feature engineering). One definition of bias is the test-set error when you have infinite training data. variance: the amount that test-set predictions change when the model is trained with different subsets of the training data. e.g., how robust the model is to having outliers or other things in the training set. The old conventional wisdom was that there's necessarily a trade-off between bias and variance. But that's not actually true: for example, even if one model has high variance, the average of many of them can have low variance while staying low-bias. Some have theorized that this is key to how deep learning works so well: it arrives at the same conclusions through many different paths. "],["other-topics.html", "5 Other Topics 5.1 Text Mining (and bias) 5.2 Resources", " 5 Other Topics 5.1 Text Mining (and bias) Tidy Text Mining (link is to a specific interesting section) How to make a racist AI without really trying and a follow-up in R 5.2 Resources ACM Selects: Algorithmic Fairness Data Science "],["communication.html", "6 Communication 6.1 Resources", " 6 Communication 6.1 Resources Tell a Meaningful Story With Data Don’t misuse “experiment” Shiny Apps but-therefore GitHub Pages "]]
