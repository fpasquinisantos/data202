[["index.html", "DATA 202 Supplemental Notes Preface", " DATA 202 Supplemental Notes K Arnold Fall 2021 Preface These notes were assembled to accompany a Data Science 202 course at Calvin University. "],["tools.html", "1 Tools 1.1 Useful Resources 1.2 Why these tools? 1.3 R and RStudio 1.4 R packages", " 1 Tools 1.1 Useful Resources Markdown Cheatsheet, Tutorial Getting Used to R, RStudio, and R Markdown: Screenshots and screencasts (with no audio) Tidyverse Style Guide 1.2 Why these tools? R It uses nouns and verbs where Python/Pandas uses syntax Its operations map pretty cleanly onto generalizable concepts It has an extensive library of useful packages It plays well with Python RMarkdown It embodies reproducibility … which helps us practice integrity and humility. Git and GitHub It helps us be hospitable to teammates and future readers It’s very commonly used. 1.2.1 Why not just use Excel? For data analysis: doing your analysis once in Excel can be instructive, especially for initial exploration or for demoing your ideas to non-technical clients. But move to a reproducible platform for reporting. The ability to click a button to regenerate a report using new data will be well worth the additional time it takes to set up the report the first time. Also, spreadsheets are difficult to debug. For example, a mistake in a formula contributed to flawed conclusions in a study of economic austerity that was used to make policy decisions in several countries: “[The formula] averaged cells in lines 30 to 44 instead of lines 30 to 49.”, “This spreadsheet error, compounded with other errors, is responsible for a −0.3 percentage-point error…” (Herndon, Ash, and Pollin 2013, Does High Public Debt Consistently Stifle Economic Growth?). For data management, tread carefully. Spreadsheets might be fine for small data and small teams. But they fall over when either gets big. One reason is that spreadsheets have capacity limitations. For example, the limited number of rows and columns in an Excel spreadsheet caused the UK to lose data about many Covid cases until it was discovered. Also, workflows that rely on emailing spreadsheets (or file-based databases) can lead to inconsistency, data corruption, lack of awareness of and accountability for changes, and tedium. Instead, seek more structure: SQL-style databases Salesforce or similar CRM platforms Other data management platforms (AirTable, Notion, …) 1.3 R and RStudio 1.3.1 rstudio.calvin.edu Your Calvin login and password should get you onto https://rstudio.calvin.edu 1.3.2 RStudio on the Linux machines R and RStudio are also installed on the Linux machines in the Maroon and Gold Labs. They are accessible via https://remote.cs.calvin.edu. 1.3.3 Rstudio on your own machine You can download and install RStudio and R on your personal computer. 1.4 R packages You will need a number of R packages. We will have these installed for your on https://rstudio.calvin.edu, but if you work on your own machine or on the Linux lab machines, you will need to install these yourself. Individual packages can be installed from within RStudio by clicking on the install icon in the packages tab. Alternatively, you can run the following code from the command line. # make sure all current packages are up-to-date update.packages() pkgs &lt;- c( &quot;rmarkdown&quot;, &quot;tidyverse&quot;, # &quot;ggformula&quot;, &quot;plotly&quot;, &quot;tidymodels&quot;, # &quot;pins&quot;, # &quot;reticulate&quot;, # &quot;mosaic&quot;, &quot;devtools&quot;, &quot;usethis&quot;, # &quot;keras&quot;, # &quot;nycflights13&quot;, &quot;skimr&quot;, &quot;maps&quot; # &quot;timeDate&quot; # &quot;qualtRics&quot; ) # skip package you already have to save time pkgs &lt;- setdiff(pkgs, installed.packages()[,1]) # install what&#39;s left install.packages(pkgs) We’re additionally using these packages for development; they’re optional for you. if (!(&quot;emo&quot; %in% installed.packages()[, 1])) devtools::install_github(&quot;hadley/emo&quot;) "],["visualization.html", "2 Visualization 2.1 Reading 2.2 References 2.3 Tweaks 2.4 Maps", " 2 Visualization We start with visualization because, well, you can see the results. 2.1 Reading To design good visuals, you need both whys and hows. You may have come here for the hows, but both are important. Our tools are changing more rapidly than ever, so if we want knowledge that lasts, we really need to know the why. 2.1.1 Why Read Look at Data from Healy “Data Visualization”. The text is wordy but well organized, so your speed reading skills should work well. Look at the examples: can you explain to someone else what those examples show? 2.1.2 How Read Data Visualization from ModernDive. Try to actually answer the “Learning Check” questions for yourself. Yes this takes longer than just skimming right past them. But they may show up on a quiz… 2.2 References 2.2.1 Visualization Design A quick guide: the Graphics Principles cheat sheet. Fundamentals of Data Visualization DataWrapper’s blog has some great advice on Area charts, colors, and maps. https://socviz.co/ 2.2.2 Implementation the ggplot2 book the R Graph Gallery 2.3 Tweaks 2.3.1 Reordering bars in a bar plot Use fct_reorder on the categorical variable. starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = species)) + geom_boxplot() starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = fct_reorder(species, height))) + geom_boxplot() starwars %&gt;% drop_na(height) %&gt;% ggplot(aes(x = height, y = fct_reorder(species, height, .fun = max))) + geom_boxplot() For more info, see the forcats vignette. 2.3.2 Tweaking scales A common request: scientific notation vs not. A few options: Use different units. e.g., millions of people. gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop / 1e6)) + geom_line() + labs(y = &quot;Population (millions)&quot;) Use scale_y_continuous with labels = scales::comma. gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop)) + geom_line() + scale_y_continuous(labels = scales::comma) + labs(y = &quot;Population&quot;) Use scales::label_number for even more control (see the help page). gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) %&gt;% ggplot(aes(x = year, y = pop)) + geom_line() + scale_y_continuous(labels = scales::label_number(scale = 1e-6, suffix = &quot;M&quot;)) + labs(y = &quot;Population&quot;) 2.3.3 Direct Labels When you have many lines, colors don’t work well for labels. Instead, use two tricks: Create a data frame with just the rightmost point of each line: gapminder_filtered &lt;- gapminder::gapminder %&gt;% group_by(country) %&gt;% filter(max(pop) &gt; 100000000) last_pop &lt;- gapminder_filtered %&gt;% group_by(country) %&gt;% slice_tail(n = 1) Use text geoms to label those points: gapminder_filtered %&gt;% ggplot(aes(x = year, y = pop, color = country)) + geom_line() + geom_text( data = last_pop, aes(label = country), # use different data color = &quot;black&quot;, hjust = &quot;left&quot; # text starts at &quot;x&quot; and faces right ) + scale_x_continuous(expand = expansion(mult = c(0, .2))) + # make some room scale_y_log10() + theme(legend.position = &quot;none&quot;) # turn off legend since it&#39;s redundant Use ggrepel::geom_text_repel to keep them from running into each other: gapminder_filtered %&gt;% ggplot(aes(x = year, y = pop, color = country)) + geom_line() + ggrepel::geom_text_repel( data = last_pop, aes(label = country), color = &quot;black&quot;, hjust = &quot;left&quot;, direction = &quot;y&quot;, # only move up or down, never left/right segment.alpha = .1, # lighten the connecting lines nudge_x = 3, seed = 0 # make this plot reproducible. ) + scale_x_continuous(expand = expansion(mult = c(0, .3))) + scale_y_log10() + theme(legend.position = &quot;none&quot;) See below (and spaghetti charts) for a cleaner alternative. 2.3.4 Legends and Labels If you need multiple rows for your legend, you probably have too many different values. But you can grit your teeth and do it… starwars %&gt;% skimr::skim() Table 2.1: Data summary Name Piped data Number of rows 87 Number of columns 14 _______________________ Column type frequency: character 8 list 3 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace name 0 1.00 3 21 0 87 0 hair_color 5 0.94 4 13 0 12 0 skin_color 0 1.00 3 19 0 31 0 eye_color 0 1.00 3 13 0 15 0 sex 4 0.95 4 14 0 4 0 gender 4 0.95 8 9 0 2 0 homeworld 10 0.89 4 14 0 48 0 species 4 0.95 3 14 0 37 0 Variable type: list skim_variable n_missing complete_rate n_unique min_length max_length films 0 1 24 1 7 vehicles 0 1 11 0 2 starships 0 1 17 0 5 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist height 6 0.93 174.36 34.77 66 167.0 180 191.0 264 ▁▁▇▅▁ mass 28 0.68 97.31 169.46 15 55.6 79 84.5 1358 ▇▁▁▁▁ birth_year 44 0.49 87.57 154.69 8 35.0 52 72.0 896 ▇▁▁▁▁ starwars %&gt;% ggplot(aes(x = height, y = mass, color = species)) + geom_point() + theme( legend.position = &quot;bottom&quot;, legend.key.size = unit(0.3, &quot;cm&quot;) # legend.box.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = &quot;pt&quot;) ) + guides(fill = guide_legend(nrow = 2, byrow = TRUE)) ## Warning: Removed 28 rows containing missing values (geom_point). 2.3.5 Context and Small Multiples Showing other data in the background can give context. The easiest way to do this is to filter the data and use multiple layers. (Note that we have to use the group aesthetic to draw multiple lines.) gapminder_just_usa &lt;- gapminder::gapminder %&gt;% filter(country == &quot;United States&quot;) gapminder::gapminder %&gt;% ggplot(aes(x = year, y = lifeExp, group = country)) + geom_line(alpha = 0.1) + geom_line(data = gapminder_just_usa, color = &quot;red&quot;) This pairs nicely with faceting; you just need to remove the faceting variable from the background plot. We can use this technique to make an easy-to-read version of the spaghetti chart above, using small multiples instead: gapminder_filtered %&gt;% ggplot(aes(x = year, y = lifeExp)) + geom_line(data = gapminder::gapminder %&gt;% rename(country2 = country), mapping = aes(group = country2), alpha = 0.1) + geom_line(color = &quot;red&quot;) + facet_wrap(vars(country), nrow = 2) That example was a bit complex because we needed to maintain the group for the lines. Scatter plots are simpler to code: gapminder::gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point(data = gapminder::gapminder %&gt;% select(-continent), alpha = 0.1, size = .5) + geom_point(color = &quot;red&quot;, size = .5) + facet_wrap(vars(continent)) + scale_x_log10() 2.4 Maps 2.4.1 Plotly This document shows examples of two simple mapping tasks using Plotly. More details are available in the plotly-r book. We’ll be using the tidyverse and the plotly package. library(tidyverse) library(plotly) 2.4.1.1 Markers When you just want to mark something on a map, you can give lat/long coordinates to add_markers. For example, let’s use a dataset of US cities: maps::us.cities %&gt;% head() ## name country.etc pop lat long capital ## 1 Abilene TX TX 113888 32.45 -99.74 0 ## 2 Akron OH OH 206634 41.08 -81.52 0 ## 3 Alameda CA CA 70069 37.77 -122.26 0 ## 4 Albany GA GA 75510 31.58 -84.18 0 ## 5 Albany NY NY 93576 42.67 -73.80 2 ## 6 Albany OR OR 45535 44.62 -123.09 0 Here’s how to draw it on a map. maps::us.cities %&gt;% # Fix the column names. rename(state = country.etc) %&gt;% # Keep only larger cities. filter(pop &gt; 100000) %&gt;% # Construct the &quot;geo&quot; projection. plot_geo() %&gt;% # Add state markers add_markers( # Set marker position. x = ~long, y = ~lat, # Set other aesthetics (here, redundantly encode population) size = ~pop, color = ~pop, # Customize the label. text = ~ glue::glue(&quot;{name}, population {scales::comma(pop)}&quot;), hoverinfo = &quot;text&quot; ) %&gt;% layout( # Zoom into just USA. geo = list( scope = &#39;usa&#39; ) ) ## Warning: `line.width` does not currently support multiple values. 2.4.1.2 Choropleths Plotly has builtin support for countries and US states. Any other granularity requires manually working with GeoJSON files; see the documentation. Let’s make a world population map. First, let’s construct a dataset of the most recent data that Gapminder has for each country: library(gapminder) latest_country_data &lt;- gapminder::gapminder_unfiltered %&gt;% arrange(year) %&gt;% group_by(country) %&gt;% slice_tail(n = 1) %&gt;% left_join(gapminder::country_codes, by = &quot;country&quot;) Now we add a “choropleth” trace. Note that this has the typical problem of choropleth maps and densities; see Fundamentals of Data Visualization for some discussion of this. latest_country_data %&gt;% plot_geo() %&gt;% add_trace( type = &quot;choropleth&quot;, # Specify that the &quot;country&quot; column contains the country names. locations = ~country, locationmode = &quot;country names&quot;, # Use fill to show population. (I don&#39;t know why it&#39;s called &#39;z&#39; and not &#39;fill&#39;.) z = ~pop ) "],["data-wrangling.html", "3 Data Wrangling 3.1 Resources 3.2 SQL and BigQuery 3.3 A File Per Year 3.4 Panel Survey Data (e.g., Pew Research) 3.5 Afterward", " 3 Data Wrangling library(tidyverse) 3.1 Resources First, here are some questions to ask if you’re working with data that you didn’t collect yourself. (That article is one of my favorites from the analytics writings by the Head of Decision Intelligence at Google For more resources, see the previous chapter but also: R for Data Science: Factors dplyr: cheat sheet lubridate: cheat sheet 3.1.1 Practice TidyTuesday has weekly examples! David Robinson, contributor to several notable R packages, has done screencasts of analyzing many TidyTuesday examples. Here’s the code. 3.2 SQL and BigQuery Query languages allow us to query big datasets from our small computers. The most popular by far is SQL. Google’s BigQuery is a SQL-like language for querying datasets stored on its cloud infrastructure. Most of the time you’ll be querying data that are internal to your organization, but Google and other providers have published some open datasets. Some examples: NFL Play-by-Play NYC Yellow-Cab Trips FiveThirtyEight analysis of subreddit relationships 3.3 A File Per Year When you have multiple data files containing the same data, differing only by year or the like, it’s typically best to combine them together early and do all the data wrangling to the combined data frame. For example, let’s pretend that gapminder gave us their data one year at a time, and we got data frames like: head(gm_1952, 3) ## # A tibble: 3 × 5 ## country continent lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 28.8 8425333 779. ## 2 Albania Europe 55.2 1282697 1601. ## 3 Algeria Africa 43.1 9279525 2449. head(gm_1957, 3) ## # A tibble: 3 × 5 ## country continent lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 30.3 9240934 821. ## 2 Albania Europe 59.3 1476505 1942. ## 3 Algeria Africa 45.7 10270856 3014. etc. We could merge those together like this: gm_combined &lt;- bind_rows( year_1952 = gm_1952, year_1957 = gm_1957, .id = &quot;year&quot; ) gm_combined ## # A tibble: 284 × 6 ## year country continent lifeExp pop gdpPercap ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 year_1952 Afghanistan Asia 28.8 8425333 779. ## 2 year_1952 Albania Europe 55.2 1282697 1601. ## 3 year_1952 Algeria Africa 43.1 9279525 2449. ## 4 year_1952 Angola Africa 30.0 4232095 3521. ## 5 year_1952 Argentina Americas 62.5 17876956 5911. ## 6 year_1952 Australia Oceania 69.1 8691212 10040. ## 7 year_1952 Austria Europe 66.8 6927772 6137. ## 8 year_1952 Bahrain Asia 50.9 120447 9867. ## 9 year_1952 Bangladesh Asia 37.5 46886859 684. ## 10 year_1952 Belgium Europe 68 8730405 8343. ## # … with 274 more rows We’d then want to convert that year column into a number. Two approaches: first the lazy one: gm_combined %&gt;% mutate(year = parse_number(year)) ## # A tibble: 284 × 6 ## year country continent lifeExp pop gdpPercap ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 Afghanistan Asia 28.8 8425333 779. ## 2 1952 Albania Europe 55.2 1282697 1601. ## 3 1952 Algeria Africa 43.1 9279525 2449. ## 4 1952 Angola Africa 30.0 4232095 3521. ## 5 1952 Argentina Americas 62.5 17876956 5911. ## 6 1952 Australia Oceania 69.1 8691212 10040. ## 7 1952 Austria Europe 66.8 6927772 6137. ## 8 1952 Bahrain Asia 50.9 120447 9867. ## 9 1952 Bangladesh Asia 37.5 46886859 684. ## 10 1952 Belgium Europe 68 8730405 8343. ## # … with 274 more rows And then the more principled one: gm_combined %&gt;% separate(year, into = c(NA, &quot;year&quot;)) %&gt;% mutate(year = as.numeric(year)) # or pass `convert = TRUE` to separate ## # A tibble: 284 × 6 ## year country continent lifeExp pop gdpPercap ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 Afghanistan Asia 28.8 8425333 779. ## 2 1952 Albania Europe 55.2 1282697 1601. ## 3 1952 Algeria Africa 43.1 9279525 2449. ## 4 1952 Angola Africa 30.0 4232095 3521. ## 5 1952 Argentina Americas 62.5 17876956 5911. ## 6 1952 Australia Oceania 69.1 8691212 10040. ## 7 1952 Austria Europe 66.8 6927772 6137. ## 8 1952 Bahrain Asia 50.9 120447 9867. ## 9 1952 Bangladesh Asia 37.5 46886859 684. ## 10 1952 Belgium Europe 68 8730405 8343. ## # … with 274 more rows For some more advanced techniques, see “Read Multiple Files into a Single Data Frame”. 3.4 Panel Survey Data (e.g., Pew Research) Pew and other sources release data in a file format used by SPSS, a commercial statistical analysis tool. Fortunately it’s straightforward to read this data in R, using the haven package. I’ll show an example with the American Trends Panel. 3.4.1 Reading SPSS files with haven library(tidyverse) atp_w34 &lt;- haven::read_sav(&quot;data/W34_Apr18/ATP W34.sav&quot;) The easiest way to look at this data is to click on it in the “Environment” panel, or run View(atp_w34) on the Console. (Remember not to leave a View call in an Rmd when you Knit.) You’ll see that each column has a label. It might be hard to read all of them, so here’s a bit of magic code to make a table of just the column labels: getColumnLabels &lt;- function(df) { tibble( name = names(df), label = map_chr(names(df), ~ attr(df[[.]], &quot;label&quot;)) ) } getColumnLabels(atp_w34) ## # A tibble: 140 × 2 ## name label ## &lt;chr&gt; &lt;chr&gt; ## 1 QKEY Unique ID USE THIS TO MERGE WAVES ## 2 Device_Type_W34 Wave 34 New Device Type ## 3 LANGUAGE_W34 Language ## 4 FORM_W34 FORM Assignment ## 5 SCI1_W34 SCI1. Overall, do you think science has made life easier or … ## 6 SCI2A_W34 SCI2A. Do you think science has had a mostly positive or mos… ## 7 SCI2B_W34 SCI2B. Do you think science has had a mostly positive or mos… ## 8 SCI2C_W34 SCI2C. Do you think science has had a mostly positive or mos… ## 9 SCI3A_W34 SCI3A. In your opinion, do you think government investments … ## 10 SCI3B_W34 SCI3B. In your opinion, do you think government investments … ## # … with 130 more rows Many of the columns are actually factors in disguise. To decode their labels, call as_factor. For example, to get party affiliations and leanings from the ATP data, we can do: atp_w34_wrangled &lt;- atp_w34 %&gt;% mutate( party = as_factor(F_PARTY_FINAL), party_lean = as_factor(F_PARTYLN_FINAL), age = as_factor(F_AGECAT_FINAL)) atp_w34_wrangled %&gt;% select(party, party_lean) ## # A tibble: 2,537 × 2 ## party party_lean ## &lt;fct&gt; &lt;fct&gt; ## 1 Republican &lt;NA&gt; ## 2 Democrat &lt;NA&gt; ## 3 Democrat &lt;NA&gt; ## 4 Independent The Republican Party ## 5 Republican &lt;NA&gt; ## 6 Republican &lt;NA&gt; ## 7 Democrat &lt;NA&gt; ## 8 Republican &lt;NA&gt; ## 9 Independent The Republican Party ## 10 Republican &lt;NA&gt; ## # … with 2,527 more rows Here’s a function to rename a bunch of columns to match their labels. I think it’s clumsy; probably the above is better. name_as_label &lt;- function(df, cols_to_rename) { new_names &lt;- list() for (col in cols_to_rename) { new_names[[attr(df[[col]], &quot;label&quot;)]] = col } df %&gt;% rename(!!as_vector(new_names)) } atp_w34 %&gt;% name_as_label(cols_to_rename = c(&quot;F_PARTY_FINAL&quot;, &quot;Device_Type_W34&quot;, &quot;LANGUAGE_W34&quot;)) ## # A tibble: 2,537 × 140 ## QKEY Wave 34 New …¹ Langu…² FORM_…³ SCI1_…⁴ SCI2A…⁵ SCI2B…⁶ SCI2C…⁷ SCI3A…⁸ ## &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; ## 1 100314 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 1 [Gov… ## 2 100363 1 [Mobile pho… 9 [Eng… 2 [For… 1 [Eas… 2 [Mos… 1 [Mos… 1 [Mos… 1 [Gov… ## 3 100588 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 2 [Mos… 1 [Gov… ## 4 100637 3 [Desktop] 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 2 [Gov… ## 5 101224 3 [Desktop] 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 2 [Gov… ## 6 101322 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Eas… 2 [Mos… 2 [Mos… 2 [Mos… 2 [Gov… ## 7 101400 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 1 [Gov… ## 8 101437 3 [Desktop] 9 [Eng… 1 [For… 1 [Eas… 2 [Mos… 2 [Mos… 1 [Mos… 2 [Gov… ## 9 101472 3 [Desktop] 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 1 [Gov… ## 10 101493 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Eas… 1 [Mos… 1 [Mos… 1 [Mos… 1 [Gov… ## # … with 2,527 more rows, 131 more variables: SCI3B_W34 &lt;dbl+lbl&gt;, ## # SCI3C_W34 &lt;dbl+lbl&gt;, SCI4_W34 &lt;dbl+lbl&gt;, SCI5_W34 &lt;dbl+lbl&gt;, ## # EAT1_W34 &lt;dbl+lbl&gt;, EAT2_W34 &lt;dbl+lbl&gt;, FUD30A_W34 &lt;dbl+lbl&gt;, ## # FUD30B_W34 &lt;dbl+lbl&gt;, FUD30C_W34 &lt;dbl+lbl&gt;, FUD30D_W34 &lt;dbl+lbl&gt;, ## # EAT3A_W34 &lt;dbl+lbl&gt;, EAT3B_W34 &lt;dbl+lbl&gt;, EAT3C_W34 &lt;dbl+lbl&gt;, ## # EAT3D_W34 &lt;dbl+lbl&gt;, EAT3E_W34 &lt;dbl+lbl&gt;, EAT3F_W34 &lt;dbl+lbl&gt;, ## # EAT3G_W34 &lt;dbl+lbl&gt;, EAT3H_W34 &lt;dbl+lbl&gt;, EAT3I_W34 &lt;dbl+lbl&gt;, … 3.4.2 Weights Note that Pew survey data include weights. Read their Methodology sections for details about these weights. Once you’ve identified the correct weights to use, you can use the wt parameter to count to weight your counts accordingly, or the weighted.mean function if you’re interested in a specific outcome. For example, the following gives the proportion of each party among survey respondents: atp_w34_wrangled %&gt;% count(party) %&gt;% mutate(proportion = n / sum(n)) ## # A tibble: 6 × 3 ## party n proportion ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Republican 575 0.227 ## 2 Democrat 973 0.384 ## 3 Independent 696 0.274 ## 4 Something else 280 0.110 ## 5 Refused 12 0.00473 ## 6 &lt;NA&gt; 1 0.000394 while this gives the (estimated) proportion of each party in the US: atp_w34_wrangled %&gt;% count(party, wt = WEIGHT_W34) %&gt;% mutate(proportion = n / sum(n)) ## # A tibble: 6 × 3 ## party n proportion ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Republican 619. 0.244 ## 2 Democrat 804. 0.317 ## 3 Independent 728. 0.287 ## 4 Something else 370. 0.146 ## 5 Refused 14.8 0.00583 ## 6 &lt;NA&gt; 1.42 0.000558 Add more variables to count to get cross-tabulations: atp_w34_wrangled %&gt;% count(party, age, wt = WEIGHT_W34) %&gt;% group_by(age) %&gt;% # Get party membership within each age range mutate(proportion = n / sum(n)) ## # A tibble: 23 × 4 ## # Groups: age [5] ## party age n proportion ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Republican 18-29 68.0 0.132 ## 2 Republican 30-49 173. 0.209 ## 3 Republican 50-64 198. 0.289 ## 4 Republican 65+ 177. 0.353 ## 5 Republican &lt;NA&gt; 2.23 0.573 ## 6 Democrat 18-29 128. 0.248 ## 7 Democrat 30-49 287. 0.345 ## 8 Democrat 50-64 233. 0.340 ## 9 Democrat 65+ 157. 0.312 ## 10 Independent 18-29 172. 0.334 ## # … with 13 more rows 3.5 Afterward Arquero is a new JavaScript library that uses almost all of the same basic concepts of the Grammar of Data, though sometimes with different names. "],["predictive-modeling.html", "4 Predictive Modeling 4.1 Lingo 4.2 Reading Guide 4.3 Modeling Goals 4.4 Defining Overfitting 4.5 Setting up a predictive modeling task 4.6 A simple example", " 4 Predictive Modeling 4.1 Lingo You’ll see people calling the approaches we’ll discuss here by a lot of different names: “Predictive Modeling”: my current preferred term, since our goal is to predict and our approach is modeling. “Predictive Analytics”: I’m unclear exactly what this means, apparently it’s used a bit more broadly than “predictive modeling”. “Data Mining”: emphasizes the observational (rather than experimental) focus. Similar to “predictive analytics” in scope I think. “Statistical Learning”: probably the most broadly used technical term “Machine Learning”: focuses on the math and engineering of predictive modeling. I see this as a good name for the internals of statistical learning; but when you also are considering the context of the data (where it came from and the story you’ll tell with it), I think “predictive modeling” and “data science” are more holistic terms. Here are some names I don’t like: “AI”: it does have a specific meaning, but it’s become both overused (to mean anything with data in it) and underused (discounting a lot of good work in intelligent systems that doesn’t happen to need tons of data) “Big Data”: a popular but (in my opinion) not very helpful umbrella term “Data Science”: some people think (wrongly) that predictive modeling is all of what data science is about. Data Science includes, but goes far beyond, predictive modeling. 4.2 Reading Guide 4.2.1 Prediction as a Goal Below you’ll find links to an interactive visual introduction to some of the basic concepts of predictive modeling. As you read, don’t worry about the details of the classification model they’re using, but read to answer questions like these: In what sense is this a “prediction” task? Why might having multiple features help make a better prediction? What is a “model”? What does it mean to “train” a model? What makes a predictive model a good model? How can we measure that? What does it mean to “test” a model? Should a model be tested on the same data that it was trained on? Why or why not? Now the readings: Part 1 Part 2 (we won’t get here for another week, so it’s optional for now) Here’s a fun application to try out: Google’s Teachable Machine. (They made a v2 that’s less fun but more practical.) 4.2.2 Linear models for regression Many models that are used in statistical inference are also used in predictive modeling, so learning about how those models work will be helpful even though we’ll use them differently. If you’re already well familiar with these kinds of models from your stats background, you can just skim these for a refresher: Read: Introduction to Modern Statistics (OpenIntro) 3.1 Fitting a Line 3.2 Least Squares Regression Try: Introduction to Linear Models tutorials, specifically: Review: Visualizing two variables Focus on: Simple linear regression continue to others if you have time or curiosity 4.2.3 tidymodels Supervised Machine Learning: Case Studies in R Case Study 4.3 Modeling Goals (Note: a better discussion of this is found in Brieman 2001) Most of what is studied in statistics classes is modeling for the sake of inference: you want to make conclusions about a population in general based on what you observe in a sample. For example, if 1 person who received a vaccine later gets Covid compared with 2 people who didn’t, how effective is the vaccine in general? What about if it were 100 people vs 200? Is it more effective for certain demographic groups, or were they just more likely to have gotten the vaccine in the first place? Inferential stats helps you think through what information you need to know to be able to start to answer a question like that and how to design a controlled experiment so that you can actually give a robust answer, and gives you mathematical tools to compute those answers from observed data. In general, a controlled experimental setting with randomization is needed to make robust inferences about a population from a sample. And in many cases we can do this: for example, most large websites are constantly running hundreds of controlled experiments on their visitors. But in many other cases, we can’t make the experimental intervention we might want but we still observe a lot of data; is that data useless? Predictive modeling is one of several angles with which to make useful conclusions from data that was collected without randomized interventions. (Other approaches include instrumental variables analysis, propensity matching, and small confirmatory controlled experiments to evaluate hypothesis generated from observational studies.) The key idea is to change the goal: instead of trying to make an inference about a proposed relationship in isolation (like “does being male make you more likely to get a severe case of Covid, all else being equal?”, which is complicated by difference in underlying health conditions, lifestyle, etc.), a predictive model only tries to make a prediction about the outcome (“here is a 70-year-old man; how severe is his disease likely to be?”). 4.4 Defining Overfitting The Wikipedia article is actually pretty good; it's a bit sloppy, but data scientists tend to be sloppy about that definition. There's a more precise definition for the underlying phenomena, which have statistically rigorous but confusing names: bias: even with infinite training data, a linear regression could never fit a parabola (without feature engineering). One definition of bias is the test-set error when you have infinite training data. variance: the amount that test-set predictions change when the model is trained with different subsets of all possible data. This measures, among other things, how robust the model is to having outliers or other unusual things in the training set. The old conventional wisdom was that there's necessarily a trade-off between bias and variance. But that's not actually true: for example, even if one model has high variance, the average of many of them can have low variance while staying low-bias. (This is called bagging.) Some have theorized that this is key to how deep learning works so well: it arrives at the same conclusions through many different paths. 4.5 Setting up a predictive modeling task Predictive modeling works best when the units (rows) you analyze are interchangeable. That is, ideally for predictive modeling you’d have each row be drawn from a big pool of basically interchangeable units. For example, suppose you’re trying to label the genre of a song given its audio features. These units are interchangeable: each song could just come or go without affecting anything about the other songs (unless, say, one is a cover of another or something). If your data has interrelationships between rows, like repeated measurements of the same cities / teams / countries / …, this makes your units much less interchangeable. So you either need to be creative in how to set up a prediction problem where each row is interchangeable, or think about how the lack of interchangeability might affect your results. 4.6 A simple example instance region age daily_comp stars liked 1 1 13 Y 4.5 TRUE 2 2 23 Y 4 TRUE 3 1 55 N 2 FALSE 4 2 14 N 3 TRUE 5 2 60 N 1.5 FALSE Our goal is to predict the star rating (how much the person would like a given product, on a scale from 1 to 5) from some predictors (features) such as age, region, and whether they use a computer most days. Here our five people, affectionately named 1, 2, 3, 4, and 5: 4.6.1 A Classification Task Construct a tree that predicts whether the person would like the product (stars &gt;= 3) or not (stars &lt; 3). Note: This doesn’t have to be the best possible tree; just try to come up with some tree. 4.6.2 A Regression Task Construct a tree that predicts the star rating value. "],["other-topics.html", "5 Other Topics 5.1 Recommendation Systems 5.2 Text Mining (and bias) 5.3 Resources 5.4 Relational Databases", " 5 Other Topics 5.1 Recommendation Systems 5.1.1 Discussion Activity Pick one of the following questions. Read one or more of the linked articles, or find a different article on the topic. Write a response of a paragraph or two where you: Summarize the main point of the article, for the sake of others who didn’t read it, Argue why the question is (or isn’t) important or interesting, and State your own response: a counter-point, an additional example, a question it raises for you, how it might shape your own life, etc. Here are the list of questions: Should social media platforms be legally responsible for the content they recommend to you? When Curation Becomes Creation (original link), By Liu Leqi, Dylan Hadfield-Menell, Zachary C. Lipton. the facebook files How do social media platforms determine what content to recommend? Read rsweeny21’s comment from a developer of Netflix’s autoplay. Do you use Pinterest? Here’s how Pintrest’s recommender system works, at least a few years ago. YouTube’s Recommender System uses neural nets to learn from watch times. This paper is a few years old but still insightful. Do recommendation systems “radicalize” people? search for “YouTube radicalize” or similar. You’ll probably find articles asserting that it happens. But you’ll hopefully find some counterpoints, like Examining the consumption of radical content on YouTube | PNAS. Designing Recommender Systems to Depolarize | Abstract What do recommendation systems do to human creativity? Predictive Text Encourages Predictable Writing Ads are often shown alongside other recommended content. Can ads be harmful? Discrimination in Online Ad Delivery – classic paper by Harvard professor Latanya Sweeney 5.2 Text Mining (and bias) Tidy Text Mining (link is to a specific interesting section) How to make a racist AI without really trying and a follow-up in R 5.3 Resources ACM Selects: Data Science 5.4 Relational Databases Contrasting data storage formats: * CSVs are useful to store and exchange small to medium amounts of “static” data * relational databases are useful when data is growing / changing (especially from multiple sources) * relational databases are useful when multiple systems or stakeholders need the same data * relational databases are useful when data is too big to fit in memory, since RDBMS’s can often automatically figure out how to compute things in a “distributed” or “streaming” way * So-called NoSQL systems, like MongoDB and Firebase, have been popular in recent years, but SQL remains pervasive in industry because of its robustness, consistency, and performance. SQL tutorials: Codecademy w3schools SQL Syntax reference I kinda like the “railway diagrams” in the sqlite syntax documentation (e.g., for the SELECT statement) Codecademy has a reference TutorialsPoint sqlfiddle "],["communication.html", "6 Communication 6.1 Resources", " 6 Communication 6.1 Resources Tell a Meaningful Story With Data Don’t misuse “experiment” Shiny Apps but-therefore GitHub Pages "],["ethics-and-social-impact.html", "7 Ethics and Social Impact 7.1 Privacy and Surveillance Discussion: Reidentification and Facial Recognition 7.2 Background", " 7 Ethics and Social Impact 7.1 Privacy and Surveillance Discussion: Reidentification and Facial Recognition Data about people inevitably brings up questions about privacy and surveillance. While the word “surveillance” may conjure images of security cameras and hidden microphones, today all kinds of organizations are collecting data on us through digital devices of all kinds–some in our pockets or living rooms. Even for those who “have nothing to hide”, this sort of surveillance is concerning. As data scientists, analysts, and decision-makers, you will be faced with the opportunity and challenge of handling data about people. You may decide what data to collect, how to store it, how to share it, and how to use it. “Acting justly” with this data requires thinking about, among other things, the right and responsibility of privacy. Pick one of the following two topics (Reidentification or Face Recognition) for this Discussion. Then: Pick an article or video to engage with. Write and post your observations and questions. Things you might mention: a one-sentence summary of the article for your future self a quote and your thoughts about it something you found surprising or cool something you disagree with something you didn’t understand a question this raises for you Read what your peers wrote about the other topic. 7.1.1 Reidentification Suppose your organization is asked to share data it has collected about individual people. You might think that removing the names is enough to protect privacy. Is it? Read this overview: https://georgetownlawtechreview.org/re-identification-of-anonymized-data/GLTR-04-2017/. Focus on sections 1 (Introduction), IV (Re-identification), and V (Conclusion). If you want to go deeper (or watch a video): Watch the video on this page for an example of the kind of concerns that come up when trying to anonymize data. Bonus: read Sweeny's original paper for context 7.1.2 Face Recognition Face recognition is useful to people: auto-tagging on social media and photo apps, unlocking your phone, convenience in boarding planes or paying for stuff. And it’s useful to business and government: tracking customer behavior in stores, finding criminals, identifying people critical of the government, etc… and many people think that some of these uses cross an ethical line. There is also evidence that many facial recognition systems perform more poorly for some groups of people, hence increasing the risk of harm to them. Others think that the potential for harm or abuse makes facial recognition technology something that should not be deployed even if it were perfectly accurate. Face recognition technology has recently gained more visibility as companies like Facebook have announced they’re discontinuing some forms of face recognition. Our question: How might facial recognition technology be useful? how might it be harmful? 7.1.2.0.1 Articles or Videos GenderShades summary video Project Green Light (Detroit) Tawana Petty Interview (second video on this page)—if you watch this, a second reading is optional Hill, K. (2020, July 24). Wrongfully Accused By an Algorithm. The New York Times. Portland's facial recognition ordinance: coverage by The Hill https://www.nytimes.com/2019/07/10/opinion/facial-recognition-race.html Going deeper: The 2021 film Coded Bias is feature-length discussion of facial recognition and related issues. The IndieLens discussion guide has a lot of summary information, background, and references. See the UMich ESC Project Green Light site for some other articles. 7.2 Background Fundamentals of Ethics: do the Integrated Ethics Labs First Look at Ethics exercise. MDSR2e chapter on Ethics ACM Select on Algorithmic Fairness https://blogs.rstudio.com/ai/posts/2021-07-15-ai-fairness/ 7.2.1 Current Issues Montreal AI Ethics Brief AlgorithmWatch "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
