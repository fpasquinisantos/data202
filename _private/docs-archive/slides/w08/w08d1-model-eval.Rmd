---
title: "What makes a 'good' model?"
author: "DATA 202 21FA"
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "slides.css"]
    lib_dir: libs
    self_contained: false
    nature:
      ratio: "4:3"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
---

```{r setup, include=FALSE, code=xfun::read_utf8('../slide-setup.R')}
```

```{r setup2, include=FALSE}
library(knitr)
library(tidyverse)
library(tidymodels)

sweep_model <- function(model, var_to_sweep, sweep_min, sweep_max, ...) {
  X <- expand_grid(!!enquo(var_to_sweep) := seq(sweep_min, sweep_max, length.out = 500), ...)
  model %>% 
    predict(X) %>% 
    bind_cols(X)
}


ames <- AmesHousing::make_ames() %>% 
  mutate(Sale_Price = Sale_Price / 1000) %>% 
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal")
```

## Q&A

> What other kinds of models can we use?

- We'll also study *linear regression* (and its extensions) and *logistic regression* (which is a *classification* model despite the name).
- Hot right now: Neural Networks

> Can we predict whether homes will get more expensive or not?

That would be *forecasting*, one type of prediction. Tricky to do right because you're *extrapolating*.

---

## Objectives

* Compare and contrast regression tasks and classification tasks, and give examples of each
* Identify two different ways of measuring accuracy for regression and for classification
* Identify several reasons why a model may predict better on some subsets of data than others

---

## Types of Tasks


* **regression**: predict a *number* ("continuous")
  * number should be "close" in some sense to the correct number
* **classification**: predict a *category*
  * which one of these two groups? three groups? 500,000 groups?
  * could ask: "how likely is it to be in group *i*"

---

## Are these tasks *regression* or *classification*?

1. Is this a picture of the inside or outside of the restaurant?
1. How much will it rain in GR next year?
1. Is this person having a seizure?
1. How much will this home sell for?
1. How much time will this person spend watching this video?
1. How big a fruit will this plant produce?
1. Which word did this person mean to type?
1. Will this person "Like" this post?

---

## Today's examples

**Regression**: housing prices in Ames, Iowa. Details:

* [Paper](http://jse.amstat.org/v19n3/decock.pdf)
* [Data Dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)

---

## What makes a good prediction? *Regression*

We predicted the home would sell for $250k. It sold for $200k. Is that good?

--

* **residual**: actual minus predicted<br>
  If home sold for $200k but we predicted $250k, residual is \_\_
* **absolute error**
* **squared error**

--

Across the entire dataset:

* **average error**: do we tend to predict too high? too low? "*bias*"
* **mean** or **max** absolute error
* **mean squared error** (MSE)
* normalized squared error: MSE / Variance
  - "R2" = 1 - normalized squared error

---

## Most commonly used regression metrics

- MAE: Mean Absolute Error ("predictions are usually off by $xxx")
- MAPE: Mean Absolute Percent Error ("predictions are usually off by yy%")

And some math-y ones:

- Traditional R^2 (fraction of variance explained)
- RMSE: Root Mean Squared Error: sorta like the standard deviation


---

## Seizure classification

First FDA-approved AI-powered medical device: Empatica [Embrace2](https://www.empatica.com/embrace2/),
company founded by MIT data scientist Rosalind Picard

```{r echo=FALSE, out.width="20%"}
include_graphics("https://www.empatica.com/assets/images/embrace/features_em2_mb_a-lg-xhdpi.png")
```

---

## What makes a good prediction? *Classification*

Suppose: every minute, the armband decides whether a seizure is occurring

<br>

The child was perfectly fine but our armband flagged a seizure. Is that good?

--

<br>

The child was having a seizure but our armband didn't flag it. Is that good?

---

## What makes a good prediction? *Classification*

|                      | Seizure predicted              | No seizure predicted           |
|----------------------|-------------------------------|-------------------------------|
| Seizure happened    | True positive                 | False negative (Type 1 error) |
| No seizure happened | False positive (Type 2 error) | True negative                 |

--
- **Accuracy** (% correct) = (TP + TN) / (# predictions made)
- **False negative** ("miss") **rate** = FN / (# actual seizures)
- **False positive** ("false alarm") **rate** = FP / (# true non-seizures)

--
- **Sensitivity** ("true positive rate") = TP / (# true seizures)
  - Sensitivity = 1 − False negative rate
- **Specificity** ("true negative rate") = TN / (# true non-seizures)
  - Specificity = 1 − False positive rate
- [Wikipedia article](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)

---

.question[
If you were designing a seizure alert system, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision? 
]

---

class: middle, center

## Validation

.large[**Key point**: you *must* evaluate predictions on *unseen* data]

---

```{r include=FALSE}
hack <- ames %>% arrange(Sale_Price) %>% select(Gr_Liv_Area, Sale_Price)# %>% mutate(idx = row_number())
data1 <- hack %>% slice(c(1, nrow(hack)))
data2 <- hack %>% slice(c(1, nrow(hack), as.integer(nrow(hack) / 2)))
data2
```

Hey look! I can predict exactly how much a home will sell for!

```{r include=FALSE}
decision_tree_fit <- decision_tree(mode = "regression", min_n = 2) %>% 
  fit(Sale_Price ~ Gr_Liv_Area, data = data1)
```

.small[
```{r echo=FALSE}
data1 %>% knitr::kable()
```
]

.pull-left[
```{r two-homes-dt, out.width="100%", echo=FALSE}
decision_tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot::rpart.plot(roundint = FALSE)
```

]

.pull-right[
```{r perfect-prediction-1, out.width="100%", echo = FALSE}
ggplot(data1, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = 4) +
  geom_line(data = sweep_model(
    decision_tree_fit, Gr_Liv_Area, 0, 4000),
    mapping = aes(y = .pred),
    color = "red") +
  labs(x = "Living Area", y = "Sale Price ($1k)")
```
]
---

## Validation: *unseen* data

.pull-left[
.small[
```{r echo=FALSE}
data2 %>% select(Gr_Liv_Area, Sale_Price) %>% knitr::kable()
```
]]

.pull-right[
```{r perfectly-wrong, out.width="100%", echo = FALSE}
ggplot(data1, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = 4) +
  geom_line(data = sweep_model(
    decision_tree_fit, Gr_Liv_Area, 0, 4000),
    mapping = aes(y = .pred),
    color = "red") +
  geom_point(data = data2 %>% slice_tail(n=1), color = "red", size = 4) +
  labs(x = "Living Area", y = "Sale Price ($1k)")
```
]

--

```{r echo=FALSE}
broom::augment(decision_tree_fit, new_data = data2) %>% knitr::kable()
```

---

## Oh ok, I'll just fix that one...

```{r include=FALSE}
decision_tree_fit <- decision_tree(mode = "regression", min_n = 2) %>% 
  fit(Sale_Price ~ Gr_Liv_Area, data = data2)
```

.pull-left[
```{r three-homes-dt, out.width="100%", echo=FALSE}
decision_tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot::rpart.plot(roundint = FALSE)
```

]

.pull-right[
```{r perfect-prediction-2, out.width="100%", echo = FALSE}
ggplot(data2, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = 4) +
  geom_line(data = sweep_model(
    decision_tree_fit, Gr_Liv_Area, 0, 4000),
    mapping = aes(y = .pred),
    color = "red") +
  labs(x = "Living Area", y = "Sale Price ($1k)")
```
]

### and look, it works!

```{r echo=FALSE}
broom::augment(decision_tree_fit, new_data = data2) %>% kable()
```

*Do you really think so?*

---

## Now, all the data.

```{r full-dataset-now, out.width="100%", echo = FALSE}
ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) +
  geom_point(size = .5) +
  geom_line(data = sweep_model(
    decision_tree_fit, Gr_Liv_Area, 0, 4000),
    mapping = aes(y = .pred),
    color = "red") +
  labs(x = "Living Area", y = "Sale Price ($1k)")
```

.question[
Our model had looked perfect. Where did we go wrong?
]

---

## Failure to generalize

Predictive models almost always do better on the data they're trained on than anything else.

.question[
Why?
]

--



* model uses a pattern that only held by chance
* model uses a pattern that only holds for some data
* model uses a pattern that's real but got a fuzzy picture of it

---

.question[
How can we accurately assess our models?
]

--

General strategy: *hold out data*.

```{r split-ames}
set.seed(10) # Make consistent train-test splits.
ames_split <- initial_split(ames, prop = 2/3)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

`r nrow(ames)` total homes:

- `r nrow(ames_train)` in `ames_train`
- `r nrow(ames_test)` in `ames_test`

.question[
How should we use `ames_train`? `ames_test`?
]
