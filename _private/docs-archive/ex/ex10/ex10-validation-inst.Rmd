---
title: "Guided Exercise 10 - Validation"
output: 
  tufte::tufte_html:
    css: ../ex.css
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
library(tidyverse)
library(tidymodels)
library(rpart.plot)
theme_set(theme_bw())
options(scipen = 5) # encourage metrics to print in fixed-point notation
options(dplyr.summarise.inform = FALSE) # silence a warning message
```

The goal of this exercise is to practice with validation, especially cross-validation.

## Getting started

Pull your portfolio repo as usual.

We'll be using the Ames home sales dataset that we saw in class.
If you're curious, you can look at the [Data dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)
that the author provided.
In the [original paper](http://jse.amstat.org/v19n3/decock.pdf), 
the author suggests working with a
subset of the data. So let's do that:

```{r load-and-subset-data}
# Get the data from the "modeldata" package, which comes with tidymodels.
data(ames, package = "modeldata")
ames_all <- ames %>% 
  filter(Gr_Liv_Area < 4000, Sale_Condition == "Normal") %>% 
  mutate(Sale_Price = Sale_Price / 1000)
rm(ames)
```

We'll use the same train-test split that we have been using:

```{r train-test-split}
set.seed(10) # Seed the random number generator
ames_split <- initial_split(ames_all, prop = 2 / 3) # Split our data randomly
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

And we'll tell the `yardstick` package that we want to measure the MAE, MAPE, and R2 (traditional version):

```{r}
metrics <- yardstick::metric_set(mae, mape, rsq_trad)
```


Here's a utility function we may need:

```{r}
add_predictions <- function(data, model, variable_name = ".pred", model_name = deparse(substitute(model))) {
  model %>%
    predict(data) %>%
    rename(!!enquo(variable_name) := .pred) %>%
    mutate(model = model_name) %>%
    bind_cols(data)
}
```


**Note**: unless explicitly mentioned below, use the *training set* for plots
and models.

## "Location, location, location!"

How much does a home's location tell us about how much it sells for?
In this lab, we'll fit several models that try to predict the sale price
based on the latitude and longitude.

1. Make a plot showing how `Sale_Price` relates to `Latitude` and `Longitude`
(make sure you get them the right way!).

* Add `coord_equal()` to make the grid square.
* Add `scale_color_viridis_c` to use an easier-to-discern range of colors.

```{r sale-price-by-location}
ggplot(ames_train, aes(x = Longitude, y = Latitude, color = Sale_Price)) +
  geom_point(size = .5) +
  scale_color_viridis_c() +
  coord_equal()
```

Now we'll fit a decision tree to predict `Sale_Price` from `Latitude` and `Longitude`.
We'll set the `tree_depth` to 2 to keep the tree shallow.

```{r}
set.seed(0)
model1 <-
  decision_tree(mode = "regression", tree_depth = 2) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

You can use the following code to visualize the tree. The root of the tree is
at the top, and each branch represents checking one condition in the data. For 
example, the top left branch, labeled `Longitude >= -93.63`, means that we follow
that branch if the longitude is east of 93.63 degrees West. Each node is
labeled with two numbers: the one on top is the prediction (the average value
of all home sales that reach that node), and the one on the bottom is the
proportion of total homes that meet that condition. The top node reads 100% 
because the algorithm starts there for all homes, and 175.9 because
`mean(ames_train$Sale_Price)` is `r mean(ames_train$Sale_Price)`.

```{r}
model1 %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, digits = 5, type = 4)
```

1. Using the tree visualization above, manually compute the predicted sale price for a home at
   a latitude of 41.98650 and longitude of -93.60372. (Be careful with comparisons
   with negative numbers!) Briefly summarize the steps you followed. Check that
   your answer is about \$`r round(predict(model1, tibble(Latitude = 41.98650, Longitude = -93.60372))$.pred, 1)`.
   
```{r include=FALSE}
ames_train %>%
  filter(Longitude >= -93.63, Latitude < 42.06) %>%
  arrange(Latitude) %>% 
  select(Latitude, Longitude, Sale_Price)
stopifnot(
  model1 %>% predict(tibble(Latitude = 41.98650, Longitude = -93.60372)) %>% pull(.pred)
  < 150)
```

### Visualizing the predictions in "data space"

Since the features are latitude and longitude, we get the unique ability to
visualize the tree in a different way: by looking at what predictions it makes
for each location.

Specifically, we make a grid that roughly covers the city of Ames. We'll ask the model
how much it thinks a home at each one of those grid points would sell for.

```{r}
lat_long_grid <- expand_grid(
  Latitude  = modelr::seq_range(ames_train$Latitude,  n = 200, expand = .05),
  Longitude = modelr::seq_range(ames_train$Longitude, n = 200, expand = .05),
)
```

To see the model's predictions alongside that data, we generally have to do `predict %>% bind_cols`. But the `parsnip` package provides an `augment` function that's a shortcut for that:

```{r}
augment(model1, new_data = lat_long_grid)
```

Some people may find that a bit confusing, and for others it might simply not work because of package versions. So I've provided another function
above, called `add_predictions`, that you can use instead:

```{r}
lat_long_grid %>% add_predictions(model1)
```



Here's a utility function to show a model's predictions; don't worry about how it works since
it's pretty customized to this scenario:

```{r plot-util}
show_latlong_model <- function(dataset, model,
                               model_name = deparse(substitute(model))) {
  ggplot(dataset, aes(x = Longitude, y = Latitude)) +
    geom_raster(
      data = add_predictions(lat_long_grid, model),
      mapping = aes(fill = .pred)
    ) +
    # Draw two point layers, so each point gets a little black outline.
    geom_point(color = "black", size = .75) +
    geom_point(aes(color = Sale_Price), size = .5) +
    scale_color_viridis_c(aesthetics = c("color", "fill"),
                          limits = range(ames_train$Sale_Price)) +
    coord_equal() +
    labs(title = model_name)
}
```

Let's see what `model1` predicts.

```{r show-model1-data}
show_latlong_model(ames_train, model1)
```

### Inspecting the mistakes

Make a histogram of the residuals. Recall the the "residual" is "actual minus predicted".

```{r}
ames_train %>% 
  add_predictions(model1) %>% 
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
    geom_histogram(binwidth = 10)
```

We can summarize that by looking at the mean absolute error.

```{r}
model1_accuracy_estimates_table <- ames_train %>% 
  add_predictions(model1) %>% 
  # summarize(mae = mean(abs(Sale_Price - .pred)))
  mae(truth = Sale_Price, estimate = .pred)
model1_accuracy_estimates_table
```

Note: when you're reporting this, you'll probably want to use inline R code. Here's how to get that number out:

```{r}
model1_accuracy_estimates <- model1_accuracy_estimates_table %>% 
  pull(.estimate, name = .metric)
model1_accuracy_estimates[['mae']]
```

So you'd write: the accuracy on the training set was `` `r knitr::inline_expr("signif(model1_accuracy_estimates[['mae']], 3)")` ``.

## A richer model


Now let's allow the tree to grow: set `tree_depth = 30` (which is actually the maximum
allowed by the `rpart` engine).

```{r}
model2 <-
  decision_tree(mode = "regression", tree_depth = 30) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

Now let's visualize the tree again, first as an algorithm:

```{r}
model2 %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, digits = 4, type = 4)
```

and then in data space:

```{r}
show_latlong_model(ames_train, model2)
```

1. Compare the data-space visualization of `model2` with `model1` above. How are
they similar? How are they different?

Plot a histogram of the residuals again. Use `+ coord_cartesian(xlim = c(XMIN, XMAX))`,
filling in the minimum and maximum appropriately, so that all of your residuals
histograms are on the same scale. (Don't try to calculate the range, just hard-code it.)

```{r echo=FALSE}
ames_train %>% 
  add_predictions(model2) %>% 
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 10) +
  coord_cartesian(xlim = c(-200, 400))
```

And we can quantify with MAE again:

```{r}
model2_accuracy_estimates <- ames_train %>% 
  add_predictions(model2) %>% 
  mae(truth = Sale_Price, estimate = .pred)
```


1. Overall, how did the residuals change between `model1` and `model2`? Which model
is better? Write a concise sentence comparing the two models quantitatively:
"On average, the predictions of model X ___ ___ ____ than model Y."

Finally, we create `model3` by allowing the model to make leaves with as few
as 2 observations (`min_n = 2` instead of the default of 20) and lower the threshold for how much improvement
each split must add (`cost_complexity = 1e-6`, instead of the default 0.01).

Since this will make a very large tree, **don't try to run `rpart.plot`**. But do repeat the
data space and residual plots.

```{r}
model3 <-
  decision_tree(mode = "regression", cost_complexity = 1e-6, min_n = 2) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

```{r}
model3_accuracy_estimates <- ames_train %>% 
  add_predictions(model3) %>% 
  mae(truth = Sale_Price, estimate = .pred)
```


```{r}
show_latlong_model(ames_train, model3)
```

1. Compare the data-space visualization of `model3` with that of the other two models
above. What do you notice? 

Now we analyze the errors again.

```{r}
ames_train %>% 
  add_predictions(model3) %>% 
  mutate(resid = Sale_Price - .pred) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 1) +
  coord_cartesian(xlim = c(-200, 400))
```

1. Write a summary of 1 to 2 sentences summarizing what you have noticed about `model3`.
  Discuss both how it makes its predictions and how good those predictions are.


## Validation

Now we evaluate the three models on the test set.

The following code fragment collects the predictions and metrics for all 3 models on `ames_train`.

```{r}
bind_rows(
  model1 = add_predictions(ames_train, model1),
  model2 = add_predictions(ames_train, model2),
  model3 = add_predictions(ames_train, model3),
  .id = "model"
) %>% 
  group_by(model) %>% 
  metrics(truth = Sale_Price, estimate = .pred) %>% 
  ggplot(aes(y = .estimate, x = model)) + geom_col() +
  facet_wrap(vars(.metric), scales = "free_y")
```

Now let's try that on the **test set**.

```{r}
bind_rows(
  model1 = add_predictions(ames_test, model1),
  model2 = add_predictions(ames_test, model2),
  model3 = add_predictions(ames_test, model3),
  .id = "model"
) %>% 
  group_by(model) %>% 
  metrics(truth = Sale_Price, estimate = .pred) %>% 
  ggplot(aes(y = .estimate, x = model)) + geom_col() +
  facet_wrap(vars(.metric), scales = "free_y")
```

Which model had the biggest difference between training and testing performance?

## Cross Validation

Ideally we could have noticed that huge difference *before* we peeked at our "unseen" test data! To do that, we use *cross validation*.

First, let's distinguish between a *fitted model* and a model *spec*. The *fitted model* is the thing that can actually make predictions on some new data. A model *spec* is the whole *workflow* for producing a fitted model from some training data.

Earlier, we had written:

```{r}
model1 <-
  decision_tree(mode = "regression", tree_depth = 2) %>%
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

The *spec* was the first step in that pipeline:

```{r}
model1_spec <- decision_tree(mode = "regression", tree_depth = 2)
model1 <- model1_spec %>% 
  fit(Sale_Price ~ Latitude + Longitude, data = ames_train)
```

Well, not entirely: the `fit` line also had something about the workflow: it told us what *formula* to use (what *outcome* to predict and what *features* to use to predict it). To be really pedantic, we could bundle the model spec and the formula into a `workflow`:

```{r}
model1_workflow <- workflow() %>% 
  add_model(decision_tree(mode = "regression", tree_depth = 2)) %>% 
  add_formula(Sale_Price ~ Latitude + Longitude)

model1 <- model1_workflow %>% fit(ames_train)
```

In cross validation, we apply the same model workflow to each split of the data. Each time we'll get a different *fitted* model.

### Declare a splitting strategy.

We split the *training* data into V = 10 *folds*.

```{r}
set.seed(0) # CV randomizes, so let's make it reproducible.
ames_resamples <- vfold_cv(ames_train, v = 10)
ames_resamples
```

### Fit and evaluate each of the splits

Each of the *splits* has an `analysis` and `assessment` set. Here's one example:

```{r}
first_split <- ames_resamples$splits[[1]]
first_split
```

So we'll be training a model on `r nrow(analysis(first_split))` homes and assessing its performance on `r nrow(assessment(first_split))` homes.

Here's an example of what would happen an a single split. (**Note: the `fit_resamples` function does this internally; you won't need to write code like this.** This is just for illustration.)

```{r}
# Note: you won't need to write code like this; fit_resamples below does this stuff internally.
first_split_fitted_model <- model1_workflow %>% 
  fit(analysis(first_split))
first_split_predictions <- first_split_fitted_model %>% 
  augment(new_data = assessment(first_split))
# or: assessment(first_split) %>% add_predictions(first_split_fitted_model)
first_split_metrics <- first_split_predictions %>% metrics(truth = Sale_Price, estimate = .pred)
first_split_metrics
```


Then the next split will be assessing on a completely different set of homes.
Fortunately we don't have to write out every single step; the `fit_resamples` function does this for us:


```{r}
model1_cv_results <- model1_workflow %>%
  fit_resamples(resamples = ames_resamples, metrics = metrics)
model1_cv_results
```

That's more data than we need, so we can use `collect_metrics` to extract what we want, and optionally summarize it:

```{r}
model1_cv_results %>%
  collect_metrics(summarize = FALSE)
model1_cv_results %>%
  collect_metrics(summarize = TRUE)
```

### Evaluate other models

Now, evaluate these models and compare them with `model1`:

```{r}
model2_workflow <- workflow() %>% 
  add_model(decision_tree(mode = "regression", tree_depth = 30)) %>% 
  add_formula(Sale_Price ~ Latitude + Longitude)

linear_workflow <- workflow() %>% 
  add_model(linear_reg()) %>% 
  add_formula(Sale_Price ~ Latitude + Longitude)

poly_workflow <- workflow() %>% 
  add_model(linear_reg()) %>% 
  add_recipe(
    recipe(Sale_Price ~ Latitude + Longitude, data = ames_train) %>% 
      step_interact(~ Latitude:Longitude) %>% 
      step_poly(Latitude, Longitude, degree = 4))
```

Just to whet your appetite, here's what the last model looks like:

```{r}
poly_model <- poly_workflow %>% fit(ames_train)
show_latlong_model(ames_train, poly_model)
```

