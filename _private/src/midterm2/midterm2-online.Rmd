```{r}
set.seed(0)
library(parsnip)
library(rsample)
cars_split <- initial_split(car_prices, prop = 3/4, strata = "make")
cars_train <- training(cars_split)
cars_test <- testing(cars_split)
```


## Modeling

Let's try to make some models to predict `price`.

We'll hold out a testing set so we can validate our model.

```{r train-test-split, echo=TRUE}
set.seed(0)
cars_split <- initial_split(car_prices, prop = 3/4, strata = "make")
cars_train <- training(cars_split)
cars_test <- testing(cars_split)
```


<!-- TODO: use decision tree here -->

First, create a `tidymodels` model called `model` that predicts `price` from `mileage` 
using linear regression on the training set. (You do not need a recipe here.)

Then, write code to recreate the following plot, which shows how the 
*residuals* (actual minus predicted) depend on the car type for the *training set*.
Plots like this can be useful for showing whether it's helpful to add additional
features to the predictive model.

(Note: In an iterative modeling process, we'll spend
a while looking at errors like this on our training set and tweaking the model to
fix its mistakes, then only look at our test set at the very end of the process
to make sure we didn't over-tweak the model. That's why we're looking at the
*training* set errors here.)

(Note: because of the limitations of this platform, you'll need to do both the
model fitting and the graph creation in the same "Code" box below:)

```{r residual-by-type-plot, eval=TRUE}
model <- 
  parsnip::linear_reg() %>% 
  set_engine('lm') %>% 
  fit(price ~ mileage, data = cars_train)

model %>% 
  predict(cars_train) %>% 
  bind_cols(cars_train) %>% 
  mutate(residual = price - .pred) %>% 
  ggplot(aes(y = fct_reorder(type, residual), x = residual)) +
    geom_vline(xintercept = 0) +
    geom_boxplot() +
    labs(y = "")
```

```{r residual-by-type-repro, exercise=TRUE}

```

*Note*: We're stopping short of a full predictive modeling workflow to keep this midterm
at a manageable length.
