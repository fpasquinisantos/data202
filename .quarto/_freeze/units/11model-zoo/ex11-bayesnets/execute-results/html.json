{
  "hash": "8a2d0d15d52d4a118fa48fbb6ed58db1",
  "result": {
    "markdown": "---\ntitle: \"Exercise 11: Bayesian networks among other models\"\nexecute: \n  echo: true\n  eval: false\n---\n\nThe goal of this exercise is to practice with Bayesian Networks and compare its predictions against other models seen so far.\n\n## Getting started\n\nWe'll use the [breast cancer data](https://archive.ics.uci.edu/dataset/14/breast+cancer) from the UC Irvine Machine Learning Repository. The dataset has 10 columns, which are described as follows:\n\n| Variable Name | Role    | Type        | Demographic | Description                                                                        | Units | Missing Values |\n|---------------|---------|-------------|-------------|------------------------------------------------------------------------------------|-------|----------------|\n| Class         | Target  | Binary      |             | no-recurrence-events, recurrence-events                                            |       | no             |\n| age           | Feature | Categorical | Age         | 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99                      | years | no             |\n| menopause     | Feature | Categorical |             | lt40, ge40, premeno                                                                |       | no             |\n| tumor-size    | Feature | Categorical |             | 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59     |       | no             |\n| inv-nodes     | Feature | Categorical |             | 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39 |       | no             |\n| node-caps     | Feature | Binary      |             | yes, no                                                                            |       | yes            |\n| deg-malig     | Feature | Integer     |             | 1, 2, 3                                                                            |       | no             |\n| breast        | Feature | Binary      |             | left, right                                                                        |       | no             |\n| breast-quad   | Feature | Categorical |             | left-up, left-low, right-up, right-low, central                                    |       | yes            |\n| irradiat      | Feature | Binary      |             | yes, no                                                                            |       | no             |\n\nNote that our target variable is \"Class\", thus what we are trying to predict is if the case of breast cancer will present recurrence events.\n\n### Installing sorobn and importing libraries\nSince we will be training a Bayesian network model, we will have to use a Python library for that (believe: you wouldn't want to implement all the algorithms in just one exercise).\n\nWe'll be using the [`sorobn`](https://github.com/MaxHalford/sorobn) library, which can be installed by typing in terminal:\n\n\n```{bash}\npip install sorobn graphviz\n```\n\n\nNow we are ready to import our libraries:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, export_text\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport sorobn as hh\n```\n:::\n\n\n### Loading and wrangling data\n\n* Dwnload the data from the website\n* Get the file `breast-cancer.data`, and put it in the `data` folder\n* Load it in python using `pd.read_csv()`.\n\nSince this file does not contain the column names, we are specifying them \"by hand\" when loading:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncolumn_names = ['class', 'age', 'menopause', 'tumor-size', 'inv-nodes', 'node-caps', 'deg-malig', 'breast', 'breast-quad', 'irradiat']\nbreast = pd.read_csv(\"data/breast-cancer.data\", names=column_names)\n```\n:::\n\n\nAs you can observe in the data description, two variables of our dataset have missing data: `node-caps` and `breast-quad`. Since we are still not seeing how to deal with those situations (which can give different problems in different models), we'll drop these variables for now. (Maybe you want to include them later to see what happens).\n\nFor that, you can run:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nbreast = breast.drop(['node-caps','breast-quad'], axis=1)\n```\n:::\n\n\n## Part 1: Supervised learning with previous models\n\nBefore training our Bayesian network, let's see what how we can predict with models we already used.\n\nTo make things more reusable, try to set variables indicating what are our target and features columns. For example, since our target is the `class` column and our features are all the other ones, you can set:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntarget = 'class'\nfeatures = [i for i in breast.columns if i != target]\n```\n:::\n\n\n### Part 1a: One-hot encoding of categorical variables\n\nOur dataset consists of categorical variables, and we haven't seen exactly how to use them as features with the sklearn library.\n\nWe will have to one-hot encode all of these features using a function available in the pandas library.\n\nExecute the following code and then observe what happened:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nbreast_onehot = pd.get_dummies(breast, columns = features)\n```\n:::\n\n\nAnswer:\n* *What are the feature columns now? How many are they in total?*\n\n### Part 1b: Train-test split and model fit\n\nWith our features having been one-hot encoded, we are ready for train-test splitting them. Split the dataset into 50% training data and 50% testing data.\n\nNow, fit any model we have seen previously using the train data. Remember not to consider the target column as a feature.\n\n*Indicate which model you are using*. Since we are using a classifier (not a regressor), make sure you are using the right model: in our case, DecisionTreeClassifier or RandomForestClassifier. (Import them like:)\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n```\n:::\n\n\n### Part 1c: Accuracy metrics\n\nNow, let's evaluate the accuracy of our model on our test dataset.\n\n* Make predictions with your model using the test dataset. For that, use the method `predict` in the model and remember to only pass feature variables to it (not target).\n* Now, use the function `accuracy_score`, from sklearn, to check our predictions against the true values of our target variable (in our case, `'class'`). How much was it?\n\n## Part 2: The Bayesian network model\n\nWe will now check how well a Bayesian network model will perform.\n\nHowever, **it is very important to know** that the reason for using a Bayesian network is not just increasing performance, but also:\n\n1. Being able to know probabilities on the outcomes (predictions), and\n2. Calculate these probabilities even in the case of some features having missing data.\n\n### Part 2a: Learning the structure\n\nFirst, we will have to learn the Bayesian network structure. For that, run:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nstructure = hh.structure.chow_liu(breast)\nbn = hh.BayesNet(*structure)\n```\n:::\n\n\nThis means we are using the Chow-Liu algorithm to find (one of) the best network structure possible given the data.\n\nYou can visualize the network structure by running:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndot = bn.graphviz()\nprint(dot.source)\n```\n:::\n\n\nThe `print` above will return a graph structure according to the Graphviz dot syntax. You can copy it and paste it in Quarto itself, which then will render the graph. For details on how to render graphs in Quarto, check this documentation.\n\n*Show the resulting graph* and reflect on it.\n\nA big question is: does this graph really reflects some knowledge on how a variable would cause another? (Clearly no). This happens because network structure learning algorithms are trying only to find **conditional independences** between the variable. However, once we have these, multiple graph structures are possible, and some of them would really not make sense in a \"causal\" way. Remember what we said in class: Bayesian networks **may** show causality, but not always --- sometimes they are just encoding some statistical relationships between variables (which may be useful enough, however).\n\n### Part 2b: Learning the parameters\n\nOnce we have our network structure, we can obtain the conditional probability distributions for our variables using data. Just run:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nbn = bn.fit(breast)\n```\n:::\n\n\nNow we are ready for queries, inferences, predictions, etc.\n\n## Part 3: Queries and predictions with a Bayesian network\n\nThere are lots of operations we can perform with a Bayesian network, and some of them are explored in the [tutorial for the sorobn library](https://github.com/MaxHalford/sorobn). For now, we'll only make queries and check the resulting probabilities of our target variable.\n\n### Part 3a: Queries with few features\n\nJust for testing, try making a query on the probabilities of `class` just by using knowledge of one of the variables. For example, you may want to know the probability of `class` given that `age` is `30-39`:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nbn.query('class', event={'age': '30-39'})\n```\n:::\n\n\nTry changing some of the features, or even changing the target variable (instead of `class`, try checking probabilities on `deg-malig`, or `breast`, given information on other variables), and see which probabilities are returned. *Put some examples, show them here*.\n\n### Part 3b: Predicting with all the features\n\nNow, let's make queries using all our feature variables. However, keep note that since we are identifying conditional independencies between these variables, it may happen that the knowledge of one variable wouldn't add anything to our prediction. (Optional: can you point out which variables are these, given our network?).\n\nIn any case, what we can do is convert our data rows to dictionaries and passing them to our queries. See:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\npredictions = []\nfor i in breast.to_dict('records'): # converting our database entries to dictionaries\n  del i[target] # dropping the target variable from the dict (only features then are remaining)\n  probs = n.query(target, event=i)  # getting probabilities on the target variable given our features\n  predictions.append(probs.idxmax()) # add to a list what will be the most likely value (with the highest probability)\n```\n:::\n\n\n### Part 3c: Accuracy metrics\n\nNow we are ready to compare our predictions with the true values of the target variable. Calculate the accuracy by using:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\naccuracy_score(\n    y_true=breast[target],\n    y_pred=predictions\n)\n```\n:::\n\n\nHow much is this? Is it better than our previous model?\n\n## Part 4: Tweaking, visualizing, reflecting...\n\nThere are still interesting things you can try: changing the prediction model used previously and comparing it again with our Bayesian network. Or at least you can try tweaking some of the parameters of the model used.\n\nHowever, as we noted above, the Bayesian network presents the advantage of giving us probabilities on our predictions. Which can lead us to answer: how \"good\" are these probabilities? Are we really certain of our predictions? How can we check that?\n\n",
    "supporting": [
      "ex11-bayesnets_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}